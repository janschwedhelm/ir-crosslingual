{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import importlib, os, math\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'ir_crosslingual.utils.strings' from '/Users/i500969/Desktop/Admin/Uni-Mannheim/02_Courses/2020_FSS/Information-Retrieval/03_Project/03_Implementation/04_Feature-Extraction/ir-crosslingual/ir_crosslingual/utils/strings.py'>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ir_crosslingual.embeddings import embeddings\n",
    "importlib.reload(embeddings)\n",
    "\n",
    "from ir_crosslingual.sentences import sentences\n",
    "importlib.reload(sentences)\n",
    "\n",
    "from ir_crosslingual.features import text_based\n",
    "importlib.reload(text_based)\n",
    "\n",
    "from ir_crosslingual.features import vector_based\n",
    "importlib.reload(vector_based)\n",
    "\n",
    "from ir_crosslingual.utils import strings\n",
    "importlib.reload(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "german = embeddings.WordEmbeddings('de')\n",
    "german.load_embeddings()\n",
    "\n",
    "english = embeddings.WordEmbeddings('en')\n",
    "english.load_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learn projection matrix for en-de\n",
      "Found 13700 valid translation pairs in expert dictionary.\n",
      "977 other pairs contained at least one unknown word (0 in source language, 977 in target language).\n",
      "Resulting subspace dimension: (13700, 300)\n",
      "Resulting subspace dimension: (13700, 300)\n",
      "Learn projection matrix for de-en\n",
      "Found 13700 valid translation pairs in expert dictionary.\n",
      "977 other pairs contained at least one unknown word (977 in source language, 0 in target language).\n",
      "Resulting subspace dimension: (13700, 300)\n",
      "Resulting subspace dimension: (13700, 300)\n"
     ]
    }
   ],
   "source": [
    "W_ende, W_deen = embeddings.WordEmbeddings.learn_projection_matrix(src_lang='en', trg_lang='de')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0837359 , -0.00533531, -0.08310661, ..., -0.00022553,\n",
       "         0.03983511, -0.01515955],\n",
       "       [ 0.07748925,  0.03427794, -0.00176127, ..., -0.0621672 ,\n",
       "         0.04705337, -0.00781926],\n",
       "       [-0.00330077, -0.01819852, -0.02009503, ...,  0.03127514,\n",
       "         0.00050988, -0.00330782],\n",
       "       ...,\n",
       "       [-0.08098747, -0.03583977, -0.02924371, ...,  0.03696562,\n",
       "        -0.10345663,  0.05307951],\n",
       "       [-0.03358475,  0.04922254,  0.09542661, ...,  0.0717814 ,\n",
       "        -0.04759582, -0.01552653],\n",
       "       [-0.01005702,  0.0915472 , -0.0616054 , ...,  0.00101319,\n",
       "         0.0084034 , -0.00668943]])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_ende"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0837359 ,  0.07748925, -0.00330077, ..., -0.08098747,\n",
       "        -0.03358475, -0.01005702],\n",
       "       [-0.00533531,  0.03427794, -0.01819852, ..., -0.03583977,\n",
       "         0.04922254,  0.0915472 ],\n",
       "       [-0.08310661, -0.00176127, -0.02009503, ..., -0.02924371,\n",
       "         0.09542661, -0.0616054 ],\n",
       "       ...,\n",
       "       [-0.00022553, -0.0621672 ,  0.03127514, ...,  0.03696562,\n",
       "         0.0717814 ,  0.00101319],\n",
       "       [ 0.03983511,  0.04705337,  0.00050988, ..., -0.10345663,\n",
       "        -0.04759582,  0.0084034 ],\n",
       "       [-0.01515955, -0.00781926, -0.00330782, ...,  0.05307951,\n",
       "        -0.01552653, -0.00668943]])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_deen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence embeddings and feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'ir_crosslingual.sentences.sentences' from '/Users/i500969/Desktop/Admin/Uni-Mannheim/02_Courses/2020_FSS/Information-Retrieval/03_Project/03_Implementation/04_Feature-Extraction/ir-crosslingual/ir_crosslingual/sentences/sentences.py'>"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from ir_crosslingual.sentences import sentences\n",
    "importlib.reload(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_features = ['num_words', 'num_punctuation', 'occ_question_mark', 'occ_exclamation_mark']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "sens = sentences.Sentences(src_words=english, trg_words=german)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target sentences loaded\n",
      "Source sentences loaded\n",
      "Sentences preprocessed\n",
      "Could not find a term of the sentence 'altener' in word embedding vocabulary and thus, could not calculate the respective embedding vector.\n",
      "Sentences embeddings extracted in en\n",
      "Could not find a term of the sentence 'regierungskonferenz' in word embedding vocabulary and thus, could not calculate the respective embedding vector.\n",
      "Could not find a term of the sentence 'sicherheitsberater f√ºr den gefahrguttransport' in word embedding vocabulary and thus, could not calculate the respective embedding vector.\n",
      "Could not find a term of the sentence 'arbeitsplan' in word embedding vocabulary and thus, could not calculate the respective embedding vector.\n",
      "Could not find a term of the sentence 'altfahrzeuge' in word embedding vocabulary and thus, could not calculate the respective embedding vector.\n",
      "Could not find a term of the sentence 'kapitalsteuer' in word embedding vocabulary and thus, could not calculate the respective embedding vector.\n",
      "Could not find a term of the sentence 'lebensmittelsicherheit' in word embedding vocabulary and thus, could not calculate the respective embedding vector.\n",
      "Could not find a term of the sentence 'dringlichkeitsdebatte' in word embedding vocabulary and thus, could not calculate the respective embedding vector.\n",
      "Could not find a term of the sentence 'altener' in word embedding vocabulary and thus, could not calculate the respective embedding vector.\n",
      "Could not find a term of the sentence 'arbeitsplan' in word embedding vocabulary and thus, could not calculate the respective embedding vector.\n",
      "Sentences embeddings extracted in de\n",
      "Embedding space of source language transformed according to projection matrix\n",
      "Sentences transformed\n",
      "Start preparation of feature num_words\n",
      "Start preparation of feature num_punctuation\n",
      "Start preparation of feature occ_question_mark\n",
      "Start preparation of feature occ_exclamation_mark\n"
     ]
    }
   ],
   "source": [
    "data = sens.load_data(n_max=10000, features=ind_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dict = {'text_based': ['diff_{}'.format(feat) for feat in ind_features], \n",
    "            'vector_based': ['cosine_similarity']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = sens.create_datasets(n_train=7000, n_test=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = sens.extract_features(features_dict, single_source=False, train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised classifier for cross-lingual retrieval (L2R) - DEMO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a logistic regression model on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [feature for values in features_dict.values() for feature in values]\n",
    "label = 'translation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "logisticRegr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data[features]\n",
    "y_train = train_data[[label]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logisticRegr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply binary classifier to test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['predictions'] = test_data.apply(lambda row: logisticRegr.predict(np.asarray(row[features]).reshape(1,-1))[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8786666666666667"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data.loc[test_data['translation'] == test_data['predictions']]) / len(test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
