{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import importlib\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'ir_crosslingual.features.element_based' from '/Users/i500969/Desktop/Admin/Uni-Mannheim/02_Courses/2020_FSS/Information-Retrieval/03_Project/03_Implementation/06_Logistic-Regression/ir-crosslingual/ir_crosslingual/features/element_based.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ir_crosslingual.utils import paths\n",
    "importlib.reload(paths)\n",
    "\n",
    "from ir_crosslingual.features import text_based\n",
    "importlib.reload(text_based)\n",
    "\n",
    "from ir_crosslingual.features import vector_based\n",
    "importlib.reload(vector_based)\n",
    "\n",
    "from ir_crosslingual.supervised_classification import sup_model\n",
    "importlib.reload(sup_model)\n",
    "\n",
    "from ir_crosslingual.sentences import sentences\n",
    "importlib.reload(sentences)\n",
    "\n",
    "from ir_crosslingual.features import element_based\n",
    "importlib.reload(element_based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time(start, stop, message):\n",
    "    print(f\"---- TIME: {datetime.now().strftime('%d-%m-%Y %H:%M:%S')} Computation time {message}: {stop-start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- INFO: Learn projection matrix for en-de\n",
      "---- INFO: Found 13700 valid translation pairs in expert dictionary.\n",
      "---- INFO: 977 other pairs contained at least one unknown word (0 in source language, 977 in target language).\n",
      "---- DONE: Seed dictionary extracted for the languages: en-de\n",
      "---- INFO: Resulting subspace dimension: (13700, 300)\n",
      "---- INFO: Resulting subspace dimension: (13700, 300)\n",
      "---- DONE: Projection matrix learned from en to de\n",
      "---- INFO: Learn projection matrix for de-en\n",
      "---- INFO: Found 10604 valid translation pairs in expert dictionary.\n",
      "---- INFO: 262 other pairs contained at least one unknown word (0 in source language, 262 in target language).\n",
      "---- DONE: Seed dictionary extracted for the languages: de-en\n",
      "---- INFO: Resulting subspace dimension: (10604, 300)\n",
      "---- INFO: Resulting subspace dimension: (10604, 300)\n",
      "---- DONE: Projection matrix learned from de to en\n",
      "---- INFO: File loaded containing training data\n",
      "---- INFO: File loaded containing test collection\n",
      "---- DONE: All files loaded and features extracted\n",
      "---- TIME: 20-05-2020 03:36:56 Computation time loading data from file: 0:02:08.409723\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "train_file = f'{paths.data_path}extracted_data/global/en-de/training_data_avg.pkl'\n",
    "test_file = f'{paths.data_path}extracted_data/global/en-de/test_collection_avg.pkl'\n",
    "sens, train_data, test_collection, features = sentences.Sentences.load_from_file(train_file, test_file)\n",
    "stop = datetime.now()\n",
    "time(start, stop, 'loading data from file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- INFO: Unique queries extracted\n",
      "---- INFO: Unique documents extracted\n",
      "---- INFO: Started extraction for src language.\n",
      "---- INFO: src_embedding_pca elements extracted for train data.\n",
      "---- INFO: src_embedding_pca elements extracted for unique queries.\n",
      "---- INFO: Unique queries merged to test collection\n",
      "---- INFO: Started extraction for trg language.\n",
      "---- INFO: trg_embedding_pca elements extracted for train data.\n",
      "---- INFO: trg_embedding_pca elements extracted for unique documents.\n",
      "---- INFO: Unique documents merged to test collection\n",
      "---- DONE: Extracted all vector elements and merged to test collection\n",
      "---- TIME: 20-05-2020 03:41:15 Computation time extracting vector elements: 0:04:18.950814\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "sens = element_based.vec2features(sens, 10)\n",
    "time(start, datetime.now(), 'extracting vector elements')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove pairs that contain sentences equal to '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sens.train_data[(sens.train_data['src_sentence'] != '.') & (sens.train_data['trg_sentence'] != '.')]\n",
    "test_collection = sens.test_collection[(sens.test_collection['src_sentence'] != '.') & (sens.test_collection['trg_sentence'] != '.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sens.train_data = train_data\n",
    "sens.test_collection = test_collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline LR Models using \"tfidf\" aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 'translation'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) All text/vector-based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_1 = list(text_based.FEATURES.keys()) + list(vector_based.FEATURES.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- TIME: 19-05-2020 15:23:32 Computation time fitting the baseline model: 0:00:03.348732\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "lr_1 = LogisticRegression(random_state=42)\n",
    "lr_1.fit(train_data[features_1], train_data[label])\n",
    "time(start, datetime.now(), 'fitting the baseline model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.969378978978979\n",
      "Precision: 0.003096429328074836\n",
      "Recall: 0.950950950950951\n",
      "F1: 0.0061727592883783185\n",
      "---- TIME: 19-05-2020 15:24:03 Computation time evaluating boolean: 0:00:31.315991\n",
      "---- INFO: Probabilities predicted\n",
      "---- INFO: Dataframe with evaluation ranking created\n",
      "---- INFO: Probabilities sorted for each query\n",
      "---- INFO: Index of ranking of true translation retrieved\n",
      "MAP: 0.6579165696100835\n",
      "---- TIME: 19-05-2020 15:34:26 Computation time computing the MAP score: 0:10:23.008112\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "sup = sup_model.SupModel()\n",
    "sup.evaluate_boolean(lr_1, sens, features_1)\n",
    "print('Accuracy: {}'.format(sup.accuracy))\n",
    "print('Precision: {}'.format(sup.precision))\n",
    "print('Recall: {}'.format(sup.recall))\n",
    "print('F1: {}'.format(sup.f1))\n",
    "time(start, datetime.now(), 'evaluating boolean')\n",
    "\n",
    "start = datetime.now()\n",
    "print('MAP: {}'.format(sup.compute_map(lr_1, sens, features_1)))\n",
    "time(start, datetime.now(), 'computing the MAP score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Self-selected features based on intuition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_2 = ['norm_diff_translated_words', 'norm_diff_num_words', 'norm_diff_num_punctuation',\n",
    "              'abs_diff_occ_question_mark', 'abs_diff_occ_exclamation_mark',\n",
    "              'euclidean_distance', 'cosine_similarity'\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- TIME: 19-05-2020 15:34:28 Computation time fitting the baseline model: 0:00:01.573912\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "lr_2 = LogisticRegression(random_state=42)\n",
    "lr_2.fit(train_data[features_2], train_data[label])\n",
    "time(start, datetime.now(), 'fitting the baseline model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9607871871871871\n",
      "Precision: 0.002394190776252684\n",
      "Recall: 0.9409409409409409\n",
      "F1: 0.004776228608593147\n",
      "---- TIME: 19-05-2020 15:34:46 Computation time evaluating boolean: 0:00:17.782429\n",
      "---- INFO: Probabilities predicted\n",
      "---- INFO: Dataframe with evaluation ranking created\n",
      "---- INFO: Probabilities sorted for each query\n",
      "---- INFO: Index of ranking of true translation retrieved\n",
      "MAP: 0.48391072285080006\n",
      "---- TIME: 19-05-2020 15:45:57 Computation time computing the MAP score: 0:11:10.985105\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "sup = sup_model.SupModel()\n",
    "sup.evaluate_boolean(lr_2, sens, features_2)\n",
    "print('Accuracy: {}'.format(sup.accuracy))\n",
    "print('Precision: {}'.format(sup.precision))\n",
    "print('Recall: {}'.format(sup.recall))\n",
    "print('F1: {}'.format(sup.f1))\n",
    "time(start, datetime.now(), 'evaluating boolean')\n",
    "\n",
    "start = datetime.now()\n",
    "print('MAP: {}'.format(sup.compute_map(lr_2, sens, features_2)))\n",
    "time(start, datetime.now(), 'computing the MAP score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Use selected features from RFECV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_3 = ['norm_diff_translated_words', 'abs_diff_num_words', 'abs_diff_num_punctuation',\n",
    "              'abs_diff_occ_question_mark', 'abs_diff_occ_exclamation_mark', 'norm_diff_num_words',\n",
    "              'euclidean_distance', 'cosine_similarity'\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- TIME: 19-05-2020 15:45:59 Computation time fitting the baseline model: 0:00:02.598792\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "lr_3 = LogisticRegression(random_state=42)\n",
    "lr_3.fit(train_data[features_3], train_data[label])\n",
    "time(start, datetime.now(), 'fitting the baseline model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9693517517517517\n",
      "Precision: 0.0030969229416534507\n",
      "Recall: 0.9519519519519519\n",
      "F1: 0.0061737611903479\n",
      "---- TIME: 19-05-2020 15:46:22 Computation time evaluating boolean: 0:00:22.996031\n",
      "---- INFO: Probabilities predicted\n",
      "---- INFO: Dataframe with evaluation ranking created\n",
      "---- INFO: Probabilities sorted for each query\n",
      "---- INFO: Index of ranking of true translation retrieved\n",
      "MAP: 0.6584851843421424\n",
      "---- TIME: 19-05-2020 15:57:00 Computation time computing the MAP score: 0:10:37.367608\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "sup = sup_model.SupModel()\n",
    "sup.evaluate_boolean(lr_3, sens, features_3)\n",
    "print('Accuracy: {}'.format(sup.accuracy))\n",
    "print('Precision: {}'.format(sup.precision))\n",
    "print('Recall: {}'.format(sup.recall))\n",
    "print('F1: {}'.format(sup.f1))\n",
    "time(start, datetime.now(), 'evaluating boolean')\n",
    "\n",
    "start = datetime.now()\n",
    "print('MAP: {}'.format(sup.compute_map(lr_3, sens, features_3)))\n",
    "time(start, datetime.now(), 'computing the MAP score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Use best of {1,2,3} + extracted elements of dimension reduced sentence embedding (10-dim) as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_4 = features_3 \\\n",
    "                + [f'src_embedding_pca_{i}' for i in range(10)] \\\n",
    "                + [f'trg_embedding_pca_{i}' for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_4 = LogisticRegression(random_state=42)\n",
    "lr_4.fit(train_data[features_4], train_data[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9721572572572572\n",
      "Precision: 0.0034649066230955554\n",
      "Recall: 0.9679679679679679\n",
      "F1: 0.006905095989403142\n",
      "---- TIME: 19-05-2020 17:43:37 Computation time evaluating boolean: 0:00:42.238030\n",
      "---- INFO: Probabilities predicted\n",
      "---- INFO: Dataframe with evaluation ranking created\n",
      "---- INFO: Probabilities sorted for each query\n",
      "---- INFO: Index of ranking of true translation retrieved\n",
      "MAP: 0.6601763795967865\n",
      "---- TIME: 19-05-2020 17:52:23 Computation time computing the MAP score: 0:08:46.540573\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "sup = sup_model.SupModel()\n",
    "sup.evaluate_boolean(lr_4, sens, features_4)\n",
    "print('Accuracy: {}'.format(sup.accuracy))\n",
    "print('Precision: {}'.format(sup.precision))\n",
    "print('Recall: {}'.format(sup.recall))\n",
    "print('F1: {}'.format(sup.f1))\n",
    "time(start, datetime.now(), 'evaluating boolean')\n",
    "\n",
    "start = datetime.now()\n",
    "print('MAP: {}'.format(sup.compute_map(lr_4, sens, features_4)))\n",
    "time(start, datetime.now(), 'computing the MAP score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) Correlation-reduced features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_5 = ['norm_diff_translated_words', 'abs_diff_occ_question_mark', 'abs_diff_occ_exclamation_mark',\n",
    "              'abs_diff_num_words', 'abs_diff_num_punctuation',\n",
    "              'cosine_similarity'\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_5 = LogisticRegression(random_state=42)\n",
    "lr_5.fit(train_data[features_5], train_data[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9673895895895895\n",
      "Precision: 0.0028928364797394302\n",
      "Recall: 0.9459459459459459\n",
      "F1: 0.005768033497320458\n",
      "---- TIME: 20-05-2020 10:35:46 Computation time evaluating boolean: 0:00:22.173894\n",
      "---- INFO: Probabilities predicted\n",
      "---- INFO: Dataframe with evaluation ranking created\n",
      "---- INFO: Probabilities sorted for each query\n",
      "---- INFO: Index of ranking of true translation retrieved\n",
      "MAP: 0.6843722994123714\n",
      "---- TIME: 20-05-2020 10:46:44 Computation time computing the MAP score: 0:10:57.214786\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now() \n",
    "sup = sup_model.SupModel()\n",
    "sup.evaluate_boolean(lr_5, sens, features_5)\n",
    "print('Accuracy: {}'.format(sup.accuracy))\n",
    "print('Precision: {}'.format(sup.precision))\n",
    "print('Recall: {}'.format(sup.recall))\n",
    "print('F1: {}'.format(sup.f1))\n",
    "time(start, datetime.now(), 'evaluating boolean')\n",
    "\n",
    "start = datetime.now()\n",
    "print('MAP: {}'.format(sup.compute_map(lr_5, sens, features_5)))\n",
    "time(start, datetime.now(), 'computing the MAP score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6) Correlation-reduced features + extracted elements of dimension reduced sentence embedding (10-dim) as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_6 = features_5 \\\n",
    "                + [f'src_embedding_pca_{i}' for i in range(10)] \\\n",
    "                + [f'trg_embedding_pca_{i}' for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_6 = LogisticRegression(random_state=42)\n",
    "lr_6.fit(train_data[features_5], train_data[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9636202202202202\n",
      "Precision: 0.002615681463244598\n",
      "Recall: 0.953953953953954\n",
      "F1: 0.005217058082881699\n",
      "---- TIME: 20-05-2020 11:22:06 Computation time evaluating boolean: 0:00:55.435638\n",
      "---- INFO: Probabilities predicted\n",
      "---- INFO: Dataframe with evaluation ranking created\n",
      "---- INFO: Probabilities sorted for each query\n",
      "---- INFO: Index of ranking of true translation retrieved\n",
      "MAP: 0.6781147670879518\n",
      "---- TIME: 20-05-2020 11:32:58 Computation time computing the MAP score: 0:10:51.552589\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now() \n",
    "sup = sup_model.SupModel()\n",
    "sup.evaluate_boolean(lr_6, sens, features_6)\n",
    "print('Accuracy: {}'.format(sup.accuracy))\n",
    "print('Precision: {}'.format(sup.precision))\n",
    "print('Recall: {}'.format(sup.recall))\n",
    "print('F1: {}'.format(sup.f1))\n",
    "time(start, datetime.now(), 'evaluating boolean')\n",
    "\n",
    "start = datetime.now()\n",
    "print('MAP: {}'.format(sup.compute_map(lr_6, sens, features_6)))\n",
    "time(start, datetime.now(), 'computing the MAP score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparametertuning of model with correlation-reduced features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create parameter grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Penalty\n",
    "penalty = ['l1', 'l2', 'elasticnet']\n",
    "\n",
    "# Regularization parameter C\n",
    "C = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "\n",
    "# Algorithm used in the optimization problem\n",
    "solver = ['newton-cg', 'lbfgs', 'sag', 'saga']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_grid = {'penalty': penalty,\n",
    "                  'C': C,\n",
    "                  'solver': solver\n",
    "                 }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform grid search for best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "lr = LogisticRegression(random_state=42)\n",
    "cv = StratifiedKFold(n_splits=5, random_state=42)\n",
    "grid_search = GridSearchCV(estimator=lr, param_grid=parameter_grid, n_jobs=-1, cv=cv, return_train_score=True, verbose=20)\n",
    "grid_search.fit(train_data[features_5], train_data[label])\n",
    "time(start, datetime.now(), 'performing the GridSearch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate grid search to identify optimal hyperparameters and resulting MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.01, 'penalty': 'l2', 'solver': 'newton-cg'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify best hyperparameters retrieved by grid search\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9455580227650083"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify best score achieved by best model in grid search\n",
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit best model on training data\n",
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- TIME: 20-05-2020 11:50:07 Computation time fitting the best model retrieved by grid search: 0:00:02.683978\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "best_model.fit(train_data[features_5], train_data[label])\n",
    "time(start, datetime.now(), 'fitting the best model retrieved by grid search')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9735627627627628\n",
      "Precision: 0.0035623044789186292\n",
      "Recall: 0.944944944944945\n",
      "F1: 0.007097851095505196\n",
      "---- TIME: 20-05-2020 11:50:37 Computation time evaluating boolean: 0:00:30.505540\n",
      "---- INFO: Probabilities predicted\n",
      "---- INFO: Dataframe with evaluation ranking created\n",
      "---- INFO: Probabilities sorted for each query\n",
      "---- INFO: Index of ranking of true translation retrieved\n",
      "MAP: 0.7482786619932548\n",
      "---- TIME: 20-05-2020 12:00:10 Computation time computing the MAP score: 0:09:32.126954\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "sup = sup_model.SupModel()\n",
    "sup.evaluate_boolean(best_model, sens, features_5)\n",
    "print('Accuracy: {}'.format(sup.accuracy))\n",
    "print('Precision: {}'.format(sup.precision))\n",
    "print('Recall: {}'.format(sup.recall))\n",
    "print('F1: {}'.format(sup.f1))\n",
    "time(start, datetime.now(), 'evaluating boolean')\n",
    "\n",
    "start = datetime.now()\n",
    "print('MAP: {}'.format(sup.compute_map(best_model, sens, features_5)))\n",
    "time(start, datetime.now(), 'computing the MAP score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_features_ = ['translated_words', 'num_words', 'num_punctuation',\n",
    "                     'occ_question_mark', 'occ_exclamation_mark']\n",
    "\n",
    "features_dict_ = {'text_based': ['norm_diff_translated_words', 'abs_diff_occ_question_mark', 'abs_diff_occ_exclamation_mark',\n",
    "                                 'abs_diff_num_words', 'abs_diff_num_punctuation'\n",
    "                                ],\n",
    "                  'vector_based': 'cosine_similarity'\n",
    "                 }\n",
    "\n",
    "info = 'Logistic Regression Model 5, averaged'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sup_model.SupModel.save_model(name='log_reg_best_avg', model=best_model, prepared_features=prepared_features_, features_dict=features_dict_, info=info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
