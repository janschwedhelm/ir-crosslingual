{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import importlib, os, math\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'ir_crosslingual.features.element_based' from '/Users/i500969/Desktop/Admin/Uni-Mannheim/02_Courses/2020_FSS/Information-Retrieval/03_Project/03_Implementation/06_Logistic-Regression/ir-crosslingual/ir_crosslingual/features/element_based.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ir_crosslingual.utils import paths\n",
    "importlib.reload(paths)\n",
    "\n",
    "from ir_crosslingual.features import text_based\n",
    "importlib.reload(text_based)\n",
    "\n",
    "from ir_crosslingual.features import vector_based\n",
    "importlib.reload(vector_based)\n",
    "\n",
    "from ir_crosslingual.supervised_classification import sup_model\n",
    "importlib.reload(sup_model)\n",
    "\n",
    "from ir_crosslingual.sentences import sentences\n",
    "importlib.reload(sentences)\n",
    "\n",
    "from ir_crosslingual.features import element_based\n",
    "importlib.reload(element_based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time(start, stop, message):\n",
    "    print(f\"---- TIME: {datetime.now().strftime('%d-%m-%Y %H:%M:%S')} Computation time {message}: {stop-start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- INFO: Learn projection matrix for en-de\n",
      "---- INFO: Found 13700 valid translation pairs in expert dictionary.\n",
      "---- INFO: 977 other pairs contained at least one unknown word (0 in source language, 977 in target language).\n",
      "---- DONE: Seed dictionary extracted for the languages: en-de\n",
      "---- INFO: Resulting subspace dimension: (13700, 300)\n",
      "---- INFO: Resulting subspace dimension: (13700, 300)\n",
      "---- DONE: Projection matrix learned from en to de\n",
      "---- INFO: Learn projection matrix for de-en\n",
      "---- INFO: Found 10604 valid translation pairs in expert dictionary.\n",
      "---- INFO: 262 other pairs contained at least one unknown word (0 in source language, 262 in target language).\n",
      "---- DONE: Seed dictionary extracted for the languages: de-en\n",
      "---- INFO: Resulting subspace dimension: (10604, 300)\n",
      "---- INFO: Resulting subspace dimension: (10604, 300)\n",
      "---- DONE: Projection matrix learned from de to en\n",
      "---- INFO: File loaded containing training data\n",
      "---- INFO: File loaded containing test collection\n",
      "---- DONE: All files loaded and features extracted\n",
      "---- TIME: 20-05-2020 01:56:19 Computation time loading data from file: 0:00:56.484800\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "train_file = f'{paths.data_path}extracted_data/global/en-de/training_data_tfidf.pkl'\n",
    "test_file = f'{paths.data_path}extracted_data/global/en-de/test_collection_tfidf.pkl'\n",
    "sens, train_data, test_collection, features = sentences.Sentences.load_from_file(train_file, test_file)\n",
    "stop = datetime.now()\n",
    "time(start, stop, 'loading data from file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- INFO: Unique queries extracted\n",
      "---- INFO: Unique documents extracted\n",
      "---- INFO: Started extraction for src language.\n",
      "---- INFO: src_embedding_pca elements extracted for train data.\n",
      "---- INFO: src_embedding_pca elements extracted for unique queries.\n",
      "---- INFO: Unique queries merged to test collection\n",
      "---- INFO: Started extraction for trg language.\n",
      "---- INFO: trg_embedding_pca elements extracted for train data.\n",
      "---- INFO: trg_embedding_pca elements extracted for unique documents.\n",
      "---- INFO: Unique documents merged to test collection\n",
      "---- DONE: Extracted all vector elements and merged to test collection\n",
      "---- TIME: 20-05-2020 01:57:31 Computation time extracting vector elements: 0:01:11.946094\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "sens = element_based.vec2features(sens, 10)\n",
    "time(start, datetime.now(), 'extracting vector elements')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove pairs that contain sentences equal to '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sens.train_data[(sens.train_data['src_sentence'] != '.') & (sens.train_data['trg_sentence'] != '.')]\n",
    "test_collection = sens.test_collection[(sens.test_collection['src_sentence'] != '.') & (sens.test_collection['trg_sentence'] != '.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sens.train_data = train_data\n",
    "sens.test_collection = test_collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline LR Models using \"tfidf\" aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 'translation'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) All text/vector-based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_1 = list(text_based.FEATURES.keys()) + list(vector_based.FEATURES.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- TIME: 20-05-2020 04:04:07 Computation time fitting the baseline model: 0:00:03.230510\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "lr_1 = LogisticRegression(random_state=42)\n",
    "lr_1.fit(train_data[features_1], train_data[label])\n",
    "time(start, datetime.now(), 'fitting the baseline model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9742427427427427\n",
      "Precision: 0.0037213733067945074\n",
      "Recall: 0.9619619619619619\n",
      "F1: 0.0074140651218768915\n",
      "---- TIME: 19-05-2020 16:38:49 Computation time evaluating boolean: 0:00:16.263245\n",
      "---- INFO: Probabilities predicted\n",
      "---- INFO: Dataframe with evaluation ranking created\n",
      "---- INFO: Probabilities sorted for each query\n",
      "---- INFO: Index of ranking of true translation retrieved\n",
      "MAP: 0.6565315849754441\n",
      "---- TIME: 19-05-2020 16:48:33 Computation time computing the MAP score: 0:09:44.257536\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "sup = sup_model.SupModel()\n",
    "sup.evaluate_boolean(lr_1, sens, features_1)\n",
    "print('Accuracy: {}'.format(sup.accuracy))\n",
    "print('Precision: {}'.format(sup.precision))\n",
    "print('Recall: {}'.format(sup.recall))\n",
    "print('F1: {}'.format(sup.f1))\n",
    "time(start, datetime.now(), 'evaluating boolean')\n",
    "\n",
    "start = datetime.now()\n",
    "print('MAP: {}'.format(sup.compute_map(lr_1, sens, features_1)))\n",
    "time(start, datetime.now(), 'computing the MAP score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Self-selected features based on intuition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_2 = ['norm_diff_translated_words', 'norm_diff_num_words', 'norm_diff_num_punctuation',\n",
    "              'abs_diff_occ_question_mark', 'abs_diff_occ_exclamation_mark',\n",
    "              'euclidean_distance', 'cosine_similarity'\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- TIME: 19-05-2020 16:48:35 Computation time fitting the baseline model: 0:00:01.368845\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "lr_2 = LogisticRegression(random_state=42)\n",
    "lr_2.fit(train_data[features_2], train_data[label])\n",
    "time(start, datetime.now(), 'fitting the baseline model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9657991991991992\n",
      "Precision: 0.002764471145285073\n",
      "Recall: 0.9479479479479479\n",
      "F1: 0.005512865292816392\n",
      "---- TIME: 19-05-2020 16:48:55 Computation time evaluating boolean: 0:00:20.346357\n",
      "---- INFO: Probabilities predicted\n",
      "---- INFO: Dataframe with evaluation ranking created\n",
      "---- INFO: Probabilities sorted for each query\n",
      "---- INFO: Index of ranking of true translation retrieved\n",
      "MAP: 0.4491584744319259\n",
      "---- TIME: 19-05-2020 16:57:50 Computation time computing the MAP score: 0:08:55.198464\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "sup = sup_model.SupModel()\n",
    "sup.evaluate_boolean(lr_2, sens, features_2)\n",
    "print('Accuracy: {}'.format(sup.accuracy))\n",
    "print('Precision: {}'.format(sup.precision))\n",
    "print('Recall: {}'.format(sup.recall))\n",
    "print('F1: {}'.format(sup.f1))\n",
    "time(start, datetime.now(), 'evaluating boolean')\n",
    "\n",
    "start = datetime.now()\n",
    "print('MAP: {}'.format(sup.compute_map(lr_2, sens, features_2)))\n",
    "time(start, datetime.now(), 'computing the MAP score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Use selected features from RFECV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_3 = ['norm_diff_translated_words', 'abs_diff_num_words', 'abs_diff_num_punctuation',\n",
    "              'abs_diff_occ_question_mark', 'abs_diff_occ_exclamation_mark', 'norm_diff_num_words',\n",
    "              'euclidean_distance', 'cosine_similarity'\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- TIME: 19-05-2020 16:57:54 Computation time fitting the baseline model: 0:00:03.931974\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "lr_3 = LogisticRegression(random_state=42)\n",
    "lr_3.fit(train_data[features_3], train_data[label])\n",
    "time(start, datetime.now(), 'fitting the baseline model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9725983983983983\n",
      "Precision: 0.003506038176860148\n",
      "Recall: 0.963963963963964\n",
      "F1: 0.006986665118911154\n",
      "---- TIME: 19-05-2020 16:58:18 Computation time evaluating boolean: 0:00:24.021501\n",
      "---- INFO: Probabilities predicted\n",
      "---- INFO: Dataframe with evaluation ranking created\n",
      "---- INFO: Probabilities sorted for each query\n",
      "---- INFO: Index of ranking of true translation retrieved\n",
      "MAP: 0.6202932314241427\n",
      "---- TIME: 19-05-2020 17:07:36 Computation time computing the MAP score: 0:09:17.383051\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "sup = sup_model.SupModel()\n",
    "sup.evaluate_boolean(lr_3, sens, features_3)\n",
    "print('Accuracy: {}'.format(sup.accuracy))\n",
    "print('Precision: {}'.format(sup.precision))\n",
    "print('Recall: {}'.format(sup.recall))\n",
    "print('F1: {}'.format(sup.f1))\n",
    "time(start, datetime.now(), 'evaluating boolean')\n",
    "\n",
    "start = datetime.now()\n",
    "print('MAP: {}'.format(sup.compute_map(lr_3, sens, features_3)))\n",
    "time(start, datetime.now(), 'computing the MAP score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Use best of {1,2,3} + extracted elements of dimension reduced sentence embedding (10-dim) as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_4 = list(text_based.FEATURES.keys()) + list(vector_based.FEATURES.keys()) \\\n",
    "                + [f'src_embedding_pca_{i}' for i in range(10)] \\\n",
    "                + [f'trg_embedding_pca_{i}' for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_4 = LogisticRegression(random_state=42)\n",
    "lr_4.fit(train_data[features_4], train_data[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9741466466466466\n",
      "Precision: 0.0037037608605071065\n",
      "Recall: 0.960960960960961\n",
      "F1: 0.0073790810738100275\n",
      "---- TIME: 20-05-2020 02:11:42 Computation time evaluating boolean: 0:00:37.812396\n",
      "---- INFO: Probabilities predicted\n",
      "---- INFO: Dataframe with evaluation ranking created\n",
      "---- INFO: Probabilities sorted for each query\n",
      "---- INFO: Index of ranking of true translation retrieved\n",
      "MAP: 0.6301510088951957\n",
      "---- TIME: 20-05-2020 02:21:16 Computation time computing the MAP score: 0:09:34.281931\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "sup = sup_model.SupModel()\n",
    "sup.evaluate_boolean(lr_4, sens, features_4)\n",
    "print('Accuracy: {}'.format(sup.accuracy))\n",
    "print('Precision: {}'.format(sup.precision))\n",
    "print('Recall: {}'.format(sup.recall))\n",
    "print('F1: {}'.format(sup.f1))\n",
    "time(start, datetime.now(), 'evaluating boolean')\n",
    "\n",
    "start = datetime.now()\n",
    "print('MAP: {}'.format(sup.compute_map(lr_4, sens, features_4)))\n",
    "time(start, datetime.now(), 'computing the MAP score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) Correlation-reduced features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[21.97079138, -0.2840837 , -0.23724321,  1.31774805,  0.20552533,\n",
       "         0.03726664, -0.04395643, -0.23821477,  0.0966708 ,  4.3831724 ,\n",
       "        23.46416402]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_1.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['norm_diff_translated_words',\n",
       " 'abs_diff_num_words',\n",
       " 'abs_diff_num_punctuation',\n",
       " 'abs_diff_occ_question_mark',\n",
       " 'abs_diff_occ_exclamation_mark',\n",
       " 'rel_diff_num_words',\n",
       " 'rel_diff_num_punctuation',\n",
       " 'norm_diff_num_words',\n",
       " 'norm_diff_num_punctuation',\n",
       " 'euclidean_distance',\n",
       " 'cosine_similarity']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_5 = ['norm_diff_translated_words', 'abs_diff_occ_question_mark', 'abs_diff_occ_exclamation_mark',\n",
    "              'abs_diff_num_words', 'abs_diff_num_punctuation',\n",
    "              'cosine_similarity'\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_5 = LogisticRegression(random_state=42)\n",
    "lr_5.fit(train_data[features_5], train_data[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9686908908908909\n",
      "Precision: 0.003012729979691968\n",
      "Recall: 0.9459459459459459\n",
      "F1: 0.006006330481650501\n",
      "---- TIME: 20-05-2020 04:07:46 Computation time evaluating boolean: 0:00:34.143862\n",
      "---- INFO: Probabilities predicted\n",
      "---- INFO: Dataframe with evaluation ranking created\n",
      "---- INFO: Probabilities sorted for each query\n",
      "---- INFO: Index of ranking of true translation retrieved\n",
      "MAP: 0.7074447005417038\n",
      "---- TIME: 20-05-2020 04:17:39 Computation time computing the MAP score: 0:09:52.397639\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now() \n",
    "sup = sup_model.SupModel()\n",
    "sup.evaluate_boolean(lr_5, sens, features_5)\n",
    "print('Accuracy: {}'.format(sup.accuracy))\n",
    "print('Precision: {}'.format(sup.precision))\n",
    "print('Recall: {}'.format(sup.recall))\n",
    "print('F1: {}'.format(sup.f1))\n",
    "time(start, datetime.now(), 'evaluating boolean')\n",
    "\n",
    "start = datetime.now()\n",
    "print('MAP: {}'.format(sup.compute_map(lr_5, sens, features_5)))\n",
    "time(start, datetime.now(), 'computing the MAP score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6) Correlation-reduced features + extracted elements of dimension reduced sentence embedding (10-dim) as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_6 = features_5 \\\n",
    "                + [f'src_embedding_pca_{i}' for i in range(10)] \\\n",
    "                + [f'trg_embedding_pca_{i}' for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_6 = LogisticRegression(random_state=42)\n",
    "lr_6.fit(train_data[features_6], train_data[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.969034934934935\n",
      "Precision: 0.003068532712750527\n",
      "Recall: 0.9529529529529529\n",
      "F1: 0.006117367347266623\n",
      "---- TIME: 20-05-2020 11:20:31 Computation time evaluating boolean: 0:00:37.516193\n",
      "---- INFO: Probabilities predicted\n",
      "---- INFO: Dataframe with evaluation ranking created\n",
      "---- INFO: Probabilities sorted for each query\n",
      "---- INFO: Index of ranking of true translation retrieved\n",
      "MAP: 0.695793385447464\n",
      "---- TIME: 20-05-2020 11:33:08 Computation time computing the MAP score: 0:12:36.698118\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now() \n",
    "sup = sup_model.SupModel()\n",
    "sup.evaluate_boolean(lr_6, sens, features_6)\n",
    "print('Accuracy: {}'.format(sup.accuracy))\n",
    "print('Precision: {}'.format(sup.precision))\n",
    "print('Recall: {}'.format(sup.recall))\n",
    "print('F1: {}'.format(sup.f1))\n",
    "time(start, datetime.now(), 'evaluating boolean')\n",
    "\n",
    "start = datetime.now()\n",
    "print('MAP: {}'.format(sup.compute_map(lr_6, sens, features_6)))\n",
    "time(start, datetime.now(), 'computing the MAP score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparametertuning of model with correlation-reduced features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create parameter grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Penalty\n",
    "penalty = ['l1', 'l2', 'elasticnet']\n",
    "\n",
    "# Regularization parameter C\n",
    "C = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "\n",
    "# Algorithm used in the optimization problem\n",
    "solver = ['newton-cg', 'lbfgs', 'sag', 'saga']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_grid = {'penalty': penalty,\n",
    "                  'C': C,\n",
    "                  'solver': solver\n",
    "                 }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform grid search for best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "lr = LogisticRegression(random_state=42)\n",
    "cv = StratifiedKFold(n_splits=5, random_state=42)\n",
    "grid_search = GridSearchCV(estimator=lr, param_grid=parameter_grid, n_jobs=-1, cv=cv, return_train_score=True, verbose=20)\n",
    "grid_search.fit(train_data[features_5], train_data[label])\n",
    "time(start, datetime.now(), 'performing the GridSearch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate grid search to identify optimal hyperparameters and resulting MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.01, 'penalty': 'l2', 'solver': 'lbfgs'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify best hyperparameters retrieved by grid search\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9474344355758267"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify best score achieved by best model in grid search\n",
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit best model on training data\n",
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- TIME: 20-05-2020 11:05:18 Computation time fitting the best model retrieved by grid search: 0:00:02.749947\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "best_model_.fit(train_data[features_5], train_data[label])\n",
    "time(start, datetime.now(), 'fitting the best model retrieved by grid search')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.968468968968969\n",
      "Precision: 0.0029915855720101556\n",
      "Recall: 0.9459459459459459\n",
      "F1: 0.00596430881865661\n",
      "---- TIME: 20-05-2020 11:05:54 Computation time evaluating boolean: 0:00:35.629633\n",
      "---- INFO: Probabilities predicted\n",
      "---- INFO: Dataframe with evaluation ranking created\n",
      "---- INFO: Probabilities sorted for each query\n",
      "---- INFO: Index of ranking of true translation retrieved\n",
      "MAP: 0.7039245191121866\n",
      "---- TIME: 20-05-2020 11:15:23 Computation time computing the MAP score: 0:09:29.715826\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "sup = sup_model.SupModel()\n",
    "sup.evaluate_boolean(best_model, sens, features_5)\n",
    "print('Accuracy: {}'.format(sup.accuracy))\n",
    "print('Precision: {}'.format(sup.precision))\n",
    "print('Recall: {}'.format(sup.recall))\n",
    "print('F1: {}'.format(sup.f1))\n",
    "time(start, datetime.now(), 'evaluating boolean')\n",
    "\n",
    "start = datetime.now()\n",
    "print('MAP: {}'.format(sup.compute_map(best_model, sens, features_5)))\n",
    "time(start, datetime.now(), 'computing the MAP score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_features_ = ['translated_words', 'num_words', 'num_punctuation',\n",
    "                     'occ_question_mark', 'occ_exclamation_mark']\n",
    "\n",
    "features_dict_ = {'text_based': ['norm_diff_translated_words', 'abs_diff_occ_question_mark', 'abs_diff_occ_exclamation_mark',\n",
    "                                 'abs_diff_num_words', 'abs_diff_num_punctuation'\n",
    "                                ],\n",
    "                  'vector_based': 'cosine_similarity'\n",
    "                 }\n",
    "\n",
    "info = 'Logistic Regression Model 5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "sup_model.SupModel.save_model(name='log_reg_best', model=lr_6, prepared_features=prepared_features_, features_dict=features_dict_, info=info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
