{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib, re, os, math\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'ir_crosslingual.sentences.sentences' from '/Users/i500969/Desktop/Admin/Uni-Mannheim/02_Courses/2020_FSS/Information-Retrieval/03_Project/03_Implementation/06_Documents/ir-crosslingual/ir_crosslingual/sentences/sentences.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ir_crosslingual.utils import paths\n",
    "importlib.reload(paths)\n",
    "\n",
    "from ir_crosslingual.features import text_based\n",
    "importlib.reload(text_based)\n",
    "\n",
    "from ir_crosslingual.features import vector_based\n",
    "importlib.reload(vector_based)\n",
    "\n",
    "from ir_crosslingual.embeddings import embeddings\n",
    "importlib.reload(embeddings)\n",
    "\n",
    "from ir_crosslingual.sentences import sentences\n",
    "importlib.reload(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time(start, stop, message):\n",
    "    print(f'---- TIME {datetime.now()}: Computation time {message}: {stop-start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_duplicate_beginnings(data):\n",
    "    tokenized_data = [re.findall(r\"\\w+|[^\\w\\s]\", sen.lower(), re.UNICODE) for sen in data]\n",
    "    tokenized_data = [tokens[1:] if (len(tokens) > 1) and (tokens[0] == tokens[1]) else tokens for tokens in tokenized_data]\n",
    "    return [' '.join(tokens) for tokens in tokenized_data] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_queries(dataset):\n",
    "    start = datetime.now()\n",
    "    german_ids, queries = list(), list()\n",
    "    with open(f'{paths.data_path}wikiclir_v1/{dataset}.queries') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            split = line.split('\\t')\n",
    "            german_ids.append(split[0])\n",
    "            queries.append(split[1])\n",
    "    time(start, datetime.now(), f'loading queries on {dataset} data')\n",
    "    return pd.DataFrame({'german_id': german_ids, 'query': delete_duplicate_beginnings(queries)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_docs(dataset):\n",
    "    start = datetime.now()\n",
    "    english_ids, documents = list(), list()\n",
    "    with open(f'{paths.data_path}wikiclir_v1/{dataset}.docs') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            split = line.split('\\t')\n",
    "            english_ids.append(split[0])\n",
    "            documents.append(split[1])\n",
    "    time(start, datetime.now(), f'loading documents on {dataset} data')\n",
    "    return pd.DataFrame({'english_id': english_ids, 'document': delete_duplicate_beginnings(documents)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qrels(dataset):\n",
    "    start = datetime.now()\n",
    "    rel_german_ids, rel_english_ids, relevance_levels = list(), list(), list()\n",
    "    with open(f'{paths.data_path}wikiclir_v1/{dataset}.qrel') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            split = line.split('\\t')\n",
    "            rel_german_ids.append(split[0])\n",
    "            rel_english_ids.append(split[2])\n",
    "            relevance_levels.append(split[3][0])\n",
    "    time(start, datetime.now(), f'loading qrels on {dataset} data')\n",
    "    return pd.DataFrame({'german_id': rel_german_ids, 'english_id': rel_english_ids, 'relevance_level': relevance_levels})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- TIME 2020-05-24 07:40:33.058566: Computation time loading queries on train data: 0:00:00.343730\n",
      "---- TIME 2020-05-24 07:40:45.398901: Computation time loading documents on train data: 0:00:08.699685\n",
      "---- TIME 2020-05-24 08:05:36.902874: Computation time loading qrels on train data: 0:00:02.960174\n"
     ]
    }
   ],
   "source": [
    "queries_train = get_queries('train')\n",
    "docs_train = get_docs('train')\n",
    "qrel_train = get_qrels('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- TIME 2020-05-24 08:05:38.645653: Computation time loading queries on dev data: 0:00:00.030387\n",
      "---- TIME 2020-05-24 08:05:40.081606: Computation time loading documents on dev data: 0:00:01.186323\n",
      "---- TIME 2020-05-24 08:06:01.036076: Computation time loading qrels on dev data: 0:00:00.082432\n"
     ]
    }
   ],
   "source": [
    "queries_dev = get_queries('dev')\n",
    "docs_dev = get_docs('dev')\n",
    "qrel_dev = get_qrels('dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- TIME 2020-05-24 08:06:01.100489: Computation time loading queries on test data: 0:00:00.016515\n",
      "---- TIME 2020-05-24 08:06:01.981791: Computation time loading documents on test data: 0:00:00.741527\n",
      "---- TIME 2020-05-24 08:06:24.704552: Computation time loading qrels on test data: 0:00:00.083691\n"
     ]
    }
   ],
   "source": [
    "queries_test = get_queries('test')\n",
    "docs_test = get_docs('test')\n",
    "qrel_test = get_qrels('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.merge(left=queries_train, right=qrel_train, on='german_id').merge(docs_train, on='english_id')\n",
    "dev_data = pd.merge(left=queries_dev, right=qrel_dev, on='german_id').merge(docs_dev, on='english_id')\n",
    "test_data = pd.merge(left=queries_test, right=qrel_test, on='german_id').merge(docs_test, on='english_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in [train_data, dev_data, test_data]:\n",
    "    dataset['relevance_level'].replace('3', 1, inplace=True)\n",
    "    dataset['relevance_level'].replace('2', 0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of queries in training data: 225294\n",
      "Number of queries in dev data: 10000\n",
      "Number of queries in test data: 10000\n"
     ]
    }
   ],
   "source": [
    "print('Number of queries in training data: {}'.format(len(train_data[train_data['relevance_level'] == 1])))\n",
    "print('Number of queries in dev data: {}'.format(len(dev_data[dev_data['relevance_level'] == 1])))\n",
    "print('Number of queries in test data: {}'.format(len(test_data[test_data['relevance_level'] == 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'das ( auch vektorprodukt , vektorielles produkt oder äußeres produkt genannt ) ist eine verknüpfung im euklidischen vektorraum , die im dreidimensionalen fall zwei vektoren wieder einen vektor zuordnet .'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[train_data['relevance_level'] == 1]['query'].iloc[1242]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cross product in mathematics the cross product or vector product is a binary operation on two vectors in three dimensional space it results in a vector which is perpendicular to both and therefore normal to the plane containing them it has many applications in mathematics physics and engineering if the vectors have the same direction or one has zero length then their cross product is zero more generally the magnitude of the product equals the area of a parallelogram with the vectors for sides in particular for perpendicular vectors this is a rectangle and the magnitude of the product is the product of their lengths the cross product is anticommutative distributive over addition and satisfies the jacobi identity the space and product form an algebra over a field which is neither commutative nor associative but is a lie algebra with the cross product being the lie bracket like the dot product it depends on the metric of euclidean space but unlike the dot product it also depends on the choice of orientation or handedness the product can be generalized in various ways it can be made independent of orientation by changing the result to pseudovector or in arbitrary dimensions the exterior product of vectors can be used with a bivector or two form result also using the orientation and metric structure just as for the traditional 3 dimensional cross product one can in n dimensions take the product of n − 1 vectors to produce a vector perpendicular to all of them but if the product is limited to non trivial binary products with vector results it exists only in three and seven dimensions definition the cross product of two vectors a and b is denoted by in physics sometimes the notation is used though this is avoided in mathematics to avoid confusion with the exterior product the cross product a × b is defined as a vector c that is perpendicular to both a and b with a direction given by the right hand rule and a magnitude equal to the area of the parallelogram that the vectors span the cross product is defined by the formula \\\\ mathbf a \\\\ times \\\\ mathbf b \\\\ left \\\\ \\\\ mathbf a \\\\ right \\\\ \\\\ left \\\\ \\\\ mathbf b \\\\ right \\\\ \\\\ sin \\\\ theta \\\\ mathbf n where θ is the measure of the smaller angle between a and b 0 ° ≤ θ ≤ 180 ° ‖ a ‖ and ‖ b ‖ are the magnitudes of vectors a and b and n is a unit vector perpendicular to the plane containing a and b in the direction given by the right hand rule as illustrated if the vectors a and b are parallel i . e . the angle θ between them is either 0 ° or 180 ° by the above formula the cross product of a and b is the zero vector 0 . the direction of the vector n is given by the right hand rule where one simply points the forefinger of the right hand'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[train_data['relevance_level'] == 1]['document'].iloc[1242]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop_duplicates(['german_id', 'relevance_level'], inplace=True, ignore_index=True)\n",
    "test_data = test_data[test_data['relevance_level'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_translations = list(train_data['relevance_level'])\n",
    "dev_translations = list(dev_data['relevance_level'])\n",
    "test_translations = list(test_data['relevance_level'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_queries = list(train_data['query'])\n",
    "train_documents = list(train_data['document'])\n",
    "test_queries = list(test_data['query'])\n",
    "test_documents = list(test_data['document'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of queries in train dataset: 433966\n",
      "Number of documents in train dataset: 433966\n",
      "--------------------------------------------------\n",
      "Number of queries in test dataset: 10000\n",
      "Number of documents in test dataset: 10000\n"
     ]
    }
   ],
   "source": [
    "print('Number of queries in train dataset: {}'.format(len(train_queries)))\n",
    "print('Number of documents in train dataset: {}'.format(len(train_documents)))\n",
    "print('-' * 50)\n",
    "print('Number of queries in test dataset: {}'.format(len(test_queries)))\n",
    "print('Number of documents in test dataset: {}'.format(len(test_documents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    225294\n",
       "0    208672\n",
       "Name: relevance_level, dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['relevance_level'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "german = embeddings.WordEmbeddings('de')\n",
    "german.load_embeddings()\n",
    "\n",
    "english = embeddings.WordEmbeddings('en')\n",
    "english.load_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- INFO: Learn projection matrix for en-de\n",
      "---- INFO: Found 13700 valid translation pairs in expert dictionary.\n",
      "---- INFO: 977 other pairs contained at least one unknown word (0 in source language, 977 in target language).\n",
      "---- DONE: Seed dictionary extracted for the languages: en-de\n",
      "---- INFO: Resulting subspace dimension: (13700, 300)\n",
      "---- INFO: Resulting subspace dimension: (13700, 300)\n",
      "---- DONE: Projection matrix learned from en to de\n",
      "---- INFO: Learn projection matrix for de-en\n",
      "---- INFO: Found 10604 valid translation pairs in expert dictionary.\n",
      "---- INFO: 262 other pairs contained at least one unknown word (0 in source language, 262 in target language).\n",
      "---- DONE: Seed dictionary extracted for the languages: de-en\n",
      "---- INFO: Resulting subspace dimension: (10604, 300)\n",
      "---- INFO: Resulting subspace dimension: (10604, 300)\n",
      "---- DONE: Projection matrix learned from de to en\n"
     ]
    }
   ],
   "source": [
    "W_ende, W_deen = embeddings.WordEmbeddings.learn_projection_matrix(src_lang='en', trg_lang='de')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load sentence embeddings for training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- DONE: Target sentences loaded\n",
      "---- DONE: Source sentences loaded\n",
      "---- DONE: Sentences preprocessed\n",
      "---- INFO: Sentences embeddings extracted in de\n",
      "---- INFO: Sentences embeddings extracted in en\n",
      "---- INFO: Extracted word embeddings of found words\n",
      "---- DONE: Sentences transformed\n",
      "---- INFO: Embedding space of source language transformed according to projection matrix\n",
      "---- DONE: Source words extracted\n",
      "---- DONE: Target words extracted\n",
      "---- INFO: Embeddings of found words added as a column\n",
      "---- INFO: Start preparation of text-based feature translated_words\n",
      "---- INFO: Start preparation of text-based feature num_words\n",
      "---- INFO: Start preparation of text-based feature num_punctuation\n",
      "---- INFO: Start preparation of text-based feature occ_question_mark\n",
      "---- INFO: Start preparation of text-based feature occ_exclamation_mark\n",
      "---- DONE: All features prepared\n",
      "---- INFO: Dropped duplicates\n",
      "---- INFO: Delete sentences containing only a '.'\n",
      "---- DONE: Data loaded. Length of dataset after preprocessing and duplicate handling: 427285\n",
      "---- TIME 2020-05-24 11:56:56.843072: Computation time loading sentences object: 0:53:24.159230\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "train_sens = sentences.Sentences(src_words=german, trg_words=english)\n",
    "prepared_features = list(text_based.PREPARED_FEATURES.keys())\n",
    "train_data = train_sens.load_data(src_sentences=train_queries, trg_sentences=train_documents, single_source=False, features=prepared_features, agg_method='average', documents=True)\n",
    "time(start, datetime.now(), 'loading sentences object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src_embedding</th>\n",
       "      <th>src_embedding_aligned</th>\n",
       "      <th>trg_embedding</th>\n",
       "      <th>src_sentence</th>\n",
       "      <th>trg_sentence</th>\n",
       "      <th>src_preprocessed</th>\n",
       "      <th>trg_preprocessed</th>\n",
       "      <th>src_words</th>\n",
       "      <th>trg_words</th>\n",
       "      <th>src_words_found_embedding</th>\n",
       "      <th>trg_words_found_embedding</th>\n",
       "      <th>src_translated_words</th>\n",
       "      <th>trg_translated_words</th>\n",
       "      <th>src_num_words</th>\n",
       "      <th>trg_num_words</th>\n",
       "      <th>src_num_punctuation</th>\n",
       "      <th>trg_num_punctuation</th>\n",
       "      <th>src_occ_question_mark</th>\n",
       "      <th>trg_occ_question_mark</th>\n",
       "      <th>src_occ_exclamation_mark</th>\n",
       "      <th>trg_occ_exclamation_mark</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-0.14160268235294118, 0.22068174705882354, -0...</td>\n",
       "      <td>[-0.12276546643300858, 0.047733963953612756, -...</td>\n",
       "      <td>[-0.08721950426540277, -0.07013083881516587, -...</td>\n",
       "      <td>steht als pseudonym für einen fiktiven regisse...</td>\n",
       "      <td>alan smithee alan smithee also allen smithee w...</td>\n",
       "      <td>[steht, pseudonym, fiktiven, regisseur, ,, fil...</td>\n",
       "      <td>[alan, smithee, alan, smithee, also, allen, sm...</td>\n",
       "      <td>[steht, pseudonym, fiktiven, regisseur, filme,...</td>\n",
       "      <td>[alan, smithee, alan, smithee, also, allen, sm...</td>\n",
       "      <td>[[-0.06589690229429432, 0.11700426091010997, -...</td>\n",
       "      <td>[[0.22971, -0.28394, 0.070427, -0.074201, -0.0...</td>\n",
       "      <td>[stands, alias, pseudonym, films, movies, name...</td>\n",
       "      <td>[alan, alan, ausserdem, ebenso, außerdem, eben...</td>\n",
       "      <td>14</td>\n",
       "      <td>236</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-0.09725738031250002, 0.10076061875, -0.10252...</td>\n",
       "      <td>[-0.13914918744447388, 0.016411580786968003, -...</td>\n",
       "      <td>[-0.08721950426540277, -0.07013083881516587, -...</td>\n",
       "      <td>das ( el - latn ‚ tarn - ' , ‚ deckname ' ) is...</td>\n",
       "      <td>alan smithee alan smithee also allen smithee w...</td>\n",
       "      <td>[(, el, -, latn, ‚, tarn, -, ', ,, ‚, deckname...</td>\n",
       "      <td>[alan, smithee, alan, smithee, also, allen, sm...</td>\n",
       "      <td>[el, latn, tarn, deckname, fingierter, name, m...</td>\n",
       "      <td>[alan, smithee, alan, smithee, also, allen, sm...</td>\n",
       "      <td>[[-0.15780262235327164, -0.1363245252964868, 0...</td>\n",
       "      <td>[[0.22971, -0.28394, 0.070427, -0.074201, -0.0...</td>\n",
       "      <td>[name, authors, creator, copyright, identities...</td>\n",
       "      <td>[alan, alan, ausserdem, ebenso, außerdem, eben...</td>\n",
       "      <td>21</td>\n",
       "      <td>236</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-0.024419767999999998, 0.2503159047619048, -0...</td>\n",
       "      <td>[-0.07746652968120009, -0.0106397692086347, -0...</td>\n",
       "      <td>[-0.08721950426540277, -0.07013083881516587, -...</td>\n",
       "      <td>der ( engl . film editing ) , oft auch synonym...</td>\n",
       "      <td>alan smithee alan smithee also allen smithee w...</td>\n",
       "      <td>[(, engl, ., film, editing, ), ,, oft, synonym...</td>\n",
       "      <td>[alan, smithee, alan, smithee, also, allen, sm...</td>\n",
       "      <td>[engl, film, editing, oft, synonym, montage, b...</td>\n",
       "      <td>[alan, smithee, alan, smithee, also, allen, sm...</td>\n",
       "      <td>[[-0.15780262235327164, -0.1363245252964868, 0...</td>\n",
       "      <td>[[0.22971, -0.28394, 0.070427, -0.074201, -0.0...</td>\n",
       "      <td>[engl, movie, film, frequently, oft, often, fr...</td>\n",
       "      <td>[alan, alan, ausserdem, ebenso, außerdem, eben...</td>\n",
       "      <td>15</td>\n",
       "      <td>236</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-0.3159817777777778, 0.2578707777777778, -0.4...</td>\n",
       "      <td>[-0.12345786112681063, -0.1265453990910237, -0...</td>\n",
       "      <td>[-0.08721950426540277, -0.07013083881516587, -...</td>\n",
       "      <td>der originaltitel : dune ist ein us - amerikan...</td>\n",
       "      <td>alan smithee alan smithee also allen smithee w...</td>\n",
       "      <td>[originaltitel, :, dune, us, -, amerikanischer...</td>\n",
       "      <td>[alan, smithee, alan, smithee, also, allen, sm...</td>\n",
       "      <td>[originaltitel, dune, us, amerikanischer, scie...</td>\n",
       "      <td>[alan, smithee, alan, smithee, also, allen, sm...</td>\n",
       "      <td>[[0.0011561304204515677, -0.12259614130011787,...</td>\n",
       "      <td>[[0.22971, -0.28394, 0.070427, -0.074201, -0.0...</td>\n",
       "      <td>[sci, fiction, year, david, roman, novel, fran...</td>\n",
       "      <td>[alan, alan, ausserdem, ebenso, außerdem, eben...</td>\n",
       "      <td>15</td>\n",
       "      <td>236</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-0.25748315624999996, 0.16391171875000002, -0...</td>\n",
       "      <td>[-0.11601057613976422, -0.13505681219389926, -...</td>\n",
       "      <td>[-0.08721950426540277, -0.07013083881516587, -...</td>\n",
       "      <td>mit ( originaltitel : meet ) ist ein us - amer...</td>\n",
       "      <td>alan smithee alan smithee also allen smithee w...</td>\n",
       "      <td>[(, originaltitel, :, meet, ), us, -, amerikan...</td>\n",
       "      <td>[alan, smithee, alan, smithee, also, allen, sm...</td>\n",
       "      <td>[originaltitel, meet, us, amerikanischer, spie...</td>\n",
       "      <td>[alan, smithee, alan, smithee, also, allen, sm...</td>\n",
       "      <td>[[-0.15780262235327164, -0.1363245252964868, 0...</td>\n",
       "      <td>[[0.22971, -0.28394, 0.070427, -0.074201, -0.0...</td>\n",
       "      <td>[drama, movie, film, martin, year, death, tod,...</td>\n",
       "      <td>[alan, alan, ausserdem, ebenso, außerdem, eben...</td>\n",
       "      <td>23</td>\n",
       "      <td>236</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       src_embedding  \\\n",
       "0  [-0.14160268235294118, 0.22068174705882354, -0...   \n",
       "1  [-0.09725738031250002, 0.10076061875, -0.10252...   \n",
       "2  [-0.024419767999999998, 0.2503159047619048, -0...   \n",
       "3  [-0.3159817777777778, 0.2578707777777778, -0.4...   \n",
       "4  [-0.25748315624999996, 0.16391171875000002, -0...   \n",
       "\n",
       "                               src_embedding_aligned  \\\n",
       "0  [-0.12276546643300858, 0.047733963953612756, -...   \n",
       "1  [-0.13914918744447388, 0.016411580786968003, -...   \n",
       "2  [-0.07746652968120009, -0.0106397692086347, -0...   \n",
       "3  [-0.12345786112681063, -0.1265453990910237, -0...   \n",
       "4  [-0.11601057613976422, -0.13505681219389926, -...   \n",
       "\n",
       "                                       trg_embedding  \\\n",
       "0  [-0.08721950426540277, -0.07013083881516587, -...   \n",
       "1  [-0.08721950426540277, -0.07013083881516587, -...   \n",
       "2  [-0.08721950426540277, -0.07013083881516587, -...   \n",
       "3  [-0.08721950426540277, -0.07013083881516587, -...   \n",
       "4  [-0.08721950426540277, -0.07013083881516587, -...   \n",
       "\n",
       "                                        src_sentence  \\\n",
       "0  steht als pseudonym für einen fiktiven regisse...   \n",
       "1  das ( el - latn ‚ tarn - ' , ‚ deckname ' ) is...   \n",
       "2  der ( engl . film editing ) , oft auch synonym...   \n",
       "3  der originaltitel : dune ist ein us - amerikan...   \n",
       "4  mit ( originaltitel : meet ) ist ein us - amer...   \n",
       "\n",
       "                                        trg_sentence  \\\n",
       "0  alan smithee alan smithee also allen smithee w...   \n",
       "1  alan smithee alan smithee also allen smithee w...   \n",
       "2  alan smithee alan smithee also allen smithee w...   \n",
       "3  alan smithee alan smithee also allen smithee w...   \n",
       "4  alan smithee alan smithee also allen smithee w...   \n",
       "\n",
       "                                    src_preprocessed  \\\n",
       "0  [steht, pseudonym, fiktiven, regisseur, ,, fil...   \n",
       "1  [(, el, -, latn, ‚, tarn, -, ', ,, ‚, deckname...   \n",
       "2  [(, engl, ., film, editing, ), ,, oft, synonym...   \n",
       "3  [originaltitel, :, dune, us, -, amerikanischer...   \n",
       "4  [(, originaltitel, :, meet, ), us, -, amerikan...   \n",
       "\n",
       "                                    trg_preprocessed  \\\n",
       "0  [alan, smithee, alan, smithee, also, allen, sm...   \n",
       "1  [alan, smithee, alan, smithee, also, allen, sm...   \n",
       "2  [alan, smithee, alan, smithee, also, allen, sm...   \n",
       "3  [alan, smithee, alan, smithee, also, allen, sm...   \n",
       "4  [alan, smithee, alan, smithee, also, allen, sm...   \n",
       "\n",
       "                                           src_words  \\\n",
       "0  [steht, pseudonym, fiktiven, regisseur, filme,...   \n",
       "1  [el, latn, tarn, deckname, fingierter, name, m...   \n",
       "2  [engl, film, editing, oft, synonym, montage, b...   \n",
       "3  [originaltitel, dune, us, amerikanischer, scie...   \n",
       "4  [originaltitel, meet, us, amerikanischer, spie...   \n",
       "\n",
       "                                           trg_words  \\\n",
       "0  [alan, smithee, alan, smithee, also, allen, sm...   \n",
       "1  [alan, smithee, alan, smithee, also, allen, sm...   \n",
       "2  [alan, smithee, alan, smithee, also, allen, sm...   \n",
       "3  [alan, smithee, alan, smithee, also, allen, sm...   \n",
       "4  [alan, smithee, alan, smithee, also, allen, sm...   \n",
       "\n",
       "                           src_words_found_embedding  \\\n",
       "0  [[-0.06589690229429432, 0.11700426091010997, -...   \n",
       "1  [[-0.15780262235327164, -0.1363245252964868, 0...   \n",
       "2  [[-0.15780262235327164, -0.1363245252964868, 0...   \n",
       "3  [[0.0011561304204515677, -0.12259614130011787,...   \n",
       "4  [[-0.15780262235327164, -0.1363245252964868, 0...   \n",
       "\n",
       "                           trg_words_found_embedding  \\\n",
       "0  [[0.22971, -0.28394, 0.070427, -0.074201, -0.0...   \n",
       "1  [[0.22971, -0.28394, 0.070427, -0.074201, -0.0...   \n",
       "2  [[0.22971, -0.28394, 0.070427, -0.074201, -0.0...   \n",
       "3  [[0.22971, -0.28394, 0.070427, -0.074201, -0.0...   \n",
       "4  [[0.22971, -0.28394, 0.070427, -0.074201, -0.0...   \n",
       "\n",
       "                                src_translated_words  \\\n",
       "0  [stands, alias, pseudonym, films, movies, name...   \n",
       "1  [name, authors, creator, copyright, identities...   \n",
       "2  [engl, movie, film, frequently, oft, often, fr...   \n",
       "3  [sci, fiction, year, david, roman, novel, fran...   \n",
       "4  [drama, movie, film, martin, year, death, tod,...   \n",
       "\n",
       "                                trg_translated_words  src_num_words  \\\n",
       "0  [alan, alan, ausserdem, ebenso, außerdem, eben...             14   \n",
       "1  [alan, alan, ausserdem, ebenso, außerdem, eben...             21   \n",
       "2  [alan, alan, ausserdem, ebenso, außerdem, eben...             15   \n",
       "3  [alan, alan, ausserdem, ebenso, außerdem, eben...             15   \n",
       "4  [alan, alan, ausserdem, ebenso, außerdem, eben...             23   \n",
       "\n",
       "   trg_num_words  src_num_punctuation  trg_num_punctuation  \\\n",
       "0            236                    3                    5   \n",
       "1            236                   15                    5   \n",
       "2            236                    7                    5   \n",
       "3            236                    5                    5   \n",
       "4            236                   11                    5   \n",
       "\n",
       "   src_occ_question_mark  trg_occ_question_mark  src_occ_exclamation_mark  \\\n",
       "0                  False                  False                     False   \n",
       "1                  False                  False                     False   \n",
       "2                  False                  False                     False   \n",
       "3                  False                  False                     False   \n",
       "4                  False                  False                     False   \n",
       "\n",
       "   trg_occ_exclamation_mark  \n",
       "0                     False  \n",
       "1                     False  \n",
       "2                     False  \n",
       "3                     False  \n",
       "4                     False  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dict = {'text_based': list(text_based.FEATURES.keys()), \n",
    "                 'vector_based': list(vector_based.FEATURES.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- INFO: No vector elements as features specified\n",
      "---- INFO: Started extraction of text-based feature norm_diff_translated_words\n",
      "---- INFO: Started extraction of text-based feature abs_diff_num_words\n",
      "---- INFO: Started extraction of text-based feature abs_diff_num_punctuation\n",
      "---- INFO: Started extraction of text-based feature abs_diff_occ_question_mark\n",
      "---- INFO: Started extraction of text-based feature abs_diff_occ_exclamation_mark\n",
      "---- INFO: Started extraction of text-based feature rel_diff_num_words\n",
      "---- INFO: Started extraction of text-based feature rel_diff_num_punctuation\n",
      "---- INFO: Started extraction of text-based feature norm_diff_num_words\n",
      "---- INFO: Started extraction of text-based feature norm_diff_num_punctuation\n",
      "---- INFO: Started extraction of vector-based feature euclidean_distance\n",
      "---- INFO: Started extraction of vector-based feature cosine_similarity\n",
      "---- TIME 2020-05-24 12:06:25.746468: Computation time extracting features on training data: 0:09:25.602591\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "train_data = train_sens.extract_features(features_dict=features_dict, data='all', drop_prepared=True)\n",
    "time(start, datetime.now(), 'extracting features on training data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_raw = pd.merge(left=queries_train, right=qrel_train, on='german_id').merge(docs_train, on='english_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_raw['relevance_level'].replace('3', 1, inplace=True)\n",
    "train_data_raw['relevance_level'].replace('2', 0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.merge(left=train_data, right=train_data_raw[['query', 'document', 'relevance_level']], left_on=['src_sentence', 'trg_sentence'], right_on=['query', 'document'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop(['query', 'document'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.rename(columns={'relevance_level': 'translation'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src_embedding</th>\n",
       "      <th>src_embedding_aligned</th>\n",
       "      <th>trg_embedding</th>\n",
       "      <th>src_sentence</th>\n",
       "      <th>trg_sentence</th>\n",
       "      <th>src_preprocessed</th>\n",
       "      <th>trg_preprocessed</th>\n",
       "      <th>src_words</th>\n",
       "      <th>trg_words</th>\n",
       "      <th>src_words_found_embedding</th>\n",
       "      <th>trg_words_found_embedding</th>\n",
       "      <th>norm_diff_translated_words</th>\n",
       "      <th>abs_diff_num_words</th>\n",
       "      <th>abs_diff_num_punctuation</th>\n",
       "      <th>abs_diff_occ_question_mark</th>\n",
       "      <th>abs_diff_occ_exclamation_mark</th>\n",
       "      <th>rel_diff_num_words</th>\n",
       "      <th>rel_diff_num_punctuation</th>\n",
       "      <th>norm_diff_num_words</th>\n",
       "      <th>norm_diff_num_punctuation</th>\n",
       "      <th>euclidean_distance</th>\n",
       "      <th>cosine_similarity</th>\n",
       "      <th>translation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-0.14160268235294118, 0.22068174705882354, -0...</td>\n",
       "      <td>[-0.12276546643300858, 0.047733963953612756, -...</td>\n",
       "      <td>[-0.08721950426540277, -0.07013083881516587, -...</td>\n",
       "      <td>steht als pseudonym für einen fiktiven regisse...</td>\n",
       "      <td>alan smithee alan smithee also allen smithee w...</td>\n",
       "      <td>[steht, pseudonym, fiktiven, regisseur, ,, fil...</td>\n",
       "      <td>[alan, smithee, alan, smithee, also, allen, sm...</td>\n",
       "      <td>[steht, pseudonym, fiktiven, regisseur, filme,...</td>\n",
       "      <td>[alan, smithee, alan, smithee, also, allen, sm...</td>\n",
       "      <td>[[-0.06589690229429432, 0.11700426091010997, -...</td>\n",
       "      <td>[[0.22971, -0.28394, 0.070427, -0.074201, -0.0...</td>\n",
       "      <td>0.083293</td>\n",
       "      <td>222</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-222</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1.776000</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.971300</td>\n",
       "      <td>0.873807</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-0.09725738031250002, 0.10076061875, -0.10252...</td>\n",
       "      <td>[-0.13914918744447388, 0.016411580786968003, -...</td>\n",
       "      <td>[-0.08721950426540277, -0.07013083881516587, -...</td>\n",
       "      <td>das ( el - latn ‚ tarn - ' , ‚ deckname ' ) is...</td>\n",
       "      <td>alan smithee alan smithee also allen smithee w...</td>\n",
       "      <td>[(, el, -, latn, ‚, tarn, -, ', ,, ‚, deckname...</td>\n",
       "      <td>[alan, smithee, alan, smithee, also, allen, sm...</td>\n",
       "      <td>[el, latn, tarn, deckname, fingierter, name, m...</td>\n",
       "      <td>[alan, smithee, alan, smithee, also, allen, sm...</td>\n",
       "      <td>[[-0.15780262235327164, -0.1363245252964868, 0...</td>\n",
       "      <td>[[0.22971, -0.28394, 0.070427, -0.074201, -0.0...</td>\n",
       "      <td>0.088502</td>\n",
       "      <td>215</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-215</td>\n",
       "      <td>10</td>\n",
       "      <td>-1.679688</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.279038</td>\n",
       "      <td>0.752012</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-0.024419767999999998, 0.2503159047619048, -0...</td>\n",
       "      <td>[-0.07746652968120009, -0.0106397692086347, -0...</td>\n",
       "      <td>[-0.08721950426540277, -0.07013083881516587, -...</td>\n",
       "      <td>der ( engl . film editing ) , oft auch synonym...</td>\n",
       "      <td>alan smithee alan smithee also allen smithee w...</td>\n",
       "      <td>[(, engl, ., film, editing, ), ,, oft, synonym...</td>\n",
       "      <td>[alan, smithee, alan, smithee, also, allen, sm...</td>\n",
       "      <td>[engl, film, editing, oft, synonym, montage, b...</td>\n",
       "      <td>[alan, smithee, alan, smithee, also, allen, sm...</td>\n",
       "      <td>[[-0.15780262235327164, -0.1363245252964868, 0...</td>\n",
       "      <td>[[0.22971, -0.28394, 0.070427, -0.074201, -0.0...</td>\n",
       "      <td>0.077798</td>\n",
       "      <td>221</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-221</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.768000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.157203</td>\n",
       "      <td>0.810826</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-0.3159817777777778, 0.2578707777777778, -0.4...</td>\n",
       "      <td>[-0.12345786112681063, -0.1265453990910237, -0...</td>\n",
       "      <td>[-0.08721950426540277, -0.07013083881516587, -...</td>\n",
       "      <td>der originaltitel : dune ist ein us - amerikan...</td>\n",
       "      <td>alan smithee alan smithee also allen smithee w...</td>\n",
       "      <td>[originaltitel, :, dune, us, -, amerikanischer...</td>\n",
       "      <td>[alan, smithee, alan, smithee, also, allen, sm...</td>\n",
       "      <td>[originaltitel, dune, us, amerikanischer, scie...</td>\n",
       "      <td>[alan, smithee, alan, smithee, also, allen, sm...</td>\n",
       "      <td>[[0.0011561304204515677, -0.12259614130011787,...</td>\n",
       "      <td>[[0.22971, -0.28394, 0.070427, -0.074201, -0.0...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>221</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-221</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.768000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.713225</td>\n",
       "      <td>0.746799</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-0.25748315624999996, 0.16391171875000002, -0...</td>\n",
       "      <td>[-0.11601057613976422, -0.13505681219389926, -...</td>\n",
       "      <td>[-0.08721950426540277, -0.07013083881516587, -...</td>\n",
       "      <td>mit ( originaltitel : meet ) ist ein us - amer...</td>\n",
       "      <td>alan smithee alan smithee also allen smithee w...</td>\n",
       "      <td>[(, originaltitel, :, meet, ), us, -, amerikan...</td>\n",
       "      <td>[alan, smithee, alan, smithee, also, allen, sm...</td>\n",
       "      <td>[originaltitel, meet, us, amerikanischer, spie...</td>\n",
       "      <td>[alan, smithee, alan, smithee, also, allen, sm...</td>\n",
       "      <td>[[-0.15780262235327164, -0.1363245252964868, 0...</td>\n",
       "      <td>[[0.22971, -0.28394, 0.070427, -0.074201, -0.0...</td>\n",
       "      <td>0.051824</td>\n",
       "      <td>213</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-213</td>\n",
       "      <td>6</td>\n",
       "      <td>-1.651163</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.271268</td>\n",
       "      <td>0.803858</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       src_embedding  \\\n",
       "0  [-0.14160268235294118, 0.22068174705882354, -0...   \n",
       "1  [-0.09725738031250002, 0.10076061875, -0.10252...   \n",
       "2  [-0.024419767999999998, 0.2503159047619048, -0...   \n",
       "3  [-0.3159817777777778, 0.2578707777777778, -0.4...   \n",
       "4  [-0.25748315624999996, 0.16391171875000002, -0...   \n",
       "\n",
       "                               src_embedding_aligned  \\\n",
       "0  [-0.12276546643300858, 0.047733963953612756, -...   \n",
       "1  [-0.13914918744447388, 0.016411580786968003, -...   \n",
       "2  [-0.07746652968120009, -0.0106397692086347, -0...   \n",
       "3  [-0.12345786112681063, -0.1265453990910237, -0...   \n",
       "4  [-0.11601057613976422, -0.13505681219389926, -...   \n",
       "\n",
       "                                       trg_embedding  \\\n",
       "0  [-0.08721950426540277, -0.07013083881516587, -...   \n",
       "1  [-0.08721950426540277, -0.07013083881516587, -...   \n",
       "2  [-0.08721950426540277, -0.07013083881516587, -...   \n",
       "3  [-0.08721950426540277, -0.07013083881516587, -...   \n",
       "4  [-0.08721950426540277, -0.07013083881516587, -...   \n",
       "\n",
       "                                        src_sentence  \\\n",
       "0  steht als pseudonym für einen fiktiven regisse...   \n",
       "1  das ( el - latn ‚ tarn - ' , ‚ deckname ' ) is...   \n",
       "2  der ( engl . film editing ) , oft auch synonym...   \n",
       "3  der originaltitel : dune ist ein us - amerikan...   \n",
       "4  mit ( originaltitel : meet ) ist ein us - amer...   \n",
       "\n",
       "                                        trg_sentence  \\\n",
       "0  alan smithee alan smithee also allen smithee w...   \n",
       "1  alan smithee alan smithee also allen smithee w...   \n",
       "2  alan smithee alan smithee also allen smithee w...   \n",
       "3  alan smithee alan smithee also allen smithee w...   \n",
       "4  alan smithee alan smithee also allen smithee w...   \n",
       "\n",
       "                                    src_preprocessed  \\\n",
       "0  [steht, pseudonym, fiktiven, regisseur, ,, fil...   \n",
       "1  [(, el, -, latn, ‚, tarn, -, ', ,, ‚, deckname...   \n",
       "2  [(, engl, ., film, editing, ), ,, oft, synonym...   \n",
       "3  [originaltitel, :, dune, us, -, amerikanischer...   \n",
       "4  [(, originaltitel, :, meet, ), us, -, amerikan...   \n",
       "\n",
       "                                    trg_preprocessed  \\\n",
       "0  [alan, smithee, alan, smithee, also, allen, sm...   \n",
       "1  [alan, smithee, alan, smithee, also, allen, sm...   \n",
       "2  [alan, smithee, alan, smithee, also, allen, sm...   \n",
       "3  [alan, smithee, alan, smithee, also, allen, sm...   \n",
       "4  [alan, smithee, alan, smithee, also, allen, sm...   \n",
       "\n",
       "                                           src_words  \\\n",
       "0  [steht, pseudonym, fiktiven, regisseur, filme,...   \n",
       "1  [el, latn, tarn, deckname, fingierter, name, m...   \n",
       "2  [engl, film, editing, oft, synonym, montage, b...   \n",
       "3  [originaltitel, dune, us, amerikanischer, scie...   \n",
       "4  [originaltitel, meet, us, amerikanischer, spie...   \n",
       "\n",
       "                                           trg_words  \\\n",
       "0  [alan, smithee, alan, smithee, also, allen, sm...   \n",
       "1  [alan, smithee, alan, smithee, also, allen, sm...   \n",
       "2  [alan, smithee, alan, smithee, also, allen, sm...   \n",
       "3  [alan, smithee, alan, smithee, also, allen, sm...   \n",
       "4  [alan, smithee, alan, smithee, also, allen, sm...   \n",
       "\n",
       "                           src_words_found_embedding  \\\n",
       "0  [[-0.06589690229429432, 0.11700426091010997, -...   \n",
       "1  [[-0.15780262235327164, -0.1363245252964868, 0...   \n",
       "2  [[-0.15780262235327164, -0.1363245252964868, 0...   \n",
       "3  [[0.0011561304204515677, -0.12259614130011787,...   \n",
       "4  [[-0.15780262235327164, -0.1363245252964868, 0...   \n",
       "\n",
       "                           trg_words_found_embedding  \\\n",
       "0  [[0.22971, -0.28394, 0.070427, -0.074201, -0.0...   \n",
       "1  [[0.22971, -0.28394, 0.070427, -0.074201, -0.0...   \n",
       "2  [[0.22971, -0.28394, 0.070427, -0.074201, -0.0...   \n",
       "3  [[0.22971, -0.28394, 0.070427, -0.074201, -0.0...   \n",
       "4  [[0.22971, -0.28394, 0.070427, -0.074201, -0.0...   \n",
       "\n",
       "   norm_diff_translated_words  abs_diff_num_words  abs_diff_num_punctuation  \\\n",
       "0                    0.083293                 222                         2   \n",
       "1                    0.088502                 215                        10   \n",
       "2                    0.077798                 221                         2   \n",
       "3                    0.000000                 221                         0   \n",
       "4                    0.051824                 213                         6   \n",
       "\n",
       "   abs_diff_occ_question_mark  abs_diff_occ_exclamation_mark  \\\n",
       "0                           1                              1   \n",
       "1                           1                              1   \n",
       "2                           1                              1   \n",
       "3                           1                              1   \n",
       "4                           1                              1   \n",
       "\n",
       "   rel_diff_num_words  rel_diff_num_punctuation  norm_diff_num_words  \\\n",
       "0                -222                        -2            -1.776000   \n",
       "1                -215                        10            -1.679688   \n",
       "2                -221                         2            -1.768000   \n",
       "3                -221                         0            -1.768000   \n",
       "4                -213                         6            -1.651163   \n",
       "\n",
       "   norm_diff_num_punctuation  euclidean_distance  cosine_similarity  \\\n",
       "0                  -0.500000            0.971300           0.873807   \n",
       "1                   1.000000            1.279038           0.752012   \n",
       "2                   0.333333            1.157203           0.810826   \n",
       "3                   0.000000            1.713225           0.746799   \n",
       "4                   0.750000            1.271268           0.803858   \n",
       "\n",
       "   translation  \n",
       "0            1  \n",
       "1            0  \n",
       "2            0  \n",
       "3            0  \n",
       "4            0  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load sentence embeddings for test collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- DONE: Target sentences loaded\n",
      "---- DONE: Source sentences loaded\n",
      "---- DONE: Sentences preprocessed\n",
      "---- INFO: Sentences embeddings extracted in de\n",
      "---- ERROR: Sentence embedding failed in en on ID 8835: hospet\n",
      "---- INFO: Sentences embeddings extracted in en\n",
      "---- INFO: Extracted word embeddings of found words\n",
      "---- DONE: Sentences transformed\n",
      "---- INFO: Embedding space of source language transformed according to projection matrix\n",
      "---- DONE: Source words extracted\n",
      "---- DONE: Target words extracted\n",
      "---- INFO: Embeddings of found words added as a column\n",
      "---- INFO: Start preparation of text-based feature translated_words\n",
      "---- INFO: Start preparation of text-based feature num_words\n",
      "---- INFO: Start preparation of text-based feature num_punctuation\n",
      "---- INFO: Start preparation of text-based feature occ_question_mark\n",
      "---- INFO: Start preparation of text-based feature occ_exclamation_mark\n",
      "---- DONE: All features prepared\n",
      "---- INFO: Dropped duplicates\n",
      "---- INFO: Delete sentences containing only a '.'\n",
      "---- DONE: Data loaded. Length of dataset after preprocessing and duplicate handling: 9999\n",
      "---- TIME 2020-05-24 12:21:28.302182: Computation time loading sentences object: 0:00:55.889858\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "test_sens = sentences.Sentences(src_words=german, trg_words=english)\n",
    "prepared_features = list(text_based.PREPARED_FEATURES.keys())\n",
    "data = test_sens.load_data(src_sentences=test_queries, trg_sentences=test_documents, single_source=False, features=prepared_features, agg_method='average', documents=True)\n",
    "time(start, datetime.now(), 'loading sentences object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src_embedding</th>\n",
       "      <th>src_embedding_aligned</th>\n",
       "      <th>trg_embedding</th>\n",
       "      <th>src_sentence</th>\n",
       "      <th>trg_sentence</th>\n",
       "      <th>src_preprocessed</th>\n",
       "      <th>trg_preprocessed</th>\n",
       "      <th>src_words</th>\n",
       "      <th>trg_words</th>\n",
       "      <th>src_words_found_embedding</th>\n",
       "      <th>trg_words_found_embedding</th>\n",
       "      <th>src_translated_words</th>\n",
       "      <th>trg_translated_words</th>\n",
       "      <th>src_num_words</th>\n",
       "      <th>trg_num_words</th>\n",
       "      <th>src_num_punctuation</th>\n",
       "      <th>trg_num_punctuation</th>\n",
       "      <th>src_occ_question_mark</th>\n",
       "      <th>trg_occ_question_mark</th>\n",
       "      <th>src_occ_exclamation_mark</th>\n",
       "      <th>trg_occ_exclamation_mark</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-0.2362156923076923, 0.08864512307692306, -0....</td>\n",
       "      <td>[-0.1801198842327313, -0.139909188680879, -0.0...</td>\n",
       "      <td>[-0.16333919276836156, -0.19536563276836164, -...</td>\n",
       "      <td>die afroasiatischen ( früher auch als hamito -...</td>\n",
       "      <td>afroasiatic languages afroasiatic afro asiatic...</td>\n",
       "      <td>[afroasiatischen, (, früher, hamito, -, semiti...</td>\n",
       "      <td>[afroasiatic, languages, afroasiatic, afro, as...</td>\n",
       "      <td>[afroasiatischen, früher, hamito, semitisch, s...</td>\n",
       "      <td>[afroasiatic, languages, afroasiatic, afro, as...</td>\n",
       "      <td>[[-0.15780262235327164, -0.1363245252964868, 0...</td>\n",
       "      <td>[[-0.043671, -0.29505, -0.20986, 0.18004, 0.02...</td>\n",
       "      <td>[previously, early, earlier, formerly, designa...</td>\n",
       "      <td>[programmiersprachen, fremdsprachen, sprachen,...</td>\n",
       "      <td>13</td>\n",
       "      <td>215</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-0.18111494444444443, 0.1288052222222222, -0....</td>\n",
       "      <td>[-0.1606634734184596, -0.16624261872403223, 0....</td>\n",
       "      <td>[-0.09051589493670884, -0.10883404240506324, -...</td>\n",
       "      <td>auch dima , eigenbezeichnung dim - aaf , dim -...</td>\n",
       "      <td>dime language dime or dima is an afro asiatic ...</td>\n",
       "      <td>[dima, ,, eigenbezeichnung, dim, -, aaf, ,, di...</td>\n",
       "      <td>[dime, language, dime, dima, afro, asiatic, la...</td>\n",
       "      <td>[dima, eigenbezeichnung, dim, aaf, dim, ko, aa...</td>\n",
       "      <td>[dime, language, dime, dima, afro, asiatic, la...</td>\n",
       "      <td>[[-0.17940864170372575, 0.0737311403301378, -0...</td>\n",
       "      <td>[[0.011212, 0.17819, 0.1576, -0.15137, 0.24949...</td>\n",
       "      <td>[southwestern, southwest, region, river, spoke...</td>\n",
       "      <td>[amtssprache, sprachen, sprache, amtssprache, ...</td>\n",
       "      <td>16</td>\n",
       "      <td>184</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-0.13061995714285712, 0.08085614285714286, -0...</td>\n",
       "      <td>[-0.12835306032659194, -0.09953887462795796, -...</td>\n",
       "      <td>[-0.05739868661691542, -0.11036642383084572, -...</td>\n",
       "      <td>( von ‚ begnadigung , straferlass , amnestie '...</td>\n",
       "      <td>amnesty international amnesty international co...</td>\n",
       "      <td>[(, ‚, begnadigung, ,, straferlass, ,, amnesti...</td>\n",
       "      <td>[amnesty, international, amnesty, internationa...</td>\n",
       "      <td>[begnadigung, straferlass, amnestie, nichtstaa...</td>\n",
       "      <td>[amnesty, international, amnesty, internationa...</td>\n",
       "      <td>[[-0.15780262235327164, -0.1363245252964868, 0...</td>\n",
       "      <td>[[-0.49984, -0.18512, 0.20571, -0.148, -0.2802...</td>\n",
       "      <td>[non, organization, organisation, organizing, ...</td>\n",
       "      <td>[international, internationalen, international...</td>\n",
       "      <td>12</td>\n",
       "      <td>220</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-0.10104247894736841, 0.07025387368421053, -0...</td>\n",
       "      <td>[-0.04052728631907548, 0.026286764620934443, -...</td>\n",
       "      <td>[0.026596005943396228, 0.007910488207547186, -...</td>\n",
       "      <td>die ( bíos ‚ leben ' ; auch als synonym zu bio...</td>\n",
       "      <td>biotechnology is the use of living systems and...</td>\n",
       "      <td>[(, bíos, ‚, leben, ', ;, synonym, biotechnik,...</td>\n",
       "      <td>[biotechnology, use, living, systems, organism...</td>\n",
       "      <td>[bíos, leben, synonym, biotechnik, kurz, biote...</td>\n",
       "      <td>[biotechnology, use, living, systems, organism...</td>\n",
       "      <td>[[-0.15780262235327164, -0.1363245252964868, 0...</td>\n",
       "      <td>[[0.17028, 0.1322, 0.21386, 0.28217, -0.50662,...</td>\n",
       "      <td>[living, life, lifestyle, live, leben, synonym...</td>\n",
       "      <td>[verwendung, benutzen, nutzung, verwenden, geb...</td>\n",
       "      <td>17</td>\n",
       "      <td>220</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-0.0073867083333333345, 0.09049094583333334, ...</td>\n",
       "      <td>[-0.060032190728019505, -0.05749486353309292, ...</td>\n",
       "      <td>[-0.03945234717948716, -0.002273775692307691, ...</td>\n",
       "      <td>wortkreuzung aus engl . pharmaceutical enginee...</td>\n",
       "      <td>pharming ( genetics ) for pharming in internet...</td>\n",
       "      <td>[wortkreuzung, engl, ., pharmaceutical, engine...</td>\n",
       "      <td>[pharming, (, genetics, ), pharming, internet,...</td>\n",
       "      <td>[wortkreuzung, engl, pharmaceutical, engineeri...</td>\n",
       "      <td>[pharming, genetics, pharming, internet, see, ...</td>\n",
       "      <td>[[0.5040755879349637, -0.08830535297655097, -0...</td>\n",
       "      <td>[[-0.23309, -0.15296, 0.18574, -0.052825, -0.0...</td>\n",
       "      <td>[engl, development, evolution, farming, agricu...</td>\n",
       "      <td>[internetseiten, internetzugang, internet, seh...</td>\n",
       "      <td>19</td>\n",
       "      <td>225</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       src_embedding  \\\n",
       "0  [-0.2362156923076923, 0.08864512307692306, -0....   \n",
       "1  [-0.18111494444444443, 0.1288052222222222, -0....   \n",
       "2  [-0.13061995714285712, 0.08085614285714286, -0...   \n",
       "3  [-0.10104247894736841, 0.07025387368421053, -0...   \n",
       "4  [-0.0073867083333333345, 0.09049094583333334, ...   \n",
       "\n",
       "                               src_embedding_aligned  \\\n",
       "0  [-0.1801198842327313, -0.139909188680879, -0.0...   \n",
       "1  [-0.1606634734184596, -0.16624261872403223, 0....   \n",
       "2  [-0.12835306032659194, -0.09953887462795796, -...   \n",
       "3  [-0.04052728631907548, 0.026286764620934443, -...   \n",
       "4  [-0.060032190728019505, -0.05749486353309292, ...   \n",
       "\n",
       "                                       trg_embedding  \\\n",
       "0  [-0.16333919276836156, -0.19536563276836164, -...   \n",
       "1  [-0.09051589493670884, -0.10883404240506324, -...   \n",
       "2  [-0.05739868661691542, -0.11036642383084572, -...   \n",
       "3  [0.026596005943396228, 0.007910488207547186, -...   \n",
       "4  [-0.03945234717948716, -0.002273775692307691, ...   \n",
       "\n",
       "                                        src_sentence  \\\n",
       "0  die afroasiatischen ( früher auch als hamito -...   \n",
       "1  auch dima , eigenbezeichnung dim - aaf , dim -...   \n",
       "2  ( von ‚ begnadigung , straferlass , amnestie '...   \n",
       "3  die ( bíos ‚ leben ' ; auch als synonym zu bio...   \n",
       "4  wortkreuzung aus engl . pharmaceutical enginee...   \n",
       "\n",
       "                                        trg_sentence  \\\n",
       "0  afroasiatic languages afroasiatic afro asiatic...   \n",
       "1  dime language dime or dima is an afro asiatic ...   \n",
       "2  amnesty international amnesty international co...   \n",
       "3  biotechnology is the use of living systems and...   \n",
       "4  pharming ( genetics ) for pharming in internet...   \n",
       "\n",
       "                                    src_preprocessed  \\\n",
       "0  [afroasiatischen, (, früher, hamito, -, semiti...   \n",
       "1  [dima, ,, eigenbezeichnung, dim, -, aaf, ,, di...   \n",
       "2  [(, ‚, begnadigung, ,, straferlass, ,, amnesti...   \n",
       "3  [(, bíos, ‚, leben, ', ;, synonym, biotechnik,...   \n",
       "4  [wortkreuzung, engl, ., pharmaceutical, engine...   \n",
       "\n",
       "                                    trg_preprocessed  \\\n",
       "0  [afroasiatic, languages, afroasiatic, afro, as...   \n",
       "1  [dime, language, dime, dima, afro, asiatic, la...   \n",
       "2  [amnesty, international, amnesty, internationa...   \n",
       "3  [biotechnology, use, living, systems, organism...   \n",
       "4  [pharming, (, genetics, ), pharming, internet,...   \n",
       "\n",
       "                                           src_words  \\\n",
       "0  [afroasiatischen, früher, hamito, semitisch, s...   \n",
       "1  [dima, eigenbezeichnung, dim, aaf, dim, ko, aa...   \n",
       "2  [begnadigung, straferlass, amnestie, nichtstaa...   \n",
       "3  [bíos, leben, synonym, biotechnik, kurz, biote...   \n",
       "4  [wortkreuzung, engl, pharmaceutical, engineeri...   \n",
       "\n",
       "                                           trg_words  \\\n",
       "0  [afroasiatic, languages, afroasiatic, afro, as...   \n",
       "1  [dime, language, dime, dima, afro, asiatic, la...   \n",
       "2  [amnesty, international, amnesty, internationa...   \n",
       "3  [biotechnology, use, living, systems, organism...   \n",
       "4  [pharming, genetics, pharming, internet, see, ...   \n",
       "\n",
       "                           src_words_found_embedding  \\\n",
       "0  [[-0.15780262235327164, -0.1363245252964868, 0...   \n",
       "1  [[-0.17940864170372575, 0.0737311403301378, -0...   \n",
       "2  [[-0.15780262235327164, -0.1363245252964868, 0...   \n",
       "3  [[-0.15780262235327164, -0.1363245252964868, 0...   \n",
       "4  [[0.5040755879349637, -0.08830535297655097, -0...   \n",
       "\n",
       "                           trg_words_found_embedding  \\\n",
       "0  [[-0.043671, -0.29505, -0.20986, 0.18004, 0.02...   \n",
       "1  [[0.011212, 0.17819, 0.1576, -0.15137, 0.24949...   \n",
       "2  [[-0.49984, -0.18512, 0.20571, -0.148, -0.2802...   \n",
       "3  [[0.17028, 0.1322, 0.21386, 0.28217, -0.50662,...   \n",
       "4  [[-0.23309, -0.15296, 0.18574, -0.052825, -0.0...   \n",
       "\n",
       "                                src_translated_words  \\\n",
       "0  [previously, early, earlier, formerly, designa...   \n",
       "1  [southwestern, southwest, region, river, spoke...   \n",
       "2  [non, organization, organisation, organizing, ...   \n",
       "3  [living, life, lifestyle, live, leben, synonym...   \n",
       "4  [engl, development, evolution, farming, agricu...   \n",
       "\n",
       "                                trg_translated_words  src_num_words  \\\n",
       "0  [programmiersprachen, fremdsprachen, sprachen,...             13   \n",
       "1  [amtssprache, sprachen, sprache, amtssprache, ...             16   \n",
       "2  [international, internationalen, international...             12   \n",
       "3  [verwendung, benutzen, nutzung, verwenden, geb...             17   \n",
       "4  [internetseiten, internetzugang, internet, seh...             19   \n",
       "\n",
       "   trg_num_words  src_num_punctuation  trg_num_punctuation  \\\n",
       "0            215                    6                    3   \n",
       "1            184                    8                   14   \n",
       "2            220                   11                    4   \n",
       "3            220                    7                    2   \n",
       "4            225                   10                    5   \n",
       "\n",
       "   src_occ_question_mark  trg_occ_question_mark  src_occ_exclamation_mark  \\\n",
       "0                  False                  False                     False   \n",
       "1                  False                  False                     False   \n",
       "2                  False                  False                     False   \n",
       "3                  False                  False                     False   \n",
       "4                  False                  False                     False   \n",
       "\n",
       "   trg_occ_exclamation_mark  \n",
       "0                     False  \n",
       "1                     False  \n",
       "2                     False  \n",
       "3                     False  \n",
       "4                     False  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- INFO: Preliminary queries dataframe created\n",
      "---- INFO: Preliminary documentes dataframe created\n",
      "---- INFO: Merged queries and documents dataframe\n",
      "---- INFO: Merged with test dataframe\n",
      "---- INFO: Added translation indicator\n",
      "---- DONE: Test collection created\n",
      "---- TIME 2020-05-24 12:22:53.851715: Computation time creating the test collection: 0:01:23.949866\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "test_collection = test_sens.create_test_collection(n_queries=1000, n_docs=10000)\n",
    "time(start, datetime.now(), 'creating the test collection')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- INFO: No vector elements as features specified\n",
      "---- INFO: Started extraction of text-based feature norm_diff_translated_words\n",
      "---- INFO: Started extraction of text-based feature abs_diff_num_words\n",
      "---- INFO: Started extraction of text-based feature abs_diff_num_punctuation\n",
      "---- INFO: Started extraction of text-based feature abs_diff_occ_question_mark\n",
      "---- INFO: Started extraction of text-based feature abs_diff_occ_exclamation_mark\n",
      "---- INFO: Started extraction of text-based feature rel_diff_num_words\n",
      "---- INFO: Started extraction of text-based feature rel_diff_num_punctuation\n",
      "---- INFO: Started extraction of text-based feature norm_diff_num_words\n",
      "---- INFO: Started extraction of text-based feature norm_diff_num_punctuation\n",
      "---- INFO: Started extraction of vector-based feature euclidean_distance\n",
      "---- INFO: Started extraction of vector-based feature cosine_similarity\n",
      "---- TIME 2020-05-24 16:00:20.557666: Computation time extracting features on test collection: 3:36:09.428113\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "test_collection = test_sens.extract_features(features_dict=features_dict, data='test', drop_prepared=True)\n",
    "time(start, datetime.now(), 'extracting features on test collection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src_sentence</th>\n",
       "      <th>src_preprocessed</th>\n",
       "      <th>src_embedding</th>\n",
       "      <th>src_embedding_aligned</th>\n",
       "      <th>src_words</th>\n",
       "      <th>src_words_found_embedding</th>\n",
       "      <th>trg_sentence</th>\n",
       "      <th>trg_preprocessed</th>\n",
       "      <th>trg_embedding</th>\n",
       "      <th>trg_words</th>\n",
       "      <th>trg_words_found_embedding</th>\n",
       "      <th>translation</th>\n",
       "      <th>norm_diff_translated_words</th>\n",
       "      <th>abs_diff_num_words</th>\n",
       "      <th>abs_diff_num_punctuation</th>\n",
       "      <th>abs_diff_occ_question_mark</th>\n",
       "      <th>abs_diff_occ_exclamation_mark</th>\n",
       "      <th>rel_diff_num_words</th>\n",
       "      <th>rel_diff_num_punctuation</th>\n",
       "      <th>norm_diff_num_words</th>\n",
       "      <th>norm_diff_num_punctuation</th>\n",
       "      <th>euclidean_distance</th>\n",
       "      <th>cosine_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>die afroasiatischen ( früher auch als hamito -...</td>\n",
       "      <td>[afroasiatischen, (, früher, hamito, -, semiti...</td>\n",
       "      <td>[-0.2362156923076923, 0.08864512307692306, -0....</td>\n",
       "      <td>[-0.1801198842327313, -0.139909188680879, -0.0...</td>\n",
       "      <td>[afroasiatischen, früher, hamito, semitisch, s...</td>\n",
       "      <td>[[-0.15780262235327164, -0.1363245252964868, 0...</td>\n",
       "      <td>afroasiatic languages afroasiatic afro asiatic...</td>\n",
       "      <td>[afroasiatic, languages, afroasiatic, afro, as...</td>\n",
       "      <td>[-0.16333919276836156, -0.19536563276836164, -...</td>\n",
       "      <td>[afroasiatic, languages, afroasiatic, afro, as...</td>\n",
       "      <td>[[-0.043671, -0.29505, -0.20986, 0.18004, 0.02...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.046726</td>\n",
       "      <td>202</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-202</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.771930</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.340791</td>\n",
       "      <td>0.792458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>die afroasiatischen ( früher auch als hamito -...</td>\n",
       "      <td>[afroasiatischen, (, früher, hamito, -, semiti...</td>\n",
       "      <td>[-0.2362156923076923, 0.08864512307692306, -0....</td>\n",
       "      <td>[-0.1801198842327313, -0.139909188680879, -0.0...</td>\n",
       "      <td>[afroasiatischen, früher, hamito, semitisch, s...</td>\n",
       "      <td>[[-0.15780262235327164, -0.1363245252964868, 0...</td>\n",
       "      <td>dime language dime or dima is an afro asiatic ...</td>\n",
       "      <td>[dime, language, dime, dima, afro, asiatic, la...</td>\n",
       "      <td>[-0.09051589493670884, -0.10883404240506324, -...</td>\n",
       "      <td>[dime, language, dime, dima, afro, asiatic, la...</td>\n",
       "      <td>[[0.011212, 0.17819, 0.1576, -0.15137, 0.24949...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.042699</td>\n",
       "      <td>171</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-171</td>\n",
       "      <td>-8</td>\n",
       "      <td>-1.744898</td>\n",
       "      <td>-0.80</td>\n",
       "      <td>1.288895</td>\n",
       "      <td>0.791433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>die afroasiatischen ( früher auch als hamito -...</td>\n",
       "      <td>[afroasiatischen, (, früher, hamito, -, semiti...</td>\n",
       "      <td>[-0.2362156923076923, 0.08864512307692306, -0....</td>\n",
       "      <td>[-0.1801198842327313, -0.139909188680879, -0.0...</td>\n",
       "      <td>[afroasiatischen, früher, hamito, semitisch, s...</td>\n",
       "      <td>[[-0.15780262235327164, -0.1363245252964868, 0...</td>\n",
       "      <td>amnesty international amnesty international co...</td>\n",
       "      <td>[amnesty, international, amnesty, internationa...</td>\n",
       "      <td>[-0.05739868661691542, -0.11036642383084572, -...</td>\n",
       "      <td>[amnesty, international, amnesty, internationa...</td>\n",
       "      <td>[[-0.49984, -0.18512, 0.20571, -0.148, -0.2802...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>207</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-207</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.784483</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.640474</td>\n",
       "      <td>0.651392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>die afroasiatischen ( früher auch als hamito -...</td>\n",
       "      <td>[afroasiatischen, (, früher, hamito, -, semiti...</td>\n",
       "      <td>[-0.2362156923076923, 0.08864512307692306, -0....</td>\n",
       "      <td>[-0.1801198842327313, -0.139909188680879, -0.0...</td>\n",
       "      <td>[afroasiatischen, früher, hamito, semitisch, s...</td>\n",
       "      <td>[[-0.15780262235327164, -0.1363245252964868, 0...</td>\n",
       "      <td>biotechnology is the use of living systems and...</td>\n",
       "      <td>[biotechnology, use, living, systems, organism...</td>\n",
       "      <td>[0.026596005943396228, 0.007910488207547186, -...</td>\n",
       "      <td>[biotechnology, use, living, systems, organism...</td>\n",
       "      <td>[[0.17028, 0.1322, 0.21386, 0.28217, -0.50662,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.041606</td>\n",
       "      <td>207</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-207</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.784483</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.787893</td>\n",
       "      <td>0.638994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>die afroasiatischen ( früher auch als hamito -...</td>\n",
       "      <td>[afroasiatischen, (, früher, hamito, -, semiti...</td>\n",
       "      <td>[-0.2362156923076923, 0.08864512307692306, -0....</td>\n",
       "      <td>[-0.1801198842327313, -0.139909188680879, -0.0...</td>\n",
       "      <td>[afroasiatischen, früher, hamito, semitisch, s...</td>\n",
       "      <td>[[-0.15780262235327164, -0.1363245252964868, 0...</td>\n",
       "      <td>pharming ( genetics ) for pharming in internet...</td>\n",
       "      <td>[pharming, (, genetics, ), pharming, internet,...</td>\n",
       "      <td>[-0.03945234717948716, -0.002273775692307691, ...</td>\n",
       "      <td>[pharming, genetics, pharming, internet, see, ...</td>\n",
       "      <td>[[-0.23309, -0.15296, 0.18574, -0.052825, -0.0...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.041510</td>\n",
       "      <td>212</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-212</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.781513</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.700643</td>\n",
       "      <td>0.652745</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        src_sentence  \\\n",
       "0  die afroasiatischen ( früher auch als hamito -...   \n",
       "1  die afroasiatischen ( früher auch als hamito -...   \n",
       "2  die afroasiatischen ( früher auch als hamito -...   \n",
       "3  die afroasiatischen ( früher auch als hamito -...   \n",
       "4  die afroasiatischen ( früher auch als hamito -...   \n",
       "\n",
       "                                    src_preprocessed  \\\n",
       "0  [afroasiatischen, (, früher, hamito, -, semiti...   \n",
       "1  [afroasiatischen, (, früher, hamito, -, semiti...   \n",
       "2  [afroasiatischen, (, früher, hamito, -, semiti...   \n",
       "3  [afroasiatischen, (, früher, hamito, -, semiti...   \n",
       "4  [afroasiatischen, (, früher, hamito, -, semiti...   \n",
       "\n",
       "                                       src_embedding  \\\n",
       "0  [-0.2362156923076923, 0.08864512307692306, -0....   \n",
       "1  [-0.2362156923076923, 0.08864512307692306, -0....   \n",
       "2  [-0.2362156923076923, 0.08864512307692306, -0....   \n",
       "3  [-0.2362156923076923, 0.08864512307692306, -0....   \n",
       "4  [-0.2362156923076923, 0.08864512307692306, -0....   \n",
       "\n",
       "                               src_embedding_aligned  \\\n",
       "0  [-0.1801198842327313, -0.139909188680879, -0.0...   \n",
       "1  [-0.1801198842327313, -0.139909188680879, -0.0...   \n",
       "2  [-0.1801198842327313, -0.139909188680879, -0.0...   \n",
       "3  [-0.1801198842327313, -0.139909188680879, -0.0...   \n",
       "4  [-0.1801198842327313, -0.139909188680879, -0.0...   \n",
       "\n",
       "                                           src_words  \\\n",
       "0  [afroasiatischen, früher, hamito, semitisch, s...   \n",
       "1  [afroasiatischen, früher, hamito, semitisch, s...   \n",
       "2  [afroasiatischen, früher, hamito, semitisch, s...   \n",
       "3  [afroasiatischen, früher, hamito, semitisch, s...   \n",
       "4  [afroasiatischen, früher, hamito, semitisch, s...   \n",
       "\n",
       "                           src_words_found_embedding  \\\n",
       "0  [[-0.15780262235327164, -0.1363245252964868, 0...   \n",
       "1  [[-0.15780262235327164, -0.1363245252964868, 0...   \n",
       "2  [[-0.15780262235327164, -0.1363245252964868, 0...   \n",
       "3  [[-0.15780262235327164, -0.1363245252964868, 0...   \n",
       "4  [[-0.15780262235327164, -0.1363245252964868, 0...   \n",
       "\n",
       "                                        trg_sentence  \\\n",
       "0  afroasiatic languages afroasiatic afro asiatic...   \n",
       "1  dime language dime or dima is an afro asiatic ...   \n",
       "2  amnesty international amnesty international co...   \n",
       "3  biotechnology is the use of living systems and...   \n",
       "4  pharming ( genetics ) for pharming in internet...   \n",
       "\n",
       "                                    trg_preprocessed  \\\n",
       "0  [afroasiatic, languages, afroasiatic, afro, as...   \n",
       "1  [dime, language, dime, dima, afro, asiatic, la...   \n",
       "2  [amnesty, international, amnesty, internationa...   \n",
       "3  [biotechnology, use, living, systems, organism...   \n",
       "4  [pharming, (, genetics, ), pharming, internet,...   \n",
       "\n",
       "                                       trg_embedding  \\\n",
       "0  [-0.16333919276836156, -0.19536563276836164, -...   \n",
       "1  [-0.09051589493670884, -0.10883404240506324, -...   \n",
       "2  [-0.05739868661691542, -0.11036642383084572, -...   \n",
       "3  [0.026596005943396228, 0.007910488207547186, -...   \n",
       "4  [-0.03945234717948716, -0.002273775692307691, ...   \n",
       "\n",
       "                                           trg_words  \\\n",
       "0  [afroasiatic, languages, afroasiatic, afro, as...   \n",
       "1  [dime, language, dime, dima, afro, asiatic, la...   \n",
       "2  [amnesty, international, amnesty, internationa...   \n",
       "3  [biotechnology, use, living, systems, organism...   \n",
       "4  [pharming, genetics, pharming, internet, see, ...   \n",
       "\n",
       "                           trg_words_found_embedding  translation  \\\n",
       "0  [[-0.043671, -0.29505, -0.20986, 0.18004, 0.02...            1   \n",
       "1  [[0.011212, 0.17819, 0.1576, -0.15137, 0.24949...            0   \n",
       "2  [[-0.49984, -0.18512, 0.20571, -0.148, -0.2802...            0   \n",
       "3  [[0.17028, 0.1322, 0.21386, 0.28217, -0.50662,...            0   \n",
       "4  [[-0.23309, -0.15296, 0.18574, -0.052825, -0.0...            0   \n",
       "\n",
       "   norm_diff_translated_words  abs_diff_num_words  abs_diff_num_punctuation  \\\n",
       "0                    0.046726                 202                         3   \n",
       "1                    0.042699                 171                         8   \n",
       "2                    0.000000                 207                         2   \n",
       "3                    0.041606                 207                         4   \n",
       "4                    0.041510                 212                         1   \n",
       "\n",
       "   abs_diff_occ_question_mark  abs_diff_occ_exclamation_mark  \\\n",
       "0                           1                              1   \n",
       "1                           1                              1   \n",
       "2                           1                              1   \n",
       "3                           1                              1   \n",
       "4                           1                              1   \n",
       "\n",
       "   rel_diff_num_words  rel_diff_num_punctuation  norm_diff_num_words  \\\n",
       "0                -202                         3            -1.771930   \n",
       "1                -171                        -8            -1.744898   \n",
       "2                -207                         2            -1.784483   \n",
       "3                -207                         4            -1.784483   \n",
       "4                -212                         1            -1.781513   \n",
       "\n",
       "   norm_diff_num_punctuation  euclidean_distance  cosine_similarity  \n",
       "0                       0.75            1.340791           0.792458  \n",
       "1                      -0.80            1.288895           0.791433  \n",
       "2                       0.40            1.640474           0.651392  \n",
       "3                       1.00            1.787893           0.638994  \n",
       "4                       0.20            1.700643           0.652745  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_collection.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop irrelevant columns to enable saving the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = [f'{prefix}_{feature}' for prefix in ['src', 'trg'] for feature in ['preprocessed', 'words', 'words_found_embedding']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop(drop_columns, inplace=True, axis=1)\n",
    "test_collection.drop(drop_columns, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop sentence, word embeddings, words found columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f'{paths.data_path}extracted_data/global/documents_v0.2'\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(f'{path}/train')\n",
    "    os.makedirs(f'{path}/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_chunks = [train_data[i:(i+math.ceil(len(train_data)/13))] \n",
    "          for i in range(0,len(train_data),math.ceil(len(train_data)/13))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- INFO: Chunk 00 saved\n",
      "---- INFO: Chunk 01 saved\n",
      "---- INFO: Chunk 02 saved\n",
      "---- INFO: Chunk 03 saved\n",
      "---- INFO: Chunk 04 saved\n",
      "---- INFO: Chunk 05 saved\n",
      "---- INFO: Chunk 06 saved\n",
      "---- INFO: Chunk 07 saved\n",
      "---- INFO: Chunk 08 saved\n",
      "---- INFO: Chunk 09 saved\n",
      "---- INFO: Chunk 10 saved\n",
      "---- INFO: Chunk 11 saved\n",
      "---- INFO: Chunk 12 saved\n",
      "---- TIME 2020-05-24 16:09:28.952292: Computation time saving training data: 0:01:17.033286\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "for idx, data in enumerate(train_chunks):\n",
    "    data.to_pickle(f'{path}/train/training_data_{idx:02d}_avg.pkl')\n",
    "    print(f'---- INFO: Chunk {idx:02d} saved')\n",
    "time(start, datetime.now(), 'saving training data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_chunks = [test_collection[i:(i+math.ceil(len(test_collection)/40))] \n",
    "          for i in range(0,len(test_collection),math.ceil(len(test_collection)/40))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- INFO: Chunk 00 saved\n",
      "---- INFO: Chunk 01 saved\n",
      "---- INFO: Chunk 02 saved\n",
      "---- INFO: Chunk 03 saved\n",
      "---- INFO: Chunk 04 saved\n",
      "---- INFO: Chunk 05 saved\n",
      "---- INFO: Chunk 06 saved\n",
      "---- INFO: Chunk 07 saved\n",
      "---- INFO: Chunk 08 saved\n",
      "---- INFO: Chunk 09 saved\n",
      "---- INFO: Chunk 10 saved\n",
      "---- INFO: Chunk 11 saved\n",
      "---- INFO: Chunk 12 saved\n",
      "---- INFO: Chunk 13 saved\n",
      "---- INFO: Chunk 14 saved\n",
      "---- INFO: Chunk 15 saved\n",
      "---- INFO: Chunk 16 saved\n",
      "---- INFO: Chunk 17 saved\n",
      "---- INFO: Chunk 18 saved\n",
      "---- INFO: Chunk 19 saved\n",
      "---- INFO: Chunk 20 saved\n",
      "---- INFO: Chunk 21 saved\n",
      "---- INFO: Chunk 22 saved\n",
      "---- INFO: Chunk 23 saved\n",
      "---- INFO: Chunk 24 saved\n",
      "---- INFO: Chunk 25 saved\n",
      "---- INFO: Chunk 26 saved\n",
      "---- INFO: Chunk 27 saved\n",
      "---- INFO: Chunk 28 saved\n",
      "---- INFO: Chunk 29 saved\n",
      "---- INFO: Chunk 30 saved\n",
      "---- INFO: Chunk 31 saved\n",
      "---- INFO: Chunk 32 saved\n",
      "---- INFO: Chunk 33 saved\n",
      "---- INFO: Chunk 34 saved\n",
      "---- INFO: Chunk 35 saved\n",
      "---- INFO: Chunk 36 saved\n",
      "---- INFO: Chunk 37 saved\n",
      "---- INFO: Chunk 38 saved\n",
      "---- INFO: Chunk 39 saved\n",
      "---- TIME 2020-05-24 16:13:34.073143: Computation time saving test collection: 0:00:20.091994\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "for idx, data in enumerate(test_chunks):\n",
    "    data.to_pickle(f'{path}/test/test_collection_{idx:02d}_avg.pkl')\n",
    "    print(f'---- INFO: Chunk {idx:02d} saved')\n",
    "time(start, datetime.now(), 'saving test collection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
