{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, os, importlib, pickle, math\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'ir_crosslingual.sentences.sentences' from '/Users/jani/PycharmProjects/ir-crosslingual/ir_crosslingual/sentences/sentences.py'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ir_crosslingual.utils import paths\n",
    "importlib.reload(paths)\n",
    "\n",
    "from ir_crosslingual.features import text_based\n",
    "importlib.reload(text_based)\n",
    "\n",
    "from ir_crosslingual.features import vector_based\n",
    "importlib.reload(vector_based)\n",
    "\n",
    "from ir_crosslingual.embeddings import embeddings\n",
    "importlib.reload(embeddings)\n",
    "\n",
    "from ir_crosslingual.sentences import sentences\n",
    "importlib.reload(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time(start, stop, message):\n",
    "    print('Computation time {}: {}'.format(message, stop-start))\n",
    "    print('Finished at: {}'.format(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_start = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "french = embeddings.WordEmbeddings('fr')\n",
    "french.load_embeddings()\n",
    "\n",
    "english = embeddings.WordEmbeddings('en')\n",
    "english.load_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- INFO: Learn projection matrix for en-fr\n",
      "---- INFO: Found 10369 valid translation pairs in expert dictionary.\n",
      "---- INFO: 503 other pairs contained at least one unknown word (0 in source language, 503 in target language).\n",
      "---- DONE: Seed dictionary extracted for the languages: en-fr\n",
      "---- INFO: Resulting subspace dimension: (10369, 300)\n",
      "---- INFO: Resulting subspace dimension: (10369, 300)\n",
      "---- DONE: Projection matrix learned from en to fr\n",
      "---- INFO: Learn projection matrix for fr-en\n",
      "---- INFO: Found 7938 valid translation pairs in expert dictionary.\n",
      "---- INFO: 332 other pairs contained at least one unknown word (0 in source language, 332 in target language).\n",
      "---- DONE: Seed dictionary extracted for the languages: fr-en\n",
      "---- INFO: Resulting subspace dimension: (7938, 300)\n",
      "---- INFO: Resulting subspace dimension: (7938, 300)\n",
      "---- DONE: Projection matrix learned from fr to en\n"
     ]
    }
   ],
   "source": [
    "W_enfr, W_fren = embeddings.WordEmbeddings.learn_projection_matrix(src_lang='en', trg_lang='fr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load sentence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- DONE: Target sentences loaded\n",
      "---- DONE: Source sentences loaded\n",
      "---- DONE: Sentences preprocessed\n",
      "---- ERROR: Sentence embedding failed in en on ID 6544: ALTENER\n",
      "---- ERROR: Sentence embedding failed in en on ID 10259: Scrapie\n",
      "---- ERROR: Sentence embedding failed in en on ID 7439: and\n",
      "---- INFO: Sentences embeddings extracted in en\n",
      "---- ERROR: Sentence embedding failed in fr on ID 6544: Altener\n",
      "---- ERROR: Sentence embedding failed in fr on ID 10259: La tremblante\n",
      "---- ERROR: Sentence embedding failed in fr on ID 7439: et\n",
      "---- INFO: Sentences embeddings extracted in fr\n",
      "---- INFO: Shape of source word embeddings\n",
      "---- INFO: Extracted word embeddings of found words\n",
      "---- DONE: Sentences transformed\n",
      "---- INFO: Embedding space of source language transformed according to projection matrix\n",
      "---- DONE: Source words extracted\n",
      "---- DONE: Target words extracted\n",
      "---- INFO: Embeddings of found words added as a column\n",
      "---- INFO: Start preparation of text-based feature translated_words\n",
      "---- INFO: Start preparation of text-based feature num_words\n",
      "---- INFO: Start preparation of text-based feature num_punctuation\n",
      "---- INFO: Start preparation of text-based feature occ_question_mark\n",
      "---- INFO: Start preparation of text-based feature occ_exclamation_mark\n",
      "---- DONE: All features prepared\n",
      "---- INFO: Dropped duplicates\n",
      "---- INFO: Delete sentences containing only a '.'\n",
      "---- DONE: Data loaded. Length of dataset after preprocessing and duplicate handling: 14864\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "sens = sentences.Sentences(src_words=english, trg_words=french)\n",
    "prepared_features = list(text_based.PREPARED_FEATURES.keys())\n",
    "data = sens.load_data(single_source=False, n_max=15000, features=prepared_features, agg_method='average')\n",
    "stop = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14864"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computation time loading the data: 0:00:05.836677\n",
      "Finished at: 2020-05-23 02:57:26.054744\n"
     ]
    }
   ],
   "source": [
    "time(start, stop, 'loading the data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src_embedding</th>\n",
       "      <th>src_embedding_aligned</th>\n",
       "      <th>trg_embedding</th>\n",
       "      <th>src_sentence</th>\n",
       "      <th>trg_sentence</th>\n",
       "      <th>src_preprocessed</th>\n",
       "      <th>trg_preprocessed</th>\n",
       "      <th>src_words</th>\n",
       "      <th>trg_words</th>\n",
       "      <th>src_words_found_embedding</th>\n",
       "      <th>trg_words_found_embedding</th>\n",
       "      <th>src_translated_words</th>\n",
       "      <th>trg_translated_words</th>\n",
       "      <th>src_num_words</th>\n",
       "      <th>trg_num_words</th>\n",
       "      <th>src_num_punctuation</th>\n",
       "      <th>trg_num_punctuation</th>\n",
       "      <th>src_occ_question_mark</th>\n",
       "      <th>trg_occ_question_mark</th>\n",
       "      <th>src_occ_exclamation_mark</th>\n",
       "      <th>trg_occ_exclamation_mark</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-0.07040999999999999, 0.094428, 0.1178755, -0...</td>\n",
       "      <td>[-0.12310489002602223, -0.09114010555187518, -...</td>\n",
       "      <td>[-0.24092, 0.0007800000000000012, -0.203353, 0...</td>\n",
       "      <td>Resumption of the session</td>\n",
       "      <td>Reprise de la session</td>\n",
       "      <td>[resumption, session]</td>\n",
       "      <td>[reprise, session]</td>\n",
       "      <td>[resumption, session]</td>\n",
       "      <td>[reprise, session]</td>\n",
       "      <td>[[0.13287063559975887, -0.039487647615057006, ...</td>\n",
       "      <td>[[-0.11368, -0.016772, -0.097256, -0.14195, 0....</td>\n",
       "      <td>[session, séance]</td>\n",
       "      <td>[resumed, resume, reprise, session]</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-0.05200671428571431, 0.016875466666666665, -...</td>\n",
       "      <td>[-0.15369624684953903, -0.02870744029014017, -...</td>\n",
       "      <td>[-0.16036031250000002, -0.0502420625, -0.25120...</td>\n",
       "      <td>I declare resumed the session of the European ...</td>\n",
       "      <td>Je déclare reprise la session du Parlement eur...</td>\n",
       "      <td>[declare, resumed, session, european, parliame...</td>\n",
       "      <td>[déclare, reprise, session, parlement, europée...</td>\n",
       "      <td>[declare, resumed, session, european, parliame...</td>\n",
       "      <td>[déclare, reprise, session, parlement, europée...</td>\n",
       "      <td>[[-0.06874171352754704, 0.06292740460440661, -...</td>\n",
       "      <td>[[0.11086, -0.02769, -0.077919, 0.16068, 0.286...</td>\n",
       "      <td>[session, séance, européens, européennes, euro...</td>\n",
       "      <td>[declares, declare, resumed, resume, reprise, ...</td>\n",
       "      <td>21</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-0.08509222727272729, -0.03279808636363638, -...</td>\n",
       "      <td>[-0.1387243927412599, 0.022607814078046323, -0...</td>\n",
       "      <td>[-0.08278590769230772, 0.042981399999999996, -...</td>\n",
       "      <td>Although, as you will have seen, the dreaded'm...</td>\n",
       "      <td>Comme vous avez pu le constater, le grand \"bog...</td>\n",
       "      <td>[although, ,, seen, ,, dreaded, ', millennium,...</td>\n",
       "      <td>[comme, pu, constater, ,, grand, \", bogue, ', ...</td>\n",
       "      <td>[although, seen, dreaded, millennium, bug, fai...</td>\n",
       "      <td>[comme, pu, constater, grand, bogue, an, produ...</td>\n",
       "      <td>[[-0.005225076408981788, 0.1502183274231433, -...</td>\n",
       "      <td>[[-0.021896, -0.075727, 0.18826, 0.21511, 0.07...</td>\n",
       "      <td>[quoique, vu, vus, échec, raté, échoué, still,...</td>\n",
       "      <td>[as, like, large, big, great, grand, product, ...</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-0.064901, -0.08438565454545456, -0.008655272...</td>\n",
       "      <td>[-0.02876501347855113, 0.043874074162248124, -...</td>\n",
       "      <td>[-0.0774160909090909, -0.11717536363636362, -0...</td>\n",
       "      <td>You have requested a debate on this subject in...</td>\n",
       "      <td>Vous avez souhaité un débat à ce sujet dans le...</td>\n",
       "      <td>[requested, debate, subject, course, next, day...</td>\n",
       "      <td>[souhaité, débat, sujet, prochains, jours, ,, ...</td>\n",
       "      <td>[requested, debate, subject, course, next, day...</td>\n",
       "      <td>[souhaité, débat, sujet, prochains, jours, cou...</td>\n",
       "      <td>[[-0.19922813348606175, -0.30227105407400734, ...</td>\n",
       "      <td>[[-0.17082, -0.37877, -0.10921, 0.13303, 0.076...</td>\n",
       "      <td>[demandée, demandé, sollicité, demandés, débat...</td>\n",
       "      <td>[debates, discussion, debate, topic, subject, ...</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-0.07612752, -0.011232175999999993, -0.131726...</td>\n",
       "      <td>[-0.10589229490151733, -0.030987626808951958, ...</td>\n",
       "      <td>[-0.13213240269230775, -0.05523615384615385, -...</td>\n",
       "      <td>In the meantime, I should like to observe a mi...</td>\n",
       "      <td>En attendant, je souhaiterais, comme un certai...</td>\n",
       "      <td>[meantime, ,, like, observe, minute, ', silenc...</td>\n",
       "      <td>[attendant, ,, souhaiterais, ,, comme, certain...</td>\n",
       "      <td>[meantime, like, observe, minute, silence, num...</td>\n",
       "      <td>[attendant, souhaiterais, comme, certain, nomb...</td>\n",
       "      <td>[[-0.2649453375682037, 0.030093366664549925, 0...</td>\n",
       "      <td>[[-0.12316, -0.109, -0.058061, 0.001211, 0.231...</td>\n",
       "      <td>[comme, aime, aiment, genre, aimez, minute, no...</td>\n",
       "      <td>[waiting, as, like, certain, number, requested...</td>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       src_embedding  \\\n",
       "0  [-0.07040999999999999, 0.094428, 0.1178755, -0...   \n",
       "1  [-0.05200671428571431, 0.016875466666666665, -...   \n",
       "2  [-0.08509222727272729, -0.03279808636363638, -...   \n",
       "3  [-0.064901, -0.08438565454545456, -0.008655272...   \n",
       "4  [-0.07612752, -0.011232175999999993, -0.131726...   \n",
       "\n",
       "                               src_embedding_aligned  \\\n",
       "0  [-0.12310489002602223, -0.09114010555187518, -...   \n",
       "1  [-0.15369624684953903, -0.02870744029014017, -...   \n",
       "2  [-0.1387243927412599, 0.022607814078046323, -0...   \n",
       "3  [-0.02876501347855113, 0.043874074162248124, -...   \n",
       "4  [-0.10589229490151733, -0.030987626808951958, ...   \n",
       "\n",
       "                                       trg_embedding  \\\n",
       "0  [-0.24092, 0.0007800000000000012, -0.203353, 0...   \n",
       "1  [-0.16036031250000002, -0.0502420625, -0.25120...   \n",
       "2  [-0.08278590769230772, 0.042981399999999996, -...   \n",
       "3  [-0.0774160909090909, -0.11717536363636362, -0...   \n",
       "4  [-0.13213240269230775, -0.05523615384615385, -...   \n",
       "\n",
       "                                        src_sentence  \\\n",
       "0                          Resumption of the session   \n",
       "1  I declare resumed the session of the European ...   \n",
       "2  Although, as you will have seen, the dreaded'm...   \n",
       "3  You have requested a debate on this subject in...   \n",
       "4  In the meantime, I should like to observe a mi...   \n",
       "\n",
       "                                        trg_sentence  \\\n",
       "0                              Reprise de la session   \n",
       "1  Je déclare reprise la session du Parlement eur...   \n",
       "2  Comme vous avez pu le constater, le grand \"bog...   \n",
       "3  Vous avez souhaité un débat à ce sujet dans le...   \n",
       "4  En attendant, je souhaiterais, comme un certai...   \n",
       "\n",
       "                                    src_preprocessed  \\\n",
       "0                              [resumption, session]   \n",
       "1  [declare, resumed, session, european, parliame...   \n",
       "2  [although, ,, seen, ,, dreaded, ', millennium,...   \n",
       "3  [requested, debate, subject, course, next, day...   \n",
       "4  [meantime, ,, like, observe, minute, ', silenc...   \n",
       "\n",
       "                                    trg_preprocessed  \\\n",
       "0                                 [reprise, session]   \n",
       "1  [déclare, reprise, session, parlement, europée...   \n",
       "2  [comme, pu, constater, ,, grand, \", bogue, ', ...   \n",
       "3  [souhaité, débat, sujet, prochains, jours, ,, ...   \n",
       "4  [attendant, ,, souhaiterais, ,, comme, certain...   \n",
       "\n",
       "                                           src_words  \\\n",
       "0                              [resumption, session]   \n",
       "1  [declare, resumed, session, european, parliame...   \n",
       "2  [although, seen, dreaded, millennium, bug, fai...   \n",
       "3  [requested, debate, subject, course, next, day...   \n",
       "4  [meantime, like, observe, minute, silence, num...   \n",
       "\n",
       "                                           trg_words  \\\n",
       "0                                 [reprise, session]   \n",
       "1  [déclare, reprise, session, parlement, europée...   \n",
       "2  [comme, pu, constater, grand, bogue, an, produ...   \n",
       "3  [souhaité, débat, sujet, prochains, jours, cou...   \n",
       "4  [attendant, souhaiterais, comme, certain, nomb...   \n",
       "\n",
       "                           src_words_found_embedding  \\\n",
       "0  [[0.13287063559975887, -0.039487647615057006, ...   \n",
       "1  [[-0.06874171352754704, 0.06292740460440661, -...   \n",
       "2  [[-0.005225076408981788, 0.1502183274231433, -...   \n",
       "3  [[-0.19922813348606175, -0.30227105407400734, ...   \n",
       "4  [[-0.2649453375682037, 0.030093366664549925, 0...   \n",
       "\n",
       "                           trg_words_found_embedding  \\\n",
       "0  [[-0.11368, -0.016772, -0.097256, -0.14195, 0....   \n",
       "1  [[0.11086, -0.02769, -0.077919, 0.16068, 0.286...   \n",
       "2  [[-0.021896, -0.075727, 0.18826, 0.21511, 0.07...   \n",
       "3  [[-0.17082, -0.37877, -0.10921, 0.13303, 0.076...   \n",
       "4  [[-0.12316, -0.109, -0.058061, 0.001211, 0.231...   \n",
       "\n",
       "                                src_translated_words  \\\n",
       "0                                  [session, séance]   \n",
       "1  [session, séance, européens, européennes, euro...   \n",
       "2  [quoique, vu, vus, échec, raté, échoué, still,...   \n",
       "3  [demandée, demandé, sollicité, demandés, débat...   \n",
       "4  [comme, aime, aiment, genre, aimez, minute, no...   \n",
       "\n",
       "                                trg_translated_words  src_num_words  \\\n",
       "0                [resumed, resume, reprise, session]              2   \n",
       "1  [declares, declare, resumed, resume, reprise, ...             21   \n",
       "2  [as, like, large, big, great, grand, product, ...             17   \n",
       "3  [debates, discussion, debate, topic, subject, ...              8   \n",
       "4  [waiting, as, like, certain, number, requested...             18   \n",
       "\n",
       "   trg_num_words  src_num_punctuation  trg_num_punctuation  \\\n",
       "0              2                    0                    0   \n",
       "1             17                    2                    1   \n",
       "2             18                    6                    9   \n",
       "3              9                    3                    2   \n",
       "4             19                    7                    8   \n",
       "\n",
       "   src_occ_question_mark  trg_occ_question_mark  src_occ_exclamation_mark  \\\n",
       "0                  False                  False                     False   \n",
       "1                  False                  False                     False   \n",
       "2                  False                  False                     False   \n",
       "3                  False                  False                     False   \n",
       "4                  False                  False                     False   \n",
       "\n",
       "   trg_occ_exclamation_mark  \n",
       "0                     False  \n",
       "1                     False  \n",
       "2                     False  \n",
       "3                     False  \n",
       "4                     False  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create test collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- INFO: Preliminary queries dataframe created\n",
      "---- INFO: Preliminary documentes dataframe created\n",
      "---- INFO: Merged queries and documents dataframe\n",
      "---- INFO: Merged with test dataframe\n",
      "---- INFO: Added translation indicator\n",
      "---- DONE: Test collection created\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "test_collection = sens.create_test_collection(n_queries=1000, n_docs=10000)\n",
    "stop = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computation time creating the test collection: 0:01:30.253024\n",
      "Finished at: 2020-05-23 02:58:56.644508\n"
     ]
    }
   ],
   "source": [
    "time(start, stop, 'creating the test collection')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dict = {'text_based': list(text_based.FEATURES.keys()), \n",
    "                 'vector_based': list(vector_based.FEATURES.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- INFO: No vector elements as features specified\n",
      "---- INFO: Started extraction of text-based feature norm_diff_translated_words\n",
      "---- INFO: Started extraction of text-based feature abs_diff_num_words\n",
      "---- INFO: Started extraction of text-based feature abs_diff_num_punctuation\n",
      "---- INFO: Started extraction of text-based feature abs_diff_occ_question_mark\n",
      "---- INFO: Started extraction of text-based feature abs_diff_occ_exclamation_mark\n",
      "---- INFO: Started extraction of text-based feature rel_diff_num_words\n",
      "---- INFO: Started extraction of text-based feature rel_diff_num_punctuation\n",
      "---- INFO: Started extraction of text-based feature norm_diff_num_words\n",
      "---- INFO: Started extraction of text-based feature norm_diff_num_punctuation\n",
      "---- INFO: Started extraction of vector-based feature euclidean_distance\n",
      "---- INFO: Started extraction of vector-based feature cosine_similarity\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "test_collection = sens.extract_features(features_dict=features_dict, data='test', drop_prepared=True)\n",
    "stop = datetime.datetime.now()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computation time extracting the features: 2:26:29.298251\n",
      "Finished at: 2020-05-23 05:25:26.023951\n"
     ]
    }
   ],
   "source": [
    "time(start, stop, 'extracting the features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src_sentence</th>\n",
       "      <th>src_preprocessed</th>\n",
       "      <th>src_embedding</th>\n",
       "      <th>src_embedding_aligned</th>\n",
       "      <th>src_words</th>\n",
       "      <th>src_words_found_embedding</th>\n",
       "      <th>trg_sentence</th>\n",
       "      <th>trg_preprocessed</th>\n",
       "      <th>trg_embedding</th>\n",
       "      <th>trg_words</th>\n",
       "      <th>trg_words_found_embedding</th>\n",
       "      <th>translation</th>\n",
       "      <th>norm_diff_translated_words</th>\n",
       "      <th>abs_diff_num_words</th>\n",
       "      <th>abs_diff_num_punctuation</th>\n",
       "      <th>abs_diff_occ_question_mark</th>\n",
       "      <th>abs_diff_occ_exclamation_mark</th>\n",
       "      <th>rel_diff_num_words</th>\n",
       "      <th>rel_diff_num_punctuation</th>\n",
       "      <th>norm_diff_num_words</th>\n",
       "      <th>norm_diff_num_punctuation</th>\n",
       "      <th>euclidean_distance</th>\n",
       "      <th>cosine_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>After this interesting exchange of opinions, a...</td>\n",
       "      <td>[interesting, exchange, opinions, ,, accordanc...</td>\n",
       "      <td>[-0.07247762499999999, -0.05773741250000001, -...</td>\n",
       "      <td>[-0.07755082538309126, 0.026599344197241616, -...</td>\n",
       "      <td>[interesting, exchange, opinions, accordance, ...</td>\n",
       "      <td>[[0.0160077380810206, 0.24196988362157357, -0....</td>\n",
       "      <td>Après cet intéressant échange d'opinions, conf...</td>\n",
       "      <td>[après, cet, intéressant, échange, ', opinions...</td>\n",
       "      <td>[-0.0852523125, 0.010679999999999997, -0.11436...</td>\n",
       "      <td>[après, cet, intéressant, échange, opinions, c...</td>\n",
       "      <td>[[-0.070378, 0.17455, -0.16929, 0.19593, -0.08...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.282051</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>0.882359</td>\n",
       "      <td>0.881754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>After this interesting exchange of opinions, a...</td>\n",
       "      <td>[interesting, exchange, opinions, ,, accordanc...</td>\n",
       "      <td>[-0.07247762499999999, -0.05773741250000001, -...</td>\n",
       "      <td>[-0.07755082538309126, 0.026599344197241616, -...</td>\n",
       "      <td>[interesting, exchange, opinions, accordance, ...</td>\n",
       "      <td>[[0.0160077380810206, 0.24196988362157357, -0....</td>\n",
       "      <td>L'heure des questions au conseil est close.</td>\n",
       "      <td>[', heure, questions, conseil, close, .]</td>\n",
       "      <td>[-0.06069066666666667, -0.017596166666666677, ...</td>\n",
       "      <td>[heure, questions, conseil, close]</td>\n",
       "      <td>[[-0.14007, -0.03196, -0.065519, 0.051289, 0.0...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.163462</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1.222222</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.488621</td>\n",
       "      <td>0.721150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>After this interesting exchange of opinions, a...</td>\n",
       "      <td>[interesting, exchange, opinions, ,, accordanc...</td>\n",
       "      <td>[-0.07247762499999999, -0.05773741250000001, -...</td>\n",
       "      <td>[-0.07755082538309126, 0.026599344197241616, -...</td>\n",
       "      <td>[interesting, exchange, opinions, accordance, ...</td>\n",
       "      <td>[[0.0160077380810206, 0.24196988362157357, -0....</td>\n",
       "      <td>Impôt sur le capital</td>\n",
       "      <td>[impôt, capital]</td>\n",
       "      <td>[-0.129457, -0.12695800000000002, -0.0954415, ...</td>\n",
       "      <td>[impôt, capital]</td>\n",
       "      <td>[[-0.21623, -0.23299, -0.20862, -0.1637, 0.175...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>1.625000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.805504</td>\n",
       "      <td>0.409065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>After this interesting exchange of opinions, a...</td>\n",
       "      <td>[interesting, exchange, opinions, ,, accordanc...</td>\n",
       "      <td>[-0.07247762499999999, -0.05773741250000001, -...</td>\n",
       "      <td>[-0.07755082538309126, 0.026599344197241616, -...</td>\n",
       "      <td>[interesting, exchange, opinions, accordance, ...</td>\n",
       "      <td>[[0.0160077380810206, 0.24196988362157357, -0....</td>\n",
       "      <td>L'ordre du jour appelle en discussion commune:</td>\n",
       "      <td>[', ordre, jour, appelle, discussion, commune, :]</td>\n",
       "      <td>[-0.12453099999999999, -0.039995, -0.042993183...</td>\n",
       "      <td>[ordre, jour, appelle, discussion, commune]</td>\n",
       "      <td>[[-0.14007, -0.03196, -0.065519, 0.051289, 0.0...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.714162</td>\n",
       "      <td>0.634488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>After this interesting exchange of opinions, a...</td>\n",
       "      <td>[interesting, exchange, opinions, ,, accordanc...</td>\n",
       "      <td>[-0.07247762499999999, -0.05773741250000001, -...</td>\n",
       "      <td>[-0.07755082538309126, 0.026599344197241616, -...</td>\n",
       "      <td>[interesting, exchange, opinions, accordance, ...</td>\n",
       "      <td>[[0.0160077380810206, 0.24196988362157357, -0....</td>\n",
       "      <td>la question orale (B5-0004/ 2000) de M. Désir ...</td>\n",
       "      <td>[question, orale, (, b5, -, 0004, /, 2000, ), ...</td>\n",
       "      <td>[-0.10444363157894736, 0.012410052631578952, -...</td>\n",
       "      <td>[question, orale, désir, autres, conseil, posi...</td>\n",
       "      <td>[[-0.12886, -0.14088, 0.036697, -0.060468, -0....</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-6</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.281260</td>\n",
       "      <td>0.741908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        src_sentence  \\\n",
       "0  After this interesting exchange of opinions, a...   \n",
       "1  After this interesting exchange of opinions, a...   \n",
       "2  After this interesting exchange of opinions, a...   \n",
       "3  After this interesting exchange of opinions, a...   \n",
       "4  After this interesting exchange of opinions, a...   \n",
       "\n",
       "                                    src_preprocessed  \\\n",
       "0  [interesting, exchange, opinions, ,, accordanc...   \n",
       "1  [interesting, exchange, opinions, ,, accordanc...   \n",
       "2  [interesting, exchange, opinions, ,, accordanc...   \n",
       "3  [interesting, exchange, opinions, ,, accordanc...   \n",
       "4  [interesting, exchange, opinions, ,, accordanc...   \n",
       "\n",
       "                                       src_embedding  \\\n",
       "0  [-0.07247762499999999, -0.05773741250000001, -...   \n",
       "1  [-0.07247762499999999, -0.05773741250000001, -...   \n",
       "2  [-0.07247762499999999, -0.05773741250000001, -...   \n",
       "3  [-0.07247762499999999, -0.05773741250000001, -...   \n",
       "4  [-0.07247762499999999, -0.05773741250000001, -...   \n",
       "\n",
       "                               src_embedding_aligned  \\\n",
       "0  [-0.07755082538309126, 0.026599344197241616, -...   \n",
       "1  [-0.07755082538309126, 0.026599344197241616, -...   \n",
       "2  [-0.07755082538309126, 0.026599344197241616, -...   \n",
       "3  [-0.07755082538309126, 0.026599344197241616, -...   \n",
       "4  [-0.07755082538309126, 0.026599344197241616, -...   \n",
       "\n",
       "                                           src_words  \\\n",
       "0  [interesting, exchange, opinions, accordance, ...   \n",
       "1  [interesting, exchange, opinions, accordance, ...   \n",
       "2  [interesting, exchange, opinions, accordance, ...   \n",
       "3  [interesting, exchange, opinions, accordance, ...   \n",
       "4  [interesting, exchange, opinions, accordance, ...   \n",
       "\n",
       "                           src_words_found_embedding  \\\n",
       "0  [[0.0160077380810206, 0.24196988362157357, -0....   \n",
       "1  [[0.0160077380810206, 0.24196988362157357, -0....   \n",
       "2  [[0.0160077380810206, 0.24196988362157357, -0....   \n",
       "3  [[0.0160077380810206, 0.24196988362157357, -0....   \n",
       "4  [[0.0160077380810206, 0.24196988362157357, -0....   \n",
       "\n",
       "                                        trg_sentence  \\\n",
       "0  Après cet intéressant échange d'opinions, conf...   \n",
       "1        L'heure des questions au conseil est close.   \n",
       "2                               Impôt sur le capital   \n",
       "3     L'ordre du jour appelle en discussion commune:   \n",
       "4  la question orale (B5-0004/ 2000) de M. Désir ...   \n",
       "\n",
       "                                    trg_preprocessed  \\\n",
       "0  [après, cet, intéressant, échange, ', opinions...   \n",
       "1           [', heure, questions, conseil, close, .]   \n",
       "2                                   [impôt, capital]   \n",
       "3  [', ordre, jour, appelle, discussion, commune, :]   \n",
       "4  [question, orale, (, b5, -, 0004, /, 2000, ), ...   \n",
       "\n",
       "                                       trg_embedding  \\\n",
       "0  [-0.0852523125, 0.010679999999999997, -0.11436...   \n",
       "1  [-0.06069066666666667, -0.017596166666666677, ...   \n",
       "2  [-0.129457, -0.12695800000000002, -0.0954415, ...   \n",
       "3  [-0.12453099999999999, -0.039995, -0.042993183...   \n",
       "4  [-0.10444363157894736, 0.012410052631578952, -...   \n",
       "\n",
       "                                           trg_words  \\\n",
       "0  [après, cet, intéressant, échange, opinions, c...   \n",
       "1                 [heure, questions, conseil, close]   \n",
       "2                                   [impôt, capital]   \n",
       "3        [ordre, jour, appelle, discussion, commune]   \n",
       "4  [question, orale, désir, autres, conseil, posi...   \n",
       "\n",
       "                           trg_words_found_embedding  translation  \\\n",
       "0  [[-0.070378, 0.17455, -0.16929, 0.19593, -0.08...            1   \n",
       "1  [[-0.14007, -0.03196, -0.065519, 0.051289, 0.0...            0   \n",
       "2  [[-0.21623, -0.23299, -0.20862, -0.1637, 0.175...            0   \n",
       "3  [[-0.14007, -0.03196, -0.065519, 0.051289, 0.0...            0   \n",
       "4  [[-0.12886, -0.14088, 0.036697, -0.060468, -0....            0   \n",
       "\n",
       "   norm_diff_translated_words  abs_diff_num_words  abs_diff_num_punctuation  \\\n",
       "0                    0.282051                   1                         1   \n",
       "1                    0.163462                  11                         1   \n",
       "2                    0.000000                  13                         3   \n",
       "3                    0.000000                  10                         1   \n",
       "4                    0.000000                   2                         6   \n",
       "\n",
       "   abs_diff_occ_question_mark  abs_diff_occ_exclamation_mark  \\\n",
       "0                           1                              1   \n",
       "1                           1                              1   \n",
       "2                           1                              1   \n",
       "3                           1                              1   \n",
       "4                           1                              1   \n",
       "\n",
       "   rel_diff_num_words  rel_diff_num_punctuation  norm_diff_num_words  \\\n",
       "0                   1                        -1             0.071429   \n",
       "1                  11                         1             1.222222   \n",
       "2                  13                         3             1.625000   \n",
       "3                  10                         1             1.000000   \n",
       "4                   2                        -6             0.142857   \n",
       "\n",
       "   norm_diff_num_punctuation  euclidean_distance  cosine_similarity  \n",
       "0                  -0.333333            0.882359           0.881754  \n",
       "1                   0.500000            1.488621           0.721150  \n",
       "2                   3.000000            3.805504           0.409065  \n",
       "3                   0.500000            1.714162           0.634488  \n",
       "4                  -1.000000            1.281260           0.741908  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_collection.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = [test_collection[i:(i+math.ceil(len(test_collection)/20))] \n",
    "          for i in range(0,len(test_collection),math.ceil(len(test_collection)/20))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f'{paths.data_path}extracted_data/global/en-fr'\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- INFO: Chunk 00 saved\n",
      "---- INFO: Chunk 01 saved\n",
      "---- INFO: Chunk 02 saved\n",
      "---- INFO: Chunk 03 saved\n",
      "---- INFO: Chunk 04 saved\n",
      "---- INFO: Chunk 05 saved\n",
      "---- INFO: Chunk 06 saved\n",
      "---- INFO: Chunk 07 saved\n",
      "---- INFO: Chunk 08 saved\n",
      "---- INFO: Chunk 09 saved\n",
      "---- INFO: Chunk 10 saved\n",
      "---- INFO: Chunk 11 saved\n",
      "---- INFO: Chunk 12 saved\n",
      "---- INFO: Chunk 13 saved\n",
      "---- INFO: Chunk 14 saved\n",
      "---- INFO: Chunk 15 saved\n",
      "---- INFO: Chunk 16 saved\n",
      "---- INFO: Chunk 17 saved\n",
      "---- INFO: Chunk 18 saved\n",
      "---- INFO: Chunk 19 saved\n",
      "Computation time saving test collection: 0:01:19.946966\n",
      "Finished at: 2020-05-23 05:26:47.384690\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "for idx, data in enumerate(chunks):\n",
    "    data.to_pickle(f'{path}/test_collection_{idx:02d}_avg.pkl')\n",
    "    print(f'---- INFO: Chunk {idx:02d} saved')\n",
    "time(start, datetime.datetime.now(), 'saving test collection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Total computation time: 2:29:27.862083'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_stop = datetime.datetime.now()\n",
    "'Total computation time: {}'.format(overall_stop-overall_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- INFO: Learn projection matrix for fr-en\n",
      "---- INFO: Found 7938 valid translation pairs in expert dictionary.\n",
      "---- INFO: 332 other pairs contained at least one unknown word (0 in source language, 332 in target language).\n",
      "---- DONE: Seed dictionary extracted for the languages: fr-en\n",
      "---- INFO: Resulting subspace dimension: (7938, 300)\n",
      "---- INFO: Resulting subspace dimension: (7938, 300)\n",
      "---- DONE: Projection matrix learned from fr to en\n",
      "---- INFO: Learn projection matrix for en-fr\n",
      "---- INFO: Found 10369 valid translation pairs in expert dictionary.\n",
      "---- INFO: 503 other pairs contained at least one unknown word (0 in source language, 503 in target language).\n",
      "---- DONE: Seed dictionary extracted for the languages: en-fr\n",
      "---- INFO: Resulting subspace dimension: (10369, 300)\n",
      "---- INFO: Resulting subspace dimension: (10369, 300)\n",
      "---- DONE: Projection matrix learned from en to fr\n"
     ]
    }
   ],
   "source": [
    "W_fren, W_enfr = embeddings.WordEmbeddings.learn_projection_matrix(src_lang='fr', trg_lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- DONE: Target sentences loaded\n",
      "---- DONE: Source sentences loaded\n",
      "---- DONE: Sentences preprocessed\n",
      "---- ERROR: Sentence embedding failed in fr on ID 6544: Altener\n",
      "---- ERROR: Sentence embedding failed in fr on ID 10259: La tremblante\n",
      "---- ERROR: Sentence embedding failed in fr on ID 7439: et\n",
      "---- INFO: Sentences embeddings extracted in fr\n",
      "---- ERROR: Sentence embedding failed in en on ID 6544: ALTENER\n",
      "---- ERROR: Sentence embedding failed in en on ID 10259: Scrapie\n",
      "---- ERROR: Sentence embedding failed in en on ID 7439: and\n",
      "---- INFO: Sentences embeddings extracted in en\n",
      "---- INFO: Shape of source word embeddings\n",
      "---- INFO: Extracted word embeddings of found words\n",
      "---- DONE: Sentences transformed\n",
      "---- INFO: Embedding space of source language transformed according to projection matrix\n",
      "---- DONE: Source words extracted\n",
      "---- DONE: Target words extracted\n",
      "---- INFO: Embeddings of found words added as a column\n",
      "---- INFO: Start preparation of text-based feature translated_words\n",
      "---- INFO: Start preparation of text-based feature num_words\n",
      "---- INFO: Start preparation of text-based feature num_punctuation\n",
      "---- INFO: Start preparation of text-based feature occ_question_mark\n",
      "---- INFO: Start preparation of text-based feature occ_exclamation_mark\n",
      "---- DONE: All features prepared\n",
      "---- INFO: Dropped duplicates\n",
      "---- INFO: Delete sentences containing only a '.'\n",
      "---- DONE: Data loaded. Length of dataset after preprocessing and duplicate handling: 14864\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "sens = sentences.Sentences(src_words=french, trg_words=english)\n",
    "prepared_features = list(text_based.PREPARED_FEATURES.keys())\n",
    "data = sens.load_data(single_source=False, n_max=15000, features=prepared_features, agg_method='average')\n",
    "stop = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- INFO: Preliminary queries dataframe created\n",
      "---- INFO: Preliminary documentes dataframe created\n",
      "---- INFO: Merged queries and documents dataframe\n",
      "---- INFO: Merged with test dataframe\n",
      "---- INFO: Added translation indicator\n",
      "---- DONE: Test collection created\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "test_collection = sens.create_test_collection(n_queries=1000, n_docs=10000)\n",
    "stop = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dict = {'text_based': list(text_based.FEATURES.keys()), \n",
    "                 'vector_based': list(vector_based.FEATURES.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- INFO: No vector elements as features specified\n",
      "---- INFO: Started extraction of text-based feature norm_diff_translated_words\n",
      "---- INFO: Started extraction of text-based feature abs_diff_num_words\n",
      "---- INFO: Started extraction of text-based feature abs_diff_num_punctuation\n",
      "---- INFO: Started extraction of text-based feature abs_diff_occ_question_mark\n",
      "---- INFO: Started extraction of text-based feature abs_diff_occ_exclamation_mark\n",
      "---- INFO: Started extraction of text-based feature rel_diff_num_words\n",
      "---- INFO: Started extraction of text-based feature rel_diff_num_punctuation\n",
      "---- INFO: Started extraction of text-based feature norm_diff_num_words\n",
      "---- INFO: Started extraction of text-based feature norm_diff_num_punctuation\n",
      "---- INFO: Started extraction of vector-based feature euclidean_distance\n",
      "---- INFO: Started extraction of vector-based feature cosine_similarity\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "test_collection = sens.extract_features(features_dict=features_dict, data='test', drop_prepared=True)\n",
    "stop = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = [test_collection[i:(i+math.ceil(len(test_collection)/20))] \n",
    "          for i in range(0,len(test_collection),math.ceil(len(test_collection)/20))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f'{paths.data_path}extracted_data/global/fr-en'\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- INFO: Chunk 00 saved\n",
      "---- INFO: Chunk 01 saved\n",
      "---- INFO: Chunk 02 saved\n",
      "---- INFO: Chunk 03 saved\n",
      "---- INFO: Chunk 04 saved\n",
      "---- INFO: Chunk 05 saved\n",
      "---- INFO: Chunk 06 saved\n",
      "---- INFO: Chunk 07 saved\n",
      "---- INFO: Chunk 08 saved\n",
      "---- INFO: Chunk 09 saved\n",
      "---- INFO: Chunk 10 saved\n",
      "---- INFO: Chunk 11 saved\n",
      "---- INFO: Chunk 12 saved\n",
      "---- INFO: Chunk 13 saved\n",
      "---- INFO: Chunk 14 saved\n",
      "---- INFO: Chunk 15 saved\n",
      "---- INFO: Chunk 16 saved\n",
      "---- INFO: Chunk 17 saved\n",
      "---- INFO: Chunk 18 saved\n",
      "---- INFO: Chunk 19 saved\n",
      "Computation time saving test collection: 0:01:09.492008\n",
      "Finished at: 2020-05-23 07:56:37.035586\n"
     ]
    }
   ],
   "source": [
    "start = datetime.datetime.now()\n",
    "for idx, data in enumerate(chunks):\n",
    "    data.to_pickle(f'{path}/test_collection_{idx:02d}_avg.pkl')\n",
    "    print(f'---- INFO: Chunk {idx:02d} saved')\n",
    "time(start, datetime.datetime.now(), 'saving test collection')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For tfidf only: Save idf values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idf_en = {k: sens.vectorizer['en'].idf_[v] for k,v in sens.vectorizer['en'].vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idf_de = {k: sens.vectorizer['de'].idf_[v] for k,v in sens.vectorizer['de'].vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(f'{path}/idf_english.json', 'wb') as fp:\n",
    "#    pickle.dump(idf_en, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "#'Finished at: {}'.format(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(f'{path}/idf_german.json', 'wb') as fp:\n",
    "#    pickle.dump(idf_de, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "#'Finished at: {}'.format(datetime.datetime.now())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
