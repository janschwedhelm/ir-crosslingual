{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import importlib, datetime\n",
    "\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'ir_crosslingual.sentences.sentences' from '/Users/jani/PycharmProjects/ir-crosslingual/ir_crosslingual/sentences/sentences.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ir_crosslingual.supervised_classification import sup_model\n",
    "importlib.reload(sup_model)\n",
    "\n",
    "from ir_crosslingual.utils import paths\n",
    "importlib.reload(paths)\n",
    "\n",
    "from ir_crosslingual.embeddings import embeddings\n",
    "importlib.reload(embeddings)\n",
    "\n",
    "from ir_crosslingual.sentences import sentences\n",
    "importlib.reload(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- INFO: Learn projection matrix for en-de\n",
      "---- INFO: Found 13700 valid translation pairs in expert dictionary.\n",
      "---- INFO: 977 other pairs contained at least one unknown word (0 in source language, 977 in target language).\n",
      "---- DONE: Seed dictionary extracted for the languages: en-de\n",
      "---- INFO: Resulting subspace dimension: (13700, 300)\n",
      "---- INFO: Resulting subspace dimension: (13700, 300)\n",
      "---- DONE: Projection matrix from en to de\n",
      "---- INFO: Learn projection matrix for de-en\n",
      "---- INFO: Found 10604 valid translation pairs in expert dictionary.\n",
      "---- INFO: 262 other pairs contained at least one unknown word (0 in source language, 262 in target language).\n",
      "---- DONE: Seed dictionary extracted for the languages: de-en\n",
      "---- INFO: Resulting subspace dimension: (10604, 300)\n",
      "---- INFO: Resulting subspace dimension: (10604, 300)\n",
      "---- DONE: Projection matrix from de to en\n"
     ]
    }
   ],
   "source": [
    "train_file_avg = f'{paths.data_path}extracted_data/global/training_data_avg.pkl'\n",
    "test_file_avg = f'{paths.data_path}extracted_data/global/test_collection_avg.pkl'\n",
    "sens_avg, train_data_avg, test_collection_avg, features_avg = sentences.Sentences.load_from_file(train_file_avg, test_file_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_collection_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline MLP Model (averaging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract sentence embedding elements as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# extract for target and source language:\n",
    "# 1) vector elements of 300-dim sentence embeddings\n",
    "# 2) vector elements of dimension reduced sentence embeddings\n",
    "dim = 20\n",
    "for prefix in ['src', 'trg']:\n",
    "    train_data_avg[['{}_embedding_pca_{}'.format(prefix, i) for i in range(dim)]] = pd.DataFrame(sens_avg.reduce_dim(train_data_avg['{}_embedding'.format(prefix)], dim, use_ppa=False).tolist())\n",
    "    print('1')\n",
    "    test_collection_avg[['{}_embedding_pca_{}'.format(prefix, i) for i in range(dim)]] = pd.DataFrame(sens_avg.reduce_dim(test_collection_avg['{}_embedding'.format(prefix)], dim, use_ppa=False).tolist())\n",
    "    print('2')\n",
    "    train_data_avg[['{}_embedding_{}'.format(prefix, i) for i in range(300)]] = pd.DataFrame(train_data_avg['{}_embedding'.format(prefix)].tolist())\n",
    "    print('3')\n",
    "    test_collection_avg[['{}_embedding_{}'.format(prefix, i) for i in range(300)]] = pd.DataFrame(test_collection_avg['{}_embedding'.format(prefix)].tolist())\n",
    "    print('4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 1) Use self-selected features based on intuition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_1 = 'norm_diff_translated_words norm_diff_num_punctuation abs_diff_occ_question_mark abs_diff_occ_exclamation_mark norm_diff_num_words euclidean_distance cosine_similarity'.split()\n",
    "label = 'translation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computation time fitting the multilayer perceptron: 0:00:11.665122\n",
      "Accuracy: 0.9707365\n",
      "Precision: 0.0032398070424070643\n",
      "Recall: 0.9500499500499501\n",
      "F1: 0.006457592764236751\n",
      "MAP: 0.7844755124611402\n"
     ]
    }
   ],
   "source": [
    "# Create baseline MLP classifier\n",
    "mlp_1 = MLPClassifier(solver='lbfgs', hidden_layer_sizes=(4,), random_state=1, learning_rate='adaptive', activation='tanh')\n",
    "\n",
    "# Fit baseline model on training data\n",
    "start = datetime.datetime.now()\n",
    "mlp_1.fit(train_data_avg[features_1], train_data_avg[label])\n",
    "stop = datetime.datetime.now()\n",
    "print('Computation time fitting the multilayer perceptron: {}'.format(stop-start))\n",
    "\n",
    "# Evaluate baseline model on test collection\n",
    "sup = sup_model.SupModel()\n",
    "sup.evaluate_boolean(mlp_1, sens_avg, features_1)\n",
    "print('Accuracy: {}'.format(sup.accuracy))\n",
    "print('Precision: {}'.format(sup.precision))\n",
    "print('Recall: {}'.format(sup.recall))\n",
    "print('F1: {}'.format(sup.f1))\n",
    "\n",
    "print('MAP: {}'.format(sup.compute_map(mlp_1, sens_avg, features_1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 2) Use extracted elements of full sentence embedding (300-dim) as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_2 = ['src_embedding_{}'.format(i) for i in range(300)] + ['trg_embedding_{}'.format(i) for i in range(300)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create baseline MLP classifier\n",
    "mlp_2 = MLPClassifier(solver='lbfgs', hidden_layer_sizes=(15,15,10), random_state=1, learning_rate='adaptive', activation='tanh')\n",
    "\n",
    "# Fit baseline model on training data\n",
    "start = datetime.datetime.now()\n",
    "mlp_2.fit(train_data_avg[features_1], train_data_avg[label])\n",
    "stop = datetime.datetime.now()\n",
    "print('Computation time fitting the multilayer perceptron: {}'.format(stop-start))\n",
    "\n",
    "# Evaluate baseline model on test collection\n",
    "sup = sup_model.SupModel()\n",
    "sup.evaluate_boolean(mlp_2, sens_avg, features_2)\n",
    "print('Accuracy: {}'.format(sup.accuracy))\n",
    "print('Precision: {}'.format(sup.precision))\n",
    "print('Recall: {}'.format(sup.recall))\n",
    "print('F1: {}'.format(sup.f1))\n",
    "\n",
    "print('MAP: {}'.format(sup.compute_map(mlp_2, sens_avg, features_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 3) Use self-selected features + extracted elements of dimension reduced sentence embedding (20-dim) as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_3 = features_1 + ['src_embedding_pca_{}'.format(i) for i in range(dim)] + ['trg_embedding_pca_{}'.format(i) for i in range(dim)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create baseline MLP classifier\n",
    "mlp_3 = MLPClassifier(solver='lbfgs', hidden_layer_sizes=(4,3), random_state=1, learning_rate='adaptive', activation='tanh')\n",
    "\n",
    "# Fit baseline model on training data\n",
    "start = datetime.datetime.now()\n",
    "mlp_3.fit(train_data_avg[features_3], train_data_avg[label])\n",
    "stop = datetime.datetime.now()\n",
    "print('Computation time fitting the multilayer perceptron: {}'.format(stop-start))\n",
    "\n",
    "# Evaluate baseline model on test collection\n",
    "sup = sup_model.SupModel()\n",
    "sup.evaluate_boolean(mlp_3, sens_avg, features_3)\n",
    "print('Accuracy: {}'.format(sup.accuracy))\n",
    "print('Precision: {}'.format(sup.precision))\n",
    "print('Recall: {}'.format(sup.recall))\n",
    "print('F1: {}'.format(sup.f1))\n",
    "\n",
    "print('MAP: {}'.format(sup.compute_map(mlp_3, sens_avg, features_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create random grid for tuning best model (variant 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hidden layers\n",
    "hidden_layers = [(i,) for i in range(10)] + [(2,2), (2,2,2), (3,3), (4,2,1), (3,3,1)]\n",
    "\n",
    "# Activation function\n",
    "activation = ['identity', 'logistic', 'tanh', 'relu']\n",
    "\n",
    "# Optimizer\n",
    "optimizer = ['lbfgs', 'sgd', 'adam']\n",
    "\n",
    "# Alpha\n",
    "alpha = [0.1, 0.01, 0.001, 0.0001]\n",
    "\n",
    "# Batch size\n",
    "batch_size = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "\n",
    "# Learning rate\n",
    "learning_rate = ['constant', 'invscaling', 'adaptive']\n",
    "\n",
    "# Learning rate init\n",
    "learning_rate_init = [0.1, 0.01, 0.001, 0.0001]\n",
    "\n",
    "# Maximum iterations\n",
    "max_iter = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_grid = {\n",
    "    'hidden_layer_sizes': hidden_layers,\n",
    "    'activation': activation,\n",
    "    'solver': optimizer,\n",
    "    'alpha': alpha,\n",
    "    'batch_size': batch_size,\n",
    "    'learning_rate': learning_rate,\n",
    "    'learning_rate_init': learning_rate_init,\n",
    "    'max_iter': max_iter\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform random search for optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:   11.2s\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   14.4s\n",
      "[Parallel(n_jobs=-1)]: Done   3 tasks      | elapsed:   14.7s\n",
      "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:   15.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:   23.6s\n",
      "[Parallel(n_jobs=-1)]: Done   6 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done   7 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done  11 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done  12 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done  13 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done  15 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done  19 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done  20 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done  22 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done  23 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done  27 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done  30 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done  31 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done  32 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done  35 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done  36 tasks      | elapsed:  5.4min\n",
      "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=-1)]: Done  39 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=-1)]: Done  41 tasks      | elapsed:  5.7min\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  5.7min\n",
      "[Parallel(n_jobs=-1)]: Done  43 tasks      | elapsed:  5.7min\n",
      "[Parallel(n_jobs=-1)]: Done  46 out of  50 | elapsed:  5.8min remaining:   30.3s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:  5.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=StratifiedKFold(n_splits=5, random_state=42, shuffle=False),\n",
       "                   error_score=nan,\n",
       "                   estimator=MLPClassifier(activation='relu', alpha=0.0001,\n",
       "                                           batch_size='auto', beta_1=0.9,\n",
       "                                           beta_2=0.999, early_stopping=False,\n",
       "                                           epsilon=1e-08,\n",
       "                                           hidden_layer_sizes=(100,),\n",
       "                                           learning_rate='constant',\n",
       "                                           learning_rate_init=0.001,\n",
       "                                           max_fun=15000, max_iter=200,\n",
       "                                           momentum=0.9, n_iter...\n",
       "                                                               (3,), (4,), (5,),\n",
       "                                                               (6,), (7,), (8,),\n",
       "                                                               (9,), (2, 2),\n",
       "                                                               (2, 2, 2),\n",
       "                                                               (3, 3),\n",
       "                                                               (4, 2, 1),\n",
       "                                                               (3, 3, 1)],\n",
       "                                        'learning_rate': ['constant',\n",
       "                                                          'invscaling',\n",
       "                                                          'adaptive'],\n",
       "                                        'learning_rate_init': [0.1, 0.01, 0.001,\n",
       "                                                               0.0001],\n",
       "                                        'max_iter': [200, 400, 600, 800, 1000,\n",
       "                                                     1200, 1400, 1600, 1800,\n",
       "                                                     2000],\n",
       "                                        'solver': ['lbfgs', 'sgd', 'adam']},\n",
       "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=20)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = StratifiedKFold(n_splits=5, random_state=42)\n",
    "mlp = MLPClassifier()\n",
    "randomized_search = RandomizedSearchCV(estimator = mlp, param_distributions = parameter_grid, n_iter = 10, \n",
    "                                cv = cv, verbose=20, random_state=42, n_jobs = -1)\n",
    "randomized_search.fit(train_data_avg[features_1], train_data_avg[label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate random search to identify optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'solver': 'lbfgs',\n",
       " 'max_iter': 200,\n",
       " 'learning_rate_init': 0.001,\n",
       " 'learning_rate': 'adaptive',\n",
       " 'hidden_layer_sizes': (8,),\n",
       " 'batch_size': 1200,\n",
       " 'alpha': 0.01,\n",
       " 'activation': 'relu'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify best hyperparameters retrieved by random search\n",
    "randomized_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9471400000000001"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify best hyperparameters retrieved by random search\n",
    "randomized_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.01, batch_size=1200, beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(8,), learning_rate='adaptive',\n",
       "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "              power_t=0.5, random_state=None, shuffle=True, solver='lbfgs',\n",
       "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit best model on training data\n",
    "best_model = randomized_search.best_estimator_\n",
    "best_model.fit(train_data_avg[features_1], train_data_avg[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computation time fitting the multilayer perceptron: 0:00:16.033537\n",
      "Accuracy: 0.9698826\n",
      "Precision: 0.003158093359065946\n",
      "Recall: 0.9530469530469531\n",
      "F1: 0.006295326017381435\n",
      "MAP: 0.7851291324638691\n"
     ]
    }
   ],
   "source": [
    "# Evaluate best model on test collection (2nd run)\n",
    "start = datetime.datetime.now()\n",
    "best_model.fit(train_data_avg[features_1], train_data_avg[label])\n",
    "stop = datetime.datetime.now()\n",
    "print('Computation time fitting the multilayer perceptron: {}'.format(stop-start))\n",
    "\n",
    "# Evaluation on test collection\n",
    "sup = sup_model.SupModel()\n",
    "sup.evaluate_boolean(best_model, sens_avg, features_1)\n",
    "print('Accuracy: {}'.format(sup.accuracy))\n",
    "print('Precision: {}'.format(sup.precision))\n",
    "print('Recall: {}'.format(sup.recall))\n",
    "print('F1: {}'.format(sup.f1))\n",
    "\n",
    "print('MAP: {}'.format(sup.compute_map(best_model, sens_avg, features_1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
