{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import importlib, datetime\n",
    "\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'ir_crosslingual.sentences.sentences' from '/Users/i500969/Desktop/Admin/Uni-Mannheim/02_Courses/2020_FSS/Information-Retrieval/03_Project/03_Implementation/03_Feature-Selection/ir-crosslingual/ir_crosslingual/sentences/sentences.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ir_crosslingual.supervised_classification import sup_model\n",
    "importlib.reload(sup_model)\n",
    "\n",
    "from ir_crosslingual.utils import paths\n",
    "importlib.reload(paths)\n",
    "\n",
    "from ir_crosslingual.embeddings import embeddings\n",
    "importlib.reload(embeddings)\n",
    "\n",
    "from ir_crosslingual.sentences import sentences\n",
    "importlib.reload(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learn projection matrix for en-de\n",
      "Found 13700 valid translation pairs in expert dictionary.\n",
      "977 other pairs contained at least one unknown word (0 in source language, 977 in target language).\n",
      "Resulting subspace dimension: (13700, 300)\n",
      "Resulting subspace dimension: (13700, 300)\n",
      "Learn projection matrix for de-en\n",
      "Found 10604 valid translation pairs in expert dictionary.\n",
      "262 other pairs contained at least one unknown word (0 in source language, 262 in target language).\n",
      "Resulting subspace dimension: (10604, 300)\n",
      "Resulting subspace dimension: (10604, 300)\n"
     ]
    }
   ],
   "source": [
    "train_file = f'{paths.data_path}extracted-data/training_data_v0.0.pkl'\n",
    "test_file = f'{paths.data_path}extracted-data/test_collection_v0.0.pkl'\n",
    "sens, train_data, test_collection, features = sentences.Sentences.load_from_file(train_file, test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "german = embeddings.WordEmbeddings('de')\n",
    "german.load_embeddings()\n",
    "\n",
    "english = embeddings.WordEmbeddings('en')\n",
    "english.load_embeddings()\n",
    "\n",
    "W_ende, W_deen = embeddings.WordEmbeddings.learn_projection_matrix(src_lang='en', trg_lang='de')\n",
    "\n",
    "prepared_features = ['num_words', 'num_punctuation', 'occ_question_mark', 'occ_exclamation_mark']\n",
    "sens = sentences.Sentences(src_words=english, trg_words=german)\n",
    "data = sens.load_data(single_source=False, n_max=5000, features=prepared_features, agg_method='average')\n",
    "\n",
    "features_dict = {'text_based': ['diff_{}'.format(feat) for feat in prepared_features], \n",
    "                 'vector_based': ['cosine_similarity']}\n",
    "features = [feature for values in features_dict.values() for feature in values]\n",
    "\n",
    "train_data = sens.create_train_set(n_train=4000, frac_pos=0.5)\n",
    "test_data = sens.create_test_collection(n_queries=50, n_docs=996)\n",
    "\n",
    "train_data, test_data = sens.extract_features(features_dict=features_dict, data='train_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = 'diff_num_words diff_num_punctuation diff_occ_question_mark diff_occ_exclamation_mark cosine_similarity'.split()\n",
    "label = 'translation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computation time fitting the multilayer perceptron: 0:00:00.631952\n",
      "Accuracy: 0.8780923694779117\n",
      "Precision: 0.00784698381559588\n",
      "Recall: 0.96\n",
      "F1: 0.015566726122912275\n",
      "MAP: 0.8122558280755896\n"
     ]
    }
   ],
   "source": [
    "# Create baseline MLP classifier\n",
    "mlp = MLPClassifier(solver='lbfgs', hidden_layer_sizes=(5, 4, 3, 2), random_state=1, learning_rate='adaptive', activation='tanh')\n",
    "\n",
    "# Fit baseline model on training data\n",
    "start = datetime.datetime.now()\n",
    "mlp.fit(train_data[features], train_data[label])\n",
    "stop = datetime.datetime.now()\n",
    "print('Computation time fitting the multilayer perceptron: {}'.format(stop-start))\n",
    "\n",
    "# Evaluate baseline model on test collection\n",
    "sup = sup_model.SupModel()\n",
    "sup.evaluate_boolean(mlp, sens, features)\n",
    "print('Accuracy: {}'.format(sup.accuracy))\n",
    "print('Precision: {}'.format(sup.precision))\n",
    "print('Recall: {}'.format(sup.recall))\n",
    "print('F1: {}'.format(sup.f1))\n",
    "\n",
    "print('MAP: {}'.format(sup.compute_map(mlp, sens, features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computation time fitting the multilayer perceptron: 0:00:00.649427\n",
      "Accuracy: 0.8926305220883534\n",
      "Precision: 0.008900426478768774\n",
      "Recall: 0.96\n",
      "F1: 0.017637332353481534\n",
      "MAP: 0.8279756873623236\n"
     ]
    }
   ],
   "source": [
    "# Create baseline MLP classifier\n",
    "mlp = MLPClassifier(solver='lbfgs', hidden_layer_sizes=(5, 4, 3, 2), random_state=1, learning_rate='adaptive', activation='tanh')\n",
    "\n",
    "# Fit baseline model on training data\n",
    "start = datetime.datetime.now()\n",
    "mlp.fit(train_data[features], train_data[label])\n",
    "stop = datetime.datetime.now()\n",
    "print('Computation time fitting the multilayer perceptron: {}'.format(stop-start))\n",
    "\n",
    "# Evaluate baseline model on test collection\n",
    "sup = sup_model.SupModel()\n",
    "sup.evaluate_boolean(mlp, sens)\n",
    "print('Accuracy: {}'.format(sup.accuracy))\n",
    "print('Precision: {}'.format(sup.precision))\n",
    "print('Recall: {}'.format(sup.recall))\n",
    "print('F1: {}'.format(sup.f1))\n",
    "\n",
    "print('MAP: {}'.format(sup.compute_map(mlp, sens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create random grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hidden layers\n",
    "hidden_layers = [(5,4,3,2), (5,4,2), (5,3,2), (4,3,2), (5,2), (4,2), (3,2)]\n",
    "\n",
    "# Activation function\n",
    "activation = ['identity', 'logistic', 'tanh', 'relu']\n",
    "\n",
    "# Optimizer\n",
    "optimizer = ['lbfgs', 'sgd', 'adam']\n",
    "\n",
    "# Alpha\n",
    "alpha = [0.1, 0.01, 0.001, 0.0001]\n",
    "\n",
    "# Batch size\n",
    "batch_size = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "\n",
    "# Learning rate\n",
    "learning_rate = ['constant', 'invscaling', 'adaptive']\n",
    "\n",
    "# Learning rate init\n",
    "learning_rate_init = [0.1, 0.01, 0.001, 0.0001]\n",
    "\n",
    "# Maximum iterations\n",
    "max_iter = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_grid = {\n",
    "    'hidden_layer_sizes': hidden_layers,\n",
    "    'activation': activation,\n",
    "    'solver': optimizer,\n",
    "    'alpha': alpha,\n",
    "    'batch_size': batch_size,\n",
    "    'learning_rate': learning_rate,\n",
    "    'learning_rate_init': learning_rate_init,\n",
    "    'max_iter': max_iter\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform random search for optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=-1)]: Done   3 tasks      | elapsed:    3.4s\n",
      "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done   6 tasks      | elapsed:    3.7s\n",
      "[Parallel(n_jobs=-1)]: Done   7 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    4.2s\n",
      "[Parallel(n_jobs=-1)]: Done  11 tasks      | elapsed:    4.3s\n",
      "[Parallel(n_jobs=-1)]: Done  12 tasks      | elapsed:    4.5s\n",
      "[Parallel(n_jobs=-1)]: Done  13 tasks      | elapsed:    4.6s\n",
      "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:    4.6s\n",
      "[Parallel(n_jobs=-1)]: Done  15 tasks      | elapsed:    4.6s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    4.7s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    4.8s\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    6.0s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  25 | elapsed:    6.9s remaining:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  25 | elapsed:   10.1s remaining:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:   12.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=StratifiedKFold(n_splits=5, random_state=42, shuffle=False),\n",
       "                   error_score=nan,\n",
       "                   estimator=MLPClassifier(activation='relu', alpha=0.0001,\n",
       "                                           batch_size='auto', beta_1=0.9,\n",
       "                                           beta_2=0.999, early_stopping=False,\n",
       "                                           epsilon=1e-08,\n",
       "                                           hidden_layer_sizes=(100,),\n",
       "                                           learning_rate='constant',\n",
       "                                           learning_rate_init=0.001,\n",
       "                                           max_fun=15000, max_iter=200,\n",
       "                                           momentum=0.9, n_iter...\n",
       "                                        'hidden_layer_sizes': [(5, 4, 3, 2),\n",
       "                                                               (5, 4, 2),\n",
       "                                                               (5, 3, 2),\n",
       "                                                               (4, 3, 2),\n",
       "                                                               (5, 2), (4, 2),\n",
       "                                                               (3, 2)],\n",
       "                                        'learning_rate': ['constant',\n",
       "                                                          'invscaling',\n",
       "                                                          'adaptive'],\n",
       "                                        'learning_rate_init': [0.1, 0.01, 0.001,\n",
       "                                                               0.0001],\n",
       "                                        'max_iter': [200, 400, 600, 800, 1000,\n",
       "                                                     1200, 1400, 1600, 1800,\n",
       "                                                     2000],\n",
       "                                        'solver': ['lbfgs', 'sgd', 'adam']},\n",
       "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=20)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = StratifiedKFold(n_splits=5, random_state=42)\n",
    "mlp = MLPClassifier()\n",
    "randomized_search = RandomizedSearchCV(estimator = mlp, param_distributions = parameter_grid, n_iter = 100, \n",
    "                                cv = cv, verbose=20, random_state=42, n_jobs = -1)\n",
    "randomized_search.fit(train_data[features], train_data[label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate random search to identify optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'solver': 'adam',\n",
       " 'max_iter': 600,\n",
       " 'learning_rate_init': 0.01,\n",
       " 'learning_rate': 'adaptive',\n",
       " 'hidden_layer_sizes': (5, 3, 2),\n",
       " 'batch_size': 1800,\n",
       " 'alpha': 0.1,\n",
       " 'activation': 'logistic'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify best hyperparameters retrieved by random search\n",
    "randomized_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8172499999999999"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify best hyperparameters retrieved by random search (2nd run)\n",
    "randomized_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8937499999999998"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify best hyperparameters retrieved by random search (1st run)\n",
    "randomized_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', alpha=0.1, batch_size=1800, beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(5, 3, 2), learning_rate='adaptive',\n",
       "              learning_rate_init=0.01, max_fun=15000, max_iter=600,\n",
       "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit best model on training data\n",
    "best_model = randomized_search.best_estimator_\n",
    "best_model.fit(train_data[features], train_data[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computation time fitting the multilayer perceptron: 0:00:00.873283\n",
      "Accuracy: 0.8624497991967871\n",
      "Precision: 0.007103508263264714\n",
      "Recall: 0.98\n",
      "F1: 0.014104778353483017\n",
      "MAP: 0.7354742504409171\n"
     ]
    }
   ],
   "source": [
    "# Evaluate best model on test collection (2nd run)\n",
    "start = datetime.datetime.now()\n",
    "best_model.fit(train_data[features], train_data[label])\n",
    "stop = datetime.datetime.now()\n",
    "print('Computation time fitting the multilayer perceptron: {}'.format(stop-start))\n",
    "\n",
    "# Evaluation on test collection\n",
    "sup = sup_model.SupModel()\n",
    "sup.evaluate_boolean(best_model, sens, features)\n",
    "print('Accuracy: {}'.format(sup.accuracy))\n",
    "print('Precision: {}'.format(sup.precision))\n",
    "print('Recall: {}'.format(sup.recall))\n",
    "print('F1: {}'.format(sup.f1))\n",
    "\n",
    "print('MAP: {}'.format(sup.compute_map(best_model, sens, features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computation time fitting the multilayer perceptron: 0:00:00.625341\n",
      "Accuracy: 0.8960040160642571\n",
      "Precision: 0.008998659774076202\n",
      "Recall: 0.94\n",
      "F1: 0.01782666413806182\n",
      "MAP: 0.776559977059977\n"
     ]
    }
   ],
   "source": [
    "# Evaluate best model on test collection (1st run)\n",
    "start = datetime.datetime.now()\n",
    "best_model.fit(train_data[features], train_data[label])\n",
    "stop = datetime.datetime.now()\n",
    "print('Computation time fitting the multilayer perceptron: {}'.format(stop-start))\n",
    "\n",
    "# Evaluation on test collection\n",
    "sup = sup_model.SupModel()\n",
    "sup.evaluate_boolean(best_model, sens)\n",
    "print('Accuracy: {}'.format(sup.accuracy))\n",
    "print('Precision: {}'.format(sup.precision))\n",
    "print('Recall: {}'.format(sup.recall))\n",
    "print('F1: {}'.format(sup.f1))\n",
    "\n",
    "print('MAP: {}'.format(sup.compute_map(best_model, sens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
