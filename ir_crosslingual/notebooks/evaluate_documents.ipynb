{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib, pickle, joblib\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'ir_crosslingual.features.element_based' from '/Users/i500969/Desktop/Admin/Uni-Mannheim/02_Courses/2020_FSS/Information-Retrieval/03_Project/03_Implementation/06_Documents/ir-crosslingual/ir_crosslingual/features/element_based.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ir_crosslingual.utils import paths\n",
    "importlib.reload(paths)\n",
    "\n",
    "from ir_crosslingual.sentences import sentences\n",
    "importlib.reload(sentences)\n",
    "\n",
    "from ir_crosslingual.supervised_classification import sup_model\n",
    "importlib.reload(sup_model)\n",
    "\n",
    "from ir_crosslingual.unsupervised_classification import unsup_model\n",
    "importlib.reload(unsup_model)\n",
    "\n",
    "from ir_crosslingual.features import element_based\n",
    "importlib.reload(element_based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time(start, stop, message):\n",
    "    print(f'---- TIME {datetime.now()}: Computation time {message}: {stop - start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model, mlp_prepared_features, mlp_features_dict = sup_model.SupModel.load_model(name='mlp_avg_best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_features = ['norm_diff_num_words', 'euclidean_distance', 'abs_diff_occ_exclamation_mark_0',\n",
    " 'abs_diff_occ_question_mark_2', 'abs_diff_occ_question_mark_0', 'cosine_similarity', 'norm_diff_translated_words',\n",
    " 'abs_diff_occ_exclamation_mark_1', 'abs_diff_occ_question_mark_1', 'abs_diff_num_words',\n",
    " 'abs_diff_occ_exclamation_mark_2', 'abs_diff_num_punctuation', 'src_embedding_pca_0', 'src_embedding_pca_1',\n",
    " 'src_embedding_pca_2', 'src_embedding_pca_3', 'src_embedding_pca_4', 'src_embedding_pca_5', 'src_embedding_pca_6',\n",
    " 'src_embedding_pca_7', 'src_embedding_pca_8', 'src_embedding_pca_9', 'trg_embedding_pca_0', 'trg_embedding_pca_1',\n",
    " 'trg_embedding_pca_2', 'trg_embedding_pca_3', 'trg_embedding_pca_4', 'trg_embedding_pca_5', 'trg_embedding_pca_6',\n",
    " 'trg_embedding_pca_7', 'trg_embedding_pca_8', 'trg_embedding_pca_9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define features\n",
    "model_features = ['src_sentence', 'trg_sentence', 'translation',\n",
    "                  'norm_diff_translated_words', 'abs_diff_num_words', 'abs_diff_num_punctuation',\n",
    "                  'abs_diff_occ_question_mark', 'abs_diff_occ_exclamation_mark',\n",
    "                  'rel_diff_num_words', 'rel_diff_num_punctuation', 'norm_diff_num_words',\n",
    "                  'norm_diff_num_punctuation', 'euclidean_distance', 'cosine_similarity'] \\\n",
    "                 + ['src_embedding_pca_{}'.format(i) for i in range(10)] \\\n",
    "                 + ['trg_embedding_pca_{}'.format(i) for i in range(10)]\n",
    "meta_features = ['src_sentence', 'trg_sentence']\n",
    "label = 'translation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = {}\n",
    "mean_scaler = {}\n",
    "scaler = joblib.load(open('../main/models/scaler/ct.pkl', 'rb'))\n",
    "for prefix in ['src', 'trg']:\n",
    "    mean_scaler['{}'.format(prefix)] = joblib.load(open('../main/models/mean_scaler/mean_scaler_{}.pkl'.format(prefix),\n",
    "                                                        'rb'))\n",
    "    pca['{}'.format(prefix)] = joblib.load(open('../main/models/pca/pca_{}.pkl'.format(prefix), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function that get feature names of transformed columns\n",
    "def get_transformer_feature_names(columnTransformer):\n",
    "\n",
    "    output_features = []\n",
    "\n",
    "    for name, pipe, features in columnTransformer.transformers_:\n",
    "        if name!='remainder':\n",
    "            for i in pipe:\n",
    "                trans_features = []\n",
    "                if hasattr(i,'categories_'):\n",
    "                    trans_features.extend(i.get_feature_names(features))\n",
    "                else:\n",
    "                    trans_features = features\n",
    "            output_features.extend(trans_features)\n",
    "\n",
    "    return output_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load documents data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- INFO: Learn projection matrix for en-de\n",
      "---- INFO: Found 13700 valid translation pairs in expert dictionary.\n",
      "---- INFO: 977 other pairs contained at least one unknown word (0 in source language, 977 in target language).\n",
      "---- DONE: Seed dictionary extracted for the languages: en-de\n",
      "---- INFO: Resulting subspace dimension: (13700, 300)\n",
      "---- INFO: Resulting subspace dimension: (13700, 300)\n",
      "---- DONE: Projection matrix learned from en to de\n",
      "---- INFO: Learn projection matrix for de-en\n",
      "---- INFO: Found 10604 valid translation pairs in expert dictionary.\n",
      "---- INFO: 262 other pairs contained at least one unknown word (0 in source language, 262 in target language).\n",
      "---- DONE: Seed dictionary extracted for the languages: de-en\n",
      "---- INFO: Resulting subspace dimension: (10604, 300)\n",
      "---- INFO: Resulting subspace dimension: (10604, 300)\n",
      "---- DONE: Projection matrix learned from de to en\n",
      "---- DONE: All chunks loaded\n"
     ]
    }
   ],
   "source": [
    "sens, chunks = sentences.Sentences.load_chunks_from_file(docs=True, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sens.test_collection = pd.concat(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unsupervised evaluation on documents data\n",
      "------------------------------------------------------------\n",
      "---- INFO: Start computing the MAP\n",
      "---- INFO: Probabilities predicted\n",
      "---- INFO: Dataframe with evaluation ranking created\n",
      "---- INFO: Probabilities sorted for each query\n",
      "---- INFO: Index of ranking of true translation retrieved\n",
      "---- DONE: MAP Score = 0.5401538175621837\n",
      "---- TIME 2020-05-24 05:11:25.857894: Computation time computing the MAP score: 0:00:00.333873\n"
     ]
    }
   ],
   "source": [
    "model = unsup_model.UnsupModel()\n",
    "sup = sup_model.SupModel()\n",
    "start = datetime.now()\n",
    "print('Unsupervised evaluation on documents data')\n",
    "print('-' * 60)\n",
    "print('---- DONE: MAP Score = {}'.format(sup.compute_map(model, sens, ['src_embedding_aligned', 'trg_embedding', 'cosine_similarity'])))\n",
    "time(start, datetime.now(), 'computing the MAP score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train best model from sentence level on reasonable documents features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_model = MLPClassifier(activation='tanh', alpha=0.1, batch_size=2000, beta_1=0.9,\n",
    "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
    "              hidden_layer_sizes=(9,), learning_rate='adaptive',\n",
    "              learning_rate_init=0.001, max_fun=15000, max_iter=1200,\n",
    "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
    "              power_t=0.5, random_state=42, shuffle=True, solver='lbfgs',\n",
    "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
    "              warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_features = ['euclidean_distance', 'cosine_similarity',\n",
    "                      'src_embedding_pca_0', 'src_embedding_pca_1', 'src_embedding_pca_2', \n",
    "                      'src_embedding_pca_3', 'src_embedding_pca_4', 'src_embedding_pca_5', \n",
    "                      'src_embedding_pca_6', 'src_embedding_pca_7', 'src_embedding_pca_8', \n",
    "                      'src_embedding_pca_9', 'trg_embedding_pca_0', 'trg_embedding_pca_1',\n",
    "                      'trg_embedding_pca_2', 'trg_embedding_pca_3', 'trg_embedding_pca_4', \n",
    "                      'trg_embedding_pca_5', 'trg_embedding_pca_6', 'trg_embedding_pca_7', \n",
    "                      'trg_embedding_pca_8', 'trg_embedding_pca_9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sens = element_based.vec2features(sens, pca, mean_scaler, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide test collection into chunks which allows for faster operations on test collection\n",
    "n = 500000  #chunk row size\n",
    "chunks_test_collection = [sens.test_collection[i:i+n] for i in range(0, sens.test_collection.shape[0], n)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## German/English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- INFO: Learn projection matrix for de-en\n",
      "---- INFO: Found 10604 valid translation pairs in expert dictionary.\n",
      "---- INFO: 262 other pairs contained at least one unknown word (0 in source language, 262 in target language).\n",
      "---- DONE: Seed dictionary extracted for the languages: de-en\n",
      "---- INFO: Resulting subspace dimension: (10604, 300)\n",
      "---- INFO: Resulting subspace dimension: (10604, 300)\n",
      "---- DONE: Projection matrix learned from de to en\n",
      "---- INFO: Learn projection matrix for en-de\n",
      "---- INFO: Found 13700 valid translation pairs in expert dictionary.\n",
      "---- INFO: 977 other pairs contained at least one unknown word (0 in source language, 977 in target language).\n",
      "---- DONE: Seed dictionary extracted for the languages: en-de\n",
      "---- INFO: Resulting subspace dimension: (13700, 300)\n",
      "---- INFO: Resulting subspace dimension: (13700, 300)\n",
      "---- DONE: Projection matrix learned from en to de\n",
      "---- DONE: All chunks loaded\n"
     ]
    }
   ],
   "source": [
    "deen_sens, deen_chunks = sentences.Sentences.load_chunks_from_file('de', 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "deen_sens.test_collection = pd.concat(deen_chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src_sentence</th>\n",
       "      <th>src_preprocessed</th>\n",
       "      <th>src_embedding</th>\n",
       "      <th>src_embedding_aligned</th>\n",
       "      <th>src_words</th>\n",
       "      <th>src_words_found_embedding</th>\n",
       "      <th>trg_sentence</th>\n",
       "      <th>trg_preprocessed</th>\n",
       "      <th>trg_embedding</th>\n",
       "      <th>trg_words</th>\n",
       "      <th>trg_words_found_embedding</th>\n",
       "      <th>translation</th>\n",
       "      <th>norm_diff_translated_words</th>\n",
       "      <th>abs_diff_num_words</th>\n",
       "      <th>abs_diff_num_punctuation</th>\n",
       "      <th>abs_diff_occ_question_mark</th>\n",
       "      <th>abs_diff_occ_exclamation_mark</th>\n",
       "      <th>rel_diff_num_words</th>\n",
       "      <th>rel_diff_num_punctuation</th>\n",
       "      <th>norm_diff_num_words</th>\n",
       "      <th>norm_diff_num_punctuation</th>\n",
       "      <th>euclidean_distance</th>\n",
       "      <th>cosine_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Man braucht nur ein Entwicklungs-Glossar zu ne...</td>\n",
       "      <td>[braucht, entwicklungs, -, glossar, nehmen, fr...</td>\n",
       "      <td>[-0.170302625, 0.08421487500000001, -0.0884655...</td>\n",
       "      <td>[0.07061240360856913, -0.042766405454618996, -...</td>\n",
       "      <td>[braucht, entwicklungs, glossar, nehmen, frage...</td>\n",
       "      <td>[[0.13183987810598768, 0.002547861925903911, -...</td>\n",
       "      <td>All we would have to do is get a directory of ...</td>\n",
       "      <td>[would, get, directory, development, put, item...</td>\n",
       "      <td>[-0.049815000000000005, -0.013216419999999998,...</td>\n",
       "      <td>[would, get, directory, development, put, item...</td>\n",
       "      <td>[[-0.1718, 0.20407, -0.12805, -0.1194, -0.0034...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.318805</td>\n",
       "      <td>0.785875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Man braucht nur ein Entwicklungs-Glossar zu ne...</td>\n",
       "      <td>[braucht, entwicklungs, -, glossar, nehmen, fr...</td>\n",
       "      <td>[-0.170302625, 0.08421487500000001, -0.0884655...</td>\n",
       "      <td>[0.07061240360856913, -0.042766405454618996, -...</td>\n",
       "      <td>[braucht, entwicklungs, glossar, nehmen, frage...</td>\n",
       "      <td>[[0.13183987810598768, 0.002547861925903911, -...</td>\n",
       "      <td>We are sufficiently responsible to realise tha...</td>\n",
       "      <td>[sufficiently, responsible, realise, include, ...</td>\n",
       "      <td>[-0.05318141428571429, -0.07261509047619048, -...</td>\n",
       "      <td>[sufficiently, responsible, realise, include, ...</td>\n",
       "      <td>[[-0.2702, 0.0608, -0.1625, 0.43151, -0.24751,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-10</td>\n",
       "      <td>-2</td>\n",
       "      <td>-0.833333</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>1.488705</td>\n",
       "      <td>0.723256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Man braucht nur ein Entwicklungs-Glossar zu ne...</td>\n",
       "      <td>[braucht, entwicklungs, -, glossar, nehmen, fr...</td>\n",
       "      <td>[-0.170302625, 0.08421487500000001, -0.0884655...</td>\n",
       "      <td>[0.07061240360856913, -0.042766405454618996, -...</td>\n",
       "      <td>[braucht, entwicklungs, glossar, nehmen, frage...</td>\n",
       "      <td>[[0.13183987810598768, 0.002547861925903911, -...</td>\n",
       "      <td>We must understand that there are limitations ...</td>\n",
       "      <td>[must, understand, limitations, achieve, ,, pa...</td>\n",
       "      <td>[-0.05662485, -0.04663293333333333, -0.1941054...</td>\n",
       "      <td>[must, understand, limitations, achieve, parti...</td>\n",
       "      <td>[[-0.013209, 0.13582, -0.17634, 0.14703, -0.14...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-3</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.470648</td>\n",
       "      <td>0.735352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Man braucht nur ein Entwicklungs-Glossar zu ne...</td>\n",
       "      <td>[braucht, entwicklungs, -, glossar, nehmen, fr...</td>\n",
       "      <td>[-0.170302625, 0.08421487500000001, -0.0884655...</td>\n",
       "      <td>[0.07061240360856913, -0.042766405454618996, -...</td>\n",
       "      <td>[braucht, entwicklungs, glossar, nehmen, frage...</td>\n",
       "      <td>[[0.13183987810598768, 0.002547861925903911, -...</td>\n",
       "      <td>You have mentioned Ethiopia, which falls withi...</td>\n",
       "      <td>[mentioned, ethiopia, ,, falls, within, scope,...</td>\n",
       "      <td>[-0.06602332142857144, -0.0758303214285714, -0...</td>\n",
       "      <td>[mentioned, ethiopia, falls, within, scope, na...</td>\n",
       "      <td>[[-0.0396, -0.12417, -0.016515, 0.35388, -0.02...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-13</td>\n",
       "      <td>-6</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.200000</td>\n",
       "      <td>1.461765</td>\n",
       "      <td>0.728323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Man braucht nur ein Entwicklungs-Glossar zu ne...</td>\n",
       "      <td>[braucht, entwicklungs, -, glossar, nehmen, fr...</td>\n",
       "      <td>[-0.170302625, 0.08421487500000001, -0.0884655...</td>\n",
       "      <td>[0.07061240360856913, -0.042766405454618996, -...</td>\n",
       "      <td>[braucht, entwicklungs, glossar, nehmen, frage...</td>\n",
       "      <td>[[0.13183987810598768, 0.002547861925903911, -...</td>\n",
       "      <td>Indeed, in terms of development aid policy, I ...</td>\n",
       "      <td>[indeed, ,, terms, development, aid, policy, ,...</td>\n",
       "      <td>[-0.11380196190476187, -0.10416507142857141, -...</td>\n",
       "      <td>[indeed, terms, development, aid, policy, thin...</td>\n",
       "      <td>[[-0.2022, 0.025995, -0.22349, 0.25912, -0.079...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-11</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.916667</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>1.448506</td>\n",
       "      <td>0.738626</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        src_sentence  \\\n",
       "0  Man braucht nur ein Entwicklungs-Glossar zu ne...   \n",
       "1  Man braucht nur ein Entwicklungs-Glossar zu ne...   \n",
       "2  Man braucht nur ein Entwicklungs-Glossar zu ne...   \n",
       "3  Man braucht nur ein Entwicklungs-Glossar zu ne...   \n",
       "4  Man braucht nur ein Entwicklungs-Glossar zu ne...   \n",
       "\n",
       "                                    src_preprocessed  \\\n",
       "0  [braucht, entwicklungs, -, glossar, nehmen, fr...   \n",
       "1  [braucht, entwicklungs, -, glossar, nehmen, fr...   \n",
       "2  [braucht, entwicklungs, -, glossar, nehmen, fr...   \n",
       "3  [braucht, entwicklungs, -, glossar, nehmen, fr...   \n",
       "4  [braucht, entwicklungs, -, glossar, nehmen, fr...   \n",
       "\n",
       "                                       src_embedding  \\\n",
       "0  [-0.170302625, 0.08421487500000001, -0.0884655...   \n",
       "1  [-0.170302625, 0.08421487500000001, -0.0884655...   \n",
       "2  [-0.170302625, 0.08421487500000001, -0.0884655...   \n",
       "3  [-0.170302625, 0.08421487500000001, -0.0884655...   \n",
       "4  [-0.170302625, 0.08421487500000001, -0.0884655...   \n",
       "\n",
       "                               src_embedding_aligned  \\\n",
       "0  [0.07061240360856913, -0.042766405454618996, -...   \n",
       "1  [0.07061240360856913, -0.042766405454618996, -...   \n",
       "2  [0.07061240360856913, -0.042766405454618996, -...   \n",
       "3  [0.07061240360856913, -0.042766405454618996, -...   \n",
       "4  [0.07061240360856913, -0.042766405454618996, -...   \n",
       "\n",
       "                                           src_words  \\\n",
       "0  [braucht, entwicklungs, glossar, nehmen, frage...   \n",
       "1  [braucht, entwicklungs, glossar, nehmen, frage...   \n",
       "2  [braucht, entwicklungs, glossar, nehmen, frage...   \n",
       "3  [braucht, entwicklungs, glossar, nehmen, frage...   \n",
       "4  [braucht, entwicklungs, glossar, nehmen, frage...   \n",
       "\n",
       "                           src_words_found_embedding  \\\n",
       "0  [[0.13183987810598768, 0.002547861925903911, -...   \n",
       "1  [[0.13183987810598768, 0.002547861925903911, -...   \n",
       "2  [[0.13183987810598768, 0.002547861925903911, -...   \n",
       "3  [[0.13183987810598768, 0.002547861925903911, -...   \n",
       "4  [[0.13183987810598768, 0.002547861925903911, -...   \n",
       "\n",
       "                                        trg_sentence  \\\n",
       "0  All we would have to do is get a directory of ...   \n",
       "1  We are sufficiently responsible to realise tha...   \n",
       "2  We must understand that there are limitations ...   \n",
       "3  You have mentioned Ethiopia, which falls withi...   \n",
       "4  Indeed, in terms of development aid policy, I ...   \n",
       "\n",
       "                                    trg_preprocessed  \\\n",
       "0  [would, get, directory, development, put, item...   \n",
       "1  [sufficiently, responsible, realise, include, ...   \n",
       "2  [must, understand, limitations, achieve, ,, pa...   \n",
       "3  [mentioned, ethiopia, ,, falls, within, scope,...   \n",
       "4  [indeed, ,, terms, development, aid, policy, ,...   \n",
       "\n",
       "                                       trg_embedding  \\\n",
       "0  [-0.049815000000000005, -0.013216419999999998,...   \n",
       "1  [-0.05318141428571429, -0.07261509047619048, -...   \n",
       "2  [-0.05662485, -0.04663293333333333, -0.1941054...   \n",
       "3  [-0.06602332142857144, -0.0758303214285714, -0...   \n",
       "4  [-0.11380196190476187, -0.10416507142857141, -...   \n",
       "\n",
       "                                           trg_words  \\\n",
       "0  [would, get, directory, development, put, item...   \n",
       "1  [sufficiently, responsible, realise, include, ...   \n",
       "2  [must, understand, limitations, achieve, parti...   \n",
       "3  [mentioned, ethiopia, falls, within, scope, na...   \n",
       "4  [indeed, terms, development, aid, policy, thin...   \n",
       "\n",
       "                           trg_words_found_embedding  translation  \\\n",
       "0  [[-0.1718, 0.20407, -0.12805, -0.1194, -0.0034...            1   \n",
       "1  [[-0.2702, 0.0608, -0.1625, 0.43151, -0.24751,...            0   \n",
       "2  [[-0.013209, 0.13582, -0.17634, 0.14703, -0.14...            0   \n",
       "3  [[-0.0396, -0.12417, -0.016515, 0.35388, -0.02...            0   \n",
       "4  [[-0.2022, 0.025995, -0.22349, 0.25912, -0.079...            0   \n",
       "\n",
       "   norm_diff_translated_words  abs_diff_num_words  abs_diff_num_punctuation  \\\n",
       "0                         0.0                   1                         0   \n",
       "1                         0.0                  10                         2   \n",
       "2                         0.0                   3                         0   \n",
       "3                         0.0                  13                         6   \n",
       "4                         0.0                  11                         1   \n",
       "\n",
       "   abs_diff_occ_question_mark  abs_diff_occ_exclamation_mark  \\\n",
       "0                           1                              1   \n",
       "1                           1                              1   \n",
       "2                           1                              1   \n",
       "3                           1                              1   \n",
       "4                           1                              1   \n",
       "\n",
       "   rel_diff_num_words  rel_diff_num_punctuation  norm_diff_num_words  \\\n",
       "0                  -1                         0            -0.142857   \n",
       "1                 -10                        -2            -0.833333   \n",
       "2                  -3                         0            -0.375000   \n",
       "3                 -13                        -6            -1.000000   \n",
       "4                 -11                        -1            -0.916667   \n",
       "\n",
       "   norm_diff_num_punctuation  euclidean_distance  cosine_similarity  \n",
       "0                   0.000000            1.318805           0.785875  \n",
       "1                  -0.666667            1.488705           0.723256  \n",
       "2                   0.000000            1.470648           0.735352  \n",
       "3                  -1.200000            1.461765           0.728323  \n",
       "4                  -0.500000            1.448506           0.738626  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deen_sens.test_collection.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- INFO: Unique queries extracted\n",
      "---- INFO: Unique documents extracted\n",
      "---- INFO: src_embedding_pca elements extracted for unique queries.\n",
      "---- INFO: Unique queries merged to test collection\n",
      "---- INFO: trg_embedding_pca elements extracted for unique documents.\n",
      "---- INFO: Unique documents merged to test collection\n",
      "---- DONE: Extracted all vector elements and merged to test collection\n"
     ]
    }
   ],
   "source": [
    "deen_sens = element_based.vec2features(deen_sens, pca, mean_scaler, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide test collection into chunks which allows for faster operations on test collection\n",
    "n = 500000  #chunk row size\n",
    "chunks_test_collection = [deen_sens.test_collection[i:i+n] for i in range(0, deen_sens.test_collection.shape[0], n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 scaled.\n",
      "Chunk 1 scaled.\n",
      "Chunk 2 scaled.\n",
      "Chunk 3 scaled.\n",
      "Chunk 4 scaled.\n",
      "Chunk 5 scaled.\n",
      "Chunk 6 scaled.\n",
      "Chunk 7 scaled.\n",
      "Chunk 8 scaled.\n",
      "Chunk 9 scaled.\n",
      "Chunk 10 scaled.\n",
      "Chunk 11 scaled.\n",
      "Chunk 12 scaled.\n",
      "Chunk 13 scaled.\n",
      "Chunk 14 scaled.\n",
      "Chunk 15 scaled.\n",
      "Chunk 16 scaled.\n",
      "Chunk 17 scaled.\n",
      "Chunk 18 scaled.\n",
      "Chunk 19 scaled.\n"
     ]
    }
   ],
   "source": [
    "# scale columns of test collection\n",
    "for i, chunk in enumerate(chunks_test_collection):\n",
    "    chunks_test_collection[i] = pd.DataFrame(scaler.transform(chunk[model_features]))\n",
    "    chunks_test_collection[i].columns = get_transformer_feature_names(scaler) + meta_features + [label]\n",
    "    chunks_test_collection[i] = chunks_test_collection[i].infer_objects()\n",
    "    print('Chunk {} scaled.'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update data attribute of Sentences object\n",
    "deen_sens.test_collection = pd.concat([chunk for chunk in chunks_test_collection], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on logistic regression model\n",
      "Accuracy: 0.5912253\n",
      "Precision: 0.00024604126694132306\n",
      "Recall: 0.9921104536489151\n",
      "F1: 0.0004919605287255313\n",
      "Computation time evaluating boolean: 0:00:24.749746\n",
      "Finished at: 2020-05-23 23:46:26.109598\n",
      "---- INFO: Start computing the MAP\n",
      "---- INFO: Probabilities predicted\n",
      "---- INFO: Dataframe with evaluation ranking created\n",
      "---- INFO: Probabilities sorted for each query\n",
      "---- INFO: Index of ranking of true translation retrieved\n",
      "MAP: 0.7857719010404002\n",
      "Computation time computing the MAP score: 0:09:12.013899\n",
      "Finished at: 2020-05-23 23:55:38.123721\n"
     ]
    }
   ],
   "source": [
    "# Evaluation of base logistic regression\n",
    "print('Evaluation on logistic regression model')\n",
    "start = datetime.datetime.now()\n",
    "sup = sup_model.SupModel()\n",
    "sup.evaluate_boolean(mlp_model, deen_sens, mlp_features)\n",
    "print('Accuracy: {}'.format(sup.accuracy))\n",
    "print('Precision: {}'.format(sup.precision))\n",
    "print('Recall: {}'.format(sup.recall))\n",
    "print('F1: {}'.format(sup.f1))\n",
    "stop = datetime.datetime.now()\n",
    "time(start, stop, 'evaluating boolean')\n",
    "start = datetime.datetime.now()\n",
    "print('MAP: {}'.format(sup.compute_map(mlp_model, deen_sens, mlp_features)))\n",
    "stop = datetime.datetime.now()\n",
    "time(start, stop, 'computing the MAP score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
