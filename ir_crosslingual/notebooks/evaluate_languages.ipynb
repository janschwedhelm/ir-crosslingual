{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, os, importlib, pickle\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'ir_crosslingual.features.element_based' from '/Users/i500969/Desktop/Admin/Uni-Mannheim/02_Courses/2020_FSS/Information-Retrieval/03_Project/03_Implementation/07_Languages/ir-crosslingual/ir_crosslingual/features/element_based.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ir_crosslingual.utils import paths\n",
    "importlib.reload(paths)\n",
    "\n",
    "from ir_crosslingual.sentences import sentences\n",
    "importlib.reload(sentences)\n",
    "\n",
    "from ir_crosslingual.supervised_classification import sup_model\n",
    "importlib.reload(sup_model)\n",
    "\n",
    "from ir_crosslingual.features import element_based\n",
    "importlib.reload(element_based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time(start, stop, message):\n",
    "    print('Computation time {}: {}'.format(message, stop-start))\n",
    "    print('Finished at: {}'.format(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model, mlp_prepared_features, mlp_features_dict = sup_model.SupModel.load_model(name='mlp_avg_best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_features = ['norm_diff_num_words', 'euclidean_distance', 'abs_diff_occ_exclamation_mark_0',\n",
    " 'abs_diff_occ_question_mark_2', 'abs_diff_occ_question_mark_0', 'cosine_similarity', 'norm_diff_translated_words',\n",
    " 'abs_diff_occ_exclamation_mark_1', 'abs_diff_occ_question_mark_1', 'abs_diff_num_words',\n",
    " 'abs_diff_occ_exclamation_mark_2', 'abs_diff_num_punctuation', 'src_embedding_pca_0', 'src_embedding_pca_1',\n",
    " 'src_embedding_pca_2', 'src_embedding_pca_3', 'src_embedding_pca_4', 'src_embedding_pca_5', 'src_embedding_pca_6',\n",
    " 'src_embedding_pca_7', 'src_embedding_pca_8', 'src_embedding_pca_9', 'trg_embedding_pca_0', 'trg_embedding_pca_1',\n",
    " 'trg_embedding_pca_2', 'trg_embedding_pca_3', 'trg_embedding_pca_4', 'trg_embedding_pca_5', 'trg_embedding_pca_6',\n",
    " 'trg_embedding_pca_7', 'trg_embedding_pca_8', 'trg_embedding_pca_9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define features\n",
    "model_features = ['src_sentence', 'trg_sentence', 'translation',\n",
    "                  'norm_diff_translated_words', 'abs_diff_num_words', 'abs_diff_num_punctuation',\n",
    "                  'abs_diff_occ_question_mark', 'abs_diff_occ_exclamation_mark',\n",
    "                  'rel_diff_num_words', 'rel_diff_num_punctuation', 'norm_diff_num_words',\n",
    "                  'norm_diff_num_punctuation', 'euclidean_distance', 'cosine_similarity'] \\\n",
    "                 + ['src_embedding_pca_{}'.format(i) for i in range(10)] \\\n",
    "                 + ['trg_embedding_pca_{}'.format(i) for i in range(10)]\n",
    "meta_features = ['src_sentence', 'trg_sentence']\n",
    "label = 'translation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = {}\n",
    "mean_scaler = {}\n",
    "scaler = joblib.load(open('../main/models/scaler/ct.pkl', 'rb'))\n",
    "for prefix in ['src', 'trg']:\n",
    "    mean_scaler['{}'.format(prefix)] = joblib.load(open('../main/models/mean_scaler/mean_scaler_{}.pkl'.format(prefix),\n",
    "                                                        'rb'))\n",
    "    pca['{}'.format(prefix)] = joblib.load(open('../main/models/pca/pca_{}.pkl'.format(prefix), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function that get feature names of transformed columns\n",
    "def get_transformer_feature_names(columnTransformer):\n",
    "\n",
    "    output_features = []\n",
    "\n",
    "    for name, pipe, features in columnTransformer.transformers_:\n",
    "        if name!='remainder':\n",
    "            for i in pipe:\n",
    "                trans_features = []\n",
    "                if hasattr(i,'categories_'):\n",
    "                    trans_features.extend(i.get_feature_names(features))\n",
    "                else:\n",
    "                    trans_features = features\n",
    "            output_features.extend(trans_features)\n",
    "\n",
    "    return output_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English/Finnish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-bbf9c7e9c934>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menfi_sens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menfi_chunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_chunks_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fi'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/PycharmProjects/ir-crosslingual/ir_crosslingual/sentences/sentences.py\u001b[0m in \u001b[0;36mload_chunks_from_file\u001b[0;34m(cls, src_language, trg_language, vector_creation, n_chunks)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordEmbeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrg_language\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mW_st\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW_ts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordEmbeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn_projection_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_lang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_language\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_lang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrg_language\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/ir-crosslingual/ir_crosslingual/embeddings/embeddings.py\u001b[0m in \u001b[0;36mload_embeddings\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# Load word2id dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}word2id.pkl'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonolingual_embedding_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;31m# Create id2word dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "enfi_sens, enfi_chunks = sentences.Sentences.load_chunks_from_file('en', 'fi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enfi_sens.test_collection = pd.concat(enfi_chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enfi_sens = element_based.vec2features(enfi_sens, pca, mean_scaler, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i, chunk in enumerate(enfi_chunks):\n",
    "#    print('Start PCA on chunk {}'.format(i))\n",
    "#    enfi_chunks[i] = chunk.drop_duplicates('src_sentence', ignore_index=True)\n",
    "#    enfi_chunks[i] = chunk.drop_duplicates('trg_sentence', ignore_index=True)\n",
    "#    for prefix in ['src', 'trg']:\n",
    "#        X = np.vstack(enfi_chunks[i]['{}_embedding'.format(prefix)])\n",
    "#        enfi_chunks[i][['{}_embedding_pca_{}'.format(prefix, i) for i in range(10)]] = \\\n",
    "#            pd.DataFrame(pca['{}'.format(prefix)].transform(mean_scaler['{}'.format(prefix)].transform(X)).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i, chunk in enumerate(enfi_chunks):\n",
    "#    print('Start scaling on chunk {}'.format(i))\n",
    "#    enfi_chunks[i] = pd.DataFrame(scaler.transform(enfi_chunks[i][model_features]))\n",
    "#    enfi_chunks[i].columns = get_transformer_feature_names(scaler) + meta_features + [label]\n",
    "#    enfi_chunks[i] = enfi_chunks[i].infer_objects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide test collection into chunks which allows for faster operations on test collection\n",
    "n = 500000  #chunk row size\n",
    "chunks_test_collection = [enfi_sens.test_collection[i:i+n] for i in range(0, enfi_sens.test_collection.shape[0], n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale columns of test collection\n",
    "for i, chunk in enumerate(chunks_test_collection):\n",
    "    chunks_test_collection[i] = pd.DataFrame(scaler.transform(chunk[model_features]))\n",
    "    chunks_test_collection[i].columns = get_transformer_feature_names(scaler) + meta_features + [label]\n",
    "    chunks_test_collection[i] = chunks_test_collection[i].infer_objects()\n",
    "    print('Chunk {} scaled.'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update data attribute of Sentences object\n",
    "enfi_sens.test_collection = pd.concat([chunk for chunk in chunks_test_collection], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of base logistic regression\n",
    "# print('Evaluation on logistic regression model')\n",
    "start = datetime.datetime.now()\n",
    "sup = sup_model.SupModel()\n",
    "sup.evaluate_boolean(mlp_model, enfi_sens, mlp_features)\n",
    "print('Accuracy: {}'.format(sup.accuracy))\n",
    "print('Precision: {}'.format(sup.precision))\n",
    "print('Recall: {}'.format(sup.recall))\n",
    "print('F1: {}'.format(sup.f1))\n",
    "stop = datetime.datetime.now()\n",
    "time(start, stop, 'evaluating boolean')\n",
    "start = datetime.datetime.now()\n",
    "print('MAP: {}'.format(sup.compute_map(mlp_model, enfi_sens, mlp_features)))\n",
    "stop = datetime.datetime.now()\n",
    "time(start, stop, 'computing the MAP score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finnish/English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fien_sens, fien_chunks = sentences.Sentences.load_chunks_from_file('fi', 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fien_sens.test_collection = pd.concat(fien_chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fien_sens = element_based.vec2features(fien_sens, pca, mean_scaler, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide test collection into chunks which allows for faster operations on test collection\n",
    "n = 500000  #chunk row size\n",
    "chunks_test_collection = [fien_sens.test_collection[i:i+n] for i in range(0, fien_sens.test_collection.shape[0], n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale columns of test collection\n",
    "for i, chunk in enumerate(chunks_test_collection):\n",
    "    chunks_test_collection[i] = pd.DataFrame(scaler.transform(chunk[model_features]))\n",
    "    chunks_test_collection[i].columns = get_transformer_feature_names(scaler) + meta_features + [label]\n",
    "    chunks_test_collection[i] = chunks_test_collection[i].infer_objects()\n",
    "    print('Chunk {} scaled.'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update data attribute of Sentences object\n",
    "fien_sens.test_collection = pd.concat([chunk for chunk in chunks_test_collection], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of base logistic regression\n",
    "# print('Evaluation on logistic regression model')\n",
    "start = datetime.datetime.now()\n",
    "sup = sup_model.SupModel()\n",
    "sup.evaluate_boolean(mlp_model, fien_sens, mlp_features)\n",
    "print('Accuracy: {}'.format(sup.accuracy))\n",
    "print('Precision: {}'.format(sup.precision))\n",
    "print('Recall: {}'.format(sup.recall))\n",
    "print('F1: {}'.format(sup.f1))\n",
    "stop = datetime.datetime.now()\n",
    "time(start, stop, 'evaluating boolean')\n",
    "start = datetime.datetime.now()\n",
    "print('MAP: {}'.format(sup.compute_map(mlp_model, fien_sens, mlp_features)))\n",
    "stop = datetime.datetime.now()\n",
    "time(start, stop, 'computing the MAP score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English/French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- INFO: Learn projection matrix for en-fr\n",
      "---- INFO: Found 10369 valid translation pairs in expert dictionary.\n",
      "---- INFO: 503 other pairs contained at least one unknown word (0 in source language, 503 in target language).\n",
      "---- DONE: Seed dictionary extracted for the languages: en-fr\n",
      "---- INFO: Resulting subspace dimension: (10369, 300)\n",
      "---- INFO: Resulting subspace dimension: (10369, 300)\n",
      "---- DONE: Projection matrix learned from en to fr\n",
      "---- INFO: Learn projection matrix for fr-en\n",
      "---- INFO: Found 7938 valid translation pairs in expert dictionary.\n",
      "---- INFO: 332 other pairs contained at least one unknown word (0 in source language, 332 in target language).\n",
      "---- DONE: Seed dictionary extracted for the languages: fr-en\n",
      "---- INFO: Resulting subspace dimension: (7938, 300)\n",
      "---- INFO: Resulting subspace dimension: (7938, 300)\n",
      "---- DONE: Projection matrix learned from fr to en\n",
      "---- DONE: All chunks loaded\n"
     ]
    }
   ],
   "source": [
    "enfr_sens, enfr_chunks = sentences.Sentences.load_chunks_from_file('en', 'fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "enfr_sens.test_collection = pd.concat(enfr_chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- INFO: Unique queries extracted\n",
      "---- INFO: Unique documents extracted\n",
      "---- INFO: src_embedding_pca elements extracted for unique queries.\n",
      "---- INFO: Unique queries merged to test collection\n",
      "---- INFO: trg_embedding_pca elements extracted for unique documents.\n",
      "---- INFO: Unique documents merged to test collection\n",
      "---- DONE: Extracted all vector elements and merged to test collection\n"
     ]
    }
   ],
   "source": [
    "enfr_sens = element_based.vec2features(enfr_sens, pca, mean_scaler, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide test collection into chunks which allows for faster operations on test collection\n",
    "n = 500000  #chunk row size\n",
    "chunks_test_collection = [enfr_sens.test_collection[i:i+n] for i in range(0, enfr_sens.test_collection.shape[0], n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 scaled.\n",
      "Chunk 1 scaled.\n",
      "Chunk 2 scaled.\n",
      "Chunk 3 scaled.\n",
      "Chunk 4 scaled.\n",
      "Chunk 5 scaled.\n",
      "Chunk 6 scaled.\n",
      "Chunk 7 scaled.\n",
      "Chunk 8 scaled.\n",
      "Chunk 9 scaled.\n",
      "Chunk 10 scaled.\n",
      "Chunk 11 scaled.\n",
      "Chunk 12 scaled.\n",
      "Chunk 13 scaled.\n",
      "Chunk 14 scaled.\n",
      "Chunk 15 scaled.\n",
      "Chunk 16 scaled.\n",
      "Chunk 17 scaled.\n",
      "Chunk 18 scaled.\n",
      "Chunk 19 scaled.\n"
     ]
    }
   ],
   "source": [
    "# scale columns of test collection\n",
    "for i, chunk in enumerate(chunks_test_collection):\n",
    "    chunks_test_collection[i] = pd.DataFrame(scaler.transform(chunk[model_features]))\n",
    "    chunks_test_collection[i].columns = get_transformer_feature_names(scaler) + meta_features + [label]\n",
    "    chunks_test_collection[i] = chunks_test_collection[i].infer_objects()\n",
    "    print('Chunk {} scaled.'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update data attribute of Sentences object\n",
    "enfr_sens.test_collection = pd.concat([chunk for chunk in chunks_test_collection], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on logistic regression model\n",
      "Accuracy: 0.7297366\n",
      "Precision: 0.0003709834019140968\n",
      "Recall: 0.9881773399014778\n",
      "F1: 0.0007416883577851396\n",
      "Computation time evaluating boolean: 0:01:53.432344\n",
      "Finished at: 2020-05-23 11:12:19.165262\n",
      "---- INFO: Start computing the MAP\n",
      "---- INFO: Probabilities predicted\n",
      "---- INFO: Dataframe with evaluation ranking created\n",
      "---- INFO: Probabilities sorted for each query\n",
      "---- INFO: Index of ranking of true translation retrieved\n",
      "MAP: 0.8029382924535479\n",
      "Computation time computing the MAP score: 0:08:56.083210\n",
      "Finished at: 2020-05-23 11:21:15.256568\n"
     ]
    }
   ],
   "source": [
    "# Evaluation of base logistic regression\n",
    "print('Evaluation on logistic regression model')\n",
    "start = datetime.datetime.now()\n",
    "sup = sup_model.SupModel()\n",
    "sup.evaluate_boolean(mlp_model, enfr_sens, mlp_features)\n",
    "print('Accuracy: {}'.format(sup.accuracy))\n",
    "print('Precision: {}'.format(sup.precision))\n",
    "print('Recall: {}'.format(sup.recall))\n",
    "print('F1: {}'.format(sup.f1))\n",
    "stop = datetime.datetime.now()\n",
    "time(start, stop, 'evaluating boolean')\n",
    "start = datetime.datetime.now()\n",
    "print('MAP: {}'.format(sup.compute_map(mlp_model, enfr_sens, mlp_features)))\n",
    "stop = datetime.datetime.now()\n",
    "time(start, stop, 'computing the MAP score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of base logistic regression\n",
    "# print('Evaluation on logistic regression model')\n",
    "#start = datetime.datetime.now()\n",
    "#sup = sup_model.SupModel()\n",
    "#sup.evaluate_boolean(lr_model, enfr_sens, lr_features)\n",
    "#print('Accuracy: {}'.format(sup.accuracy))\n",
    "#print('Precision: {}'.format(sup.precision))\n",
    "#print('Recall: {}'.format(sup.recall))\n",
    "#print('F1: {}'.format(sup.f1))\n",
    "#stop = datetime.datetime.now()\n",
    "#time(start, stop, 'evaluating boolean')\n",
    "#start = datetime.datetime.now()\n",
    "#print('MAP: {}'.format(sup.compute_map(lr_model, enfr_sens, lr_features)))\n",
    "#stop = datetime.datetime.now()\n",
    "#time(start, stop, 'computing the MAP score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('Evaluation on baseline MLP model')\n",
    "#print('-'*60)\n",
    "#start = datetime.datetime.now()\n",
    "#sup = sup_model.SupModel()\n",
    "#sup.evaluate_boolean(mlp_model, enfr_sens, mlp_features)\n",
    "#print('Accuracy: {}'.format(sup.accuracy))\n",
    "#print('Precision: {}'.format(sup.precision))\n",
    "#print('Recall: {}'.format(sup.recall))\n",
    "#print('F1: {}'.format(sup.f1))\n",
    "#stop = datetime.datetime.now()\n",
    "#time(start, stop, 'evaluating boolean')\n",
    "#start = datetime.datetime.now()\n",
    "#print('MAP: {}'.format(sup.compute_map(mlp_model, enfr_sens, mlp_features)))\n",
    "#stop = datetime.datetime.now()\n",
    "#time(start, stop, 'computing the MAP score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## German/English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- INFO: Learn projection matrix for de-en\n",
      "---- INFO: Found 10604 valid translation pairs in expert dictionary.\n",
      "---- INFO: 262 other pairs contained at least one unknown word (0 in source language, 262 in target language).\n",
      "---- DONE: Seed dictionary extracted for the languages: de-en\n",
      "---- INFO: Resulting subspace dimension: (10604, 300)\n",
      "---- INFO: Resulting subspace dimension: (10604, 300)\n",
      "---- DONE: Projection matrix learned from de to en\n",
      "---- INFO: Learn projection matrix for en-de\n",
      "---- INFO: Found 13700 valid translation pairs in expert dictionary.\n",
      "---- INFO: 977 other pairs contained at least one unknown word (0 in source language, 977 in target language).\n",
      "---- DONE: Seed dictionary extracted for the languages: en-de\n",
      "---- INFO: Resulting subspace dimension: (13700, 300)\n",
      "---- INFO: Resulting subspace dimension: (13700, 300)\n",
      "---- DONE: Projection matrix learned from en to de\n",
      "---- DONE: All chunks loaded\n"
     ]
    }
   ],
   "source": [
    "deen_sens, deen_chunks = sentences.Sentences.load_chunks_from_file('de', 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "deen_sens.test_collection = pd.concat(deen_chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src_sentence</th>\n",
       "      <th>src_preprocessed</th>\n",
       "      <th>src_embedding</th>\n",
       "      <th>src_embedding_aligned</th>\n",
       "      <th>src_words</th>\n",
       "      <th>src_words_found_embedding</th>\n",
       "      <th>trg_sentence</th>\n",
       "      <th>trg_preprocessed</th>\n",
       "      <th>trg_embedding</th>\n",
       "      <th>trg_words</th>\n",
       "      <th>trg_words_found_embedding</th>\n",
       "      <th>translation</th>\n",
       "      <th>norm_diff_translated_words</th>\n",
       "      <th>abs_diff_num_words</th>\n",
       "      <th>abs_diff_num_punctuation</th>\n",
       "      <th>abs_diff_occ_question_mark</th>\n",
       "      <th>abs_diff_occ_exclamation_mark</th>\n",
       "      <th>rel_diff_num_words</th>\n",
       "      <th>rel_diff_num_punctuation</th>\n",
       "      <th>norm_diff_num_words</th>\n",
       "      <th>norm_diff_num_punctuation</th>\n",
       "      <th>euclidean_distance</th>\n",
       "      <th>cosine_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Man braucht nur ein Entwicklungs-Glossar zu ne...</td>\n",
       "      <td>[braucht, entwicklungs, -, glossar, nehmen, fr...</td>\n",
       "      <td>[-0.170302625, 0.08421487500000001, -0.0884655...</td>\n",
       "      <td>[0.07061240360856913, -0.042766405454618996, -...</td>\n",
       "      <td>[braucht, entwicklungs, glossar, nehmen, frage...</td>\n",
       "      <td>[[0.13183987810598768, 0.002547861925903911, -...</td>\n",
       "      <td>All we would have to do is get a directory of ...</td>\n",
       "      <td>[would, get, directory, development, put, item...</td>\n",
       "      <td>[-0.049815000000000005, -0.013216419999999998,...</td>\n",
       "      <td>[would, get, directory, development, put, item...</td>\n",
       "      <td>[[-0.1718, 0.20407, -0.12805, -0.1194, -0.0034...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.318805</td>\n",
       "      <td>0.785875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Man braucht nur ein Entwicklungs-Glossar zu ne...</td>\n",
       "      <td>[braucht, entwicklungs, -, glossar, nehmen, fr...</td>\n",
       "      <td>[-0.170302625, 0.08421487500000001, -0.0884655...</td>\n",
       "      <td>[0.07061240360856913, -0.042766405454618996, -...</td>\n",
       "      <td>[braucht, entwicklungs, glossar, nehmen, frage...</td>\n",
       "      <td>[[0.13183987810598768, 0.002547861925903911, -...</td>\n",
       "      <td>We are sufficiently responsible to realise tha...</td>\n",
       "      <td>[sufficiently, responsible, realise, include, ...</td>\n",
       "      <td>[-0.05318141428571429, -0.07261509047619048, -...</td>\n",
       "      <td>[sufficiently, responsible, realise, include, ...</td>\n",
       "      <td>[[-0.2702, 0.0608, -0.1625, 0.43151, -0.24751,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-10</td>\n",
       "      <td>-2</td>\n",
       "      <td>-0.833333</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>1.488705</td>\n",
       "      <td>0.723256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Man braucht nur ein Entwicklungs-Glossar zu ne...</td>\n",
       "      <td>[braucht, entwicklungs, -, glossar, nehmen, fr...</td>\n",
       "      <td>[-0.170302625, 0.08421487500000001, -0.0884655...</td>\n",
       "      <td>[0.07061240360856913, -0.042766405454618996, -...</td>\n",
       "      <td>[braucht, entwicklungs, glossar, nehmen, frage...</td>\n",
       "      <td>[[0.13183987810598768, 0.002547861925903911, -...</td>\n",
       "      <td>We must understand that there are limitations ...</td>\n",
       "      <td>[must, understand, limitations, achieve, ,, pa...</td>\n",
       "      <td>[-0.05662485, -0.04663293333333333, -0.1941054...</td>\n",
       "      <td>[must, understand, limitations, achieve, parti...</td>\n",
       "      <td>[[-0.013209, 0.13582, -0.17634, 0.14703, -0.14...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-3</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.375000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.470648</td>\n",
       "      <td>0.735352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Man braucht nur ein Entwicklungs-Glossar zu ne...</td>\n",
       "      <td>[braucht, entwicklungs, -, glossar, nehmen, fr...</td>\n",
       "      <td>[-0.170302625, 0.08421487500000001, -0.0884655...</td>\n",
       "      <td>[0.07061240360856913, -0.042766405454618996, -...</td>\n",
       "      <td>[braucht, entwicklungs, glossar, nehmen, frage...</td>\n",
       "      <td>[[0.13183987810598768, 0.002547861925903911, -...</td>\n",
       "      <td>You have mentioned Ethiopia, which falls withi...</td>\n",
       "      <td>[mentioned, ethiopia, ,, falls, within, scope,...</td>\n",
       "      <td>[-0.06602332142857144, -0.0758303214285714, -0...</td>\n",
       "      <td>[mentioned, ethiopia, falls, within, scope, na...</td>\n",
       "      <td>[[-0.0396, -0.12417, -0.016515, 0.35388, -0.02...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-13</td>\n",
       "      <td>-6</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.200000</td>\n",
       "      <td>1.461765</td>\n",
       "      <td>0.728323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Man braucht nur ein Entwicklungs-Glossar zu ne...</td>\n",
       "      <td>[braucht, entwicklungs, -, glossar, nehmen, fr...</td>\n",
       "      <td>[-0.170302625, 0.08421487500000001, -0.0884655...</td>\n",
       "      <td>[0.07061240360856913, -0.042766405454618996, -...</td>\n",
       "      <td>[braucht, entwicklungs, glossar, nehmen, frage...</td>\n",
       "      <td>[[0.13183987810598768, 0.002547861925903911, -...</td>\n",
       "      <td>Indeed, in terms of development aid policy, I ...</td>\n",
       "      <td>[indeed, ,, terms, development, aid, policy, ,...</td>\n",
       "      <td>[-0.11380196190476187, -0.10416507142857141, -...</td>\n",
       "      <td>[indeed, terms, development, aid, policy, thin...</td>\n",
       "      <td>[[-0.2022, 0.025995, -0.22349, 0.25912, -0.079...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-11</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.916667</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>1.448506</td>\n",
       "      <td>0.738626</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        src_sentence  \\\n",
       "0  Man braucht nur ein Entwicklungs-Glossar zu ne...   \n",
       "1  Man braucht nur ein Entwicklungs-Glossar zu ne...   \n",
       "2  Man braucht nur ein Entwicklungs-Glossar zu ne...   \n",
       "3  Man braucht nur ein Entwicklungs-Glossar zu ne...   \n",
       "4  Man braucht nur ein Entwicklungs-Glossar zu ne...   \n",
       "\n",
       "                                    src_preprocessed  \\\n",
       "0  [braucht, entwicklungs, -, glossar, nehmen, fr...   \n",
       "1  [braucht, entwicklungs, -, glossar, nehmen, fr...   \n",
       "2  [braucht, entwicklungs, -, glossar, nehmen, fr...   \n",
       "3  [braucht, entwicklungs, -, glossar, nehmen, fr...   \n",
       "4  [braucht, entwicklungs, -, glossar, nehmen, fr...   \n",
       "\n",
       "                                       src_embedding  \\\n",
       "0  [-0.170302625, 0.08421487500000001, -0.0884655...   \n",
       "1  [-0.170302625, 0.08421487500000001, -0.0884655...   \n",
       "2  [-0.170302625, 0.08421487500000001, -0.0884655...   \n",
       "3  [-0.170302625, 0.08421487500000001, -0.0884655...   \n",
       "4  [-0.170302625, 0.08421487500000001, -0.0884655...   \n",
       "\n",
       "                               src_embedding_aligned  \\\n",
       "0  [0.07061240360856913, -0.042766405454618996, -...   \n",
       "1  [0.07061240360856913, -0.042766405454618996, -...   \n",
       "2  [0.07061240360856913, -0.042766405454618996, -...   \n",
       "3  [0.07061240360856913, -0.042766405454618996, -...   \n",
       "4  [0.07061240360856913, -0.042766405454618996, -...   \n",
       "\n",
       "                                           src_words  \\\n",
       "0  [braucht, entwicklungs, glossar, nehmen, frage...   \n",
       "1  [braucht, entwicklungs, glossar, nehmen, frage...   \n",
       "2  [braucht, entwicklungs, glossar, nehmen, frage...   \n",
       "3  [braucht, entwicklungs, glossar, nehmen, frage...   \n",
       "4  [braucht, entwicklungs, glossar, nehmen, frage...   \n",
       "\n",
       "                           src_words_found_embedding  \\\n",
       "0  [[0.13183987810598768, 0.002547861925903911, -...   \n",
       "1  [[0.13183987810598768, 0.002547861925903911, -...   \n",
       "2  [[0.13183987810598768, 0.002547861925903911, -...   \n",
       "3  [[0.13183987810598768, 0.002547861925903911, -...   \n",
       "4  [[0.13183987810598768, 0.002547861925903911, -...   \n",
       "\n",
       "                                        trg_sentence  \\\n",
       "0  All we would have to do is get a directory of ...   \n",
       "1  We are sufficiently responsible to realise tha...   \n",
       "2  We must understand that there are limitations ...   \n",
       "3  You have mentioned Ethiopia, which falls withi...   \n",
       "4  Indeed, in terms of development aid policy, I ...   \n",
       "\n",
       "                                    trg_preprocessed  \\\n",
       "0  [would, get, directory, development, put, item...   \n",
       "1  [sufficiently, responsible, realise, include, ...   \n",
       "2  [must, understand, limitations, achieve, ,, pa...   \n",
       "3  [mentioned, ethiopia, ,, falls, within, scope,...   \n",
       "4  [indeed, ,, terms, development, aid, policy, ,...   \n",
       "\n",
       "                                       trg_embedding  \\\n",
       "0  [-0.049815000000000005, -0.013216419999999998,...   \n",
       "1  [-0.05318141428571429, -0.07261509047619048, -...   \n",
       "2  [-0.05662485, -0.04663293333333333, -0.1941054...   \n",
       "3  [-0.06602332142857144, -0.0758303214285714, -0...   \n",
       "4  [-0.11380196190476187, -0.10416507142857141, -...   \n",
       "\n",
       "                                           trg_words  \\\n",
       "0  [would, get, directory, development, put, item...   \n",
       "1  [sufficiently, responsible, realise, include, ...   \n",
       "2  [must, understand, limitations, achieve, parti...   \n",
       "3  [mentioned, ethiopia, falls, within, scope, na...   \n",
       "4  [indeed, terms, development, aid, policy, thin...   \n",
       "\n",
       "                           trg_words_found_embedding  translation  \\\n",
       "0  [[-0.1718, 0.20407, -0.12805, -0.1194, -0.0034...            1   \n",
       "1  [[-0.2702, 0.0608, -0.1625, 0.43151, -0.24751,...            0   \n",
       "2  [[-0.013209, 0.13582, -0.17634, 0.14703, -0.14...            0   \n",
       "3  [[-0.0396, -0.12417, -0.016515, 0.35388, -0.02...            0   \n",
       "4  [[-0.2022, 0.025995, -0.22349, 0.25912, -0.079...            0   \n",
       "\n",
       "   norm_diff_translated_words  abs_diff_num_words  abs_diff_num_punctuation  \\\n",
       "0                         0.0                   1                         0   \n",
       "1                         0.0                  10                         2   \n",
       "2                         0.0                   3                         0   \n",
       "3                         0.0                  13                         6   \n",
       "4                         0.0                  11                         1   \n",
       "\n",
       "   abs_diff_occ_question_mark  abs_diff_occ_exclamation_mark  \\\n",
       "0                           1                              1   \n",
       "1                           1                              1   \n",
       "2                           1                              1   \n",
       "3                           1                              1   \n",
       "4                           1                              1   \n",
       "\n",
       "   rel_diff_num_words  rel_diff_num_punctuation  norm_diff_num_words  \\\n",
       "0                  -1                         0            -0.142857   \n",
       "1                 -10                        -2            -0.833333   \n",
       "2                  -3                         0            -0.375000   \n",
       "3                 -13                        -6            -1.000000   \n",
       "4                 -11                        -1            -0.916667   \n",
       "\n",
       "   norm_diff_num_punctuation  euclidean_distance  cosine_similarity  \n",
       "0                   0.000000            1.318805           0.785875  \n",
       "1                  -0.666667            1.488705           0.723256  \n",
       "2                   0.000000            1.470648           0.735352  \n",
       "3                  -1.200000            1.461765           0.728323  \n",
       "4                  -0.500000            1.448506           0.738626  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deen_sens.test_collection.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- INFO: Unique queries extracted\n",
      "---- INFO: Unique documents extracted\n",
      "---- INFO: src_embedding_pca elements extracted for unique queries.\n",
      "---- INFO: Unique queries merged to test collection\n",
      "---- INFO: trg_embedding_pca elements extracted for unique documents.\n",
      "---- INFO: Unique documents merged to test collection\n",
      "---- DONE: Extracted all vector elements and merged to test collection\n"
     ]
    }
   ],
   "source": [
    "deen_sens = element_based.vec2features(deen_sens, pca, mean_scaler, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide test collection into chunks which allows for faster operations on test collection\n",
    "n = 500000  #chunk row size\n",
    "chunks_test_collection = [deen_sens.test_collection[i:i+n] for i in range(0, deen_sens.test_collection.shape[0], n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 scaled.\n",
      "Chunk 1 scaled.\n",
      "Chunk 2 scaled.\n",
      "Chunk 3 scaled.\n",
      "Chunk 4 scaled.\n",
      "Chunk 5 scaled.\n",
      "Chunk 6 scaled.\n",
      "Chunk 7 scaled.\n",
      "Chunk 8 scaled.\n",
      "Chunk 9 scaled.\n",
      "Chunk 10 scaled.\n",
      "Chunk 11 scaled.\n",
      "Chunk 12 scaled.\n",
      "Chunk 13 scaled.\n",
      "Chunk 14 scaled.\n",
      "Chunk 15 scaled.\n",
      "Chunk 16 scaled.\n",
      "Chunk 17 scaled.\n",
      "Chunk 18 scaled.\n",
      "Chunk 19 scaled.\n"
     ]
    }
   ],
   "source": [
    "# scale columns of test collection\n",
    "for i, chunk in enumerate(chunks_test_collection):\n",
    "    chunks_test_collection[i] = pd.DataFrame(scaler.transform(chunk[model_features]))\n",
    "    chunks_test_collection[i].columns = get_transformer_feature_names(scaler) + meta_features + [label]\n",
    "    chunks_test_collection[i] = chunks_test_collection[i].infer_objects()\n",
    "    print('Chunk {} scaled.'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update data attribute of Sentences object\n",
    "deen_sens.test_collection = pd.concat([chunk for chunk in chunks_test_collection], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on logistic regression model\n",
      "Accuracy: 0.5912253\n",
      "Precision: 0.00024604126694132306\n",
      "Recall: 0.9921104536489151\n",
      "F1: 0.0004919605287255313\n",
      "Computation time evaluating boolean: 0:00:24.749746\n",
      "Finished at: 2020-05-23 23:46:26.109598\n",
      "---- INFO: Start computing the MAP\n",
      "---- INFO: Probabilities predicted\n",
      "---- INFO: Dataframe with evaluation ranking created\n",
      "---- INFO: Probabilities sorted for each query\n",
      "---- INFO: Index of ranking of true translation retrieved\n",
      "MAP: 0.7857719010404002\n",
      "Computation time computing the MAP score: 0:09:12.013899\n",
      "Finished at: 2020-05-23 23:55:38.123721\n"
     ]
    }
   ],
   "source": [
    "# Evaluation of base logistic regression\n",
    "print('Evaluation on logistic regression model')\n",
    "start = datetime.datetime.now()\n",
    "sup = sup_model.SupModel()\n",
    "sup.evaluate_boolean(mlp_model, deen_sens, mlp_features)\n",
    "print('Accuracy: {}'.format(sup.accuracy))\n",
    "print('Precision: {}'.format(sup.precision))\n",
    "print('Recall: {}'.format(sup.recall))\n",
    "print('F1: {}'.format(sup.f1))\n",
    "stop = datetime.datetime.now()\n",
    "time(start, stop, 'evaluating boolean')\n",
    "start = datetime.datetime.now()\n",
    "print('MAP: {}'.format(sup.compute_map(mlp_model, deen_sens, mlp_features)))\n",
    "stop = datetime.datetime.now()\n",
    "time(start, stop, 'computing the MAP score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
