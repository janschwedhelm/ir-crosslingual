{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, os, importlib, pickle\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'ir_crosslingual.features.element_based' from '/Users/jani/PycharmProjects/ir-crosslingual/ir_crosslingual/features/element_based.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ir_crosslingual.utils import paths\n",
    "importlib.reload(paths)\n",
    "\n",
    "from ir_crosslingual.sentences import sentences\n",
    "importlib.reload(sentences)\n",
    "\n",
    "from ir_crosslingual.supervised_classification import sup_model\n",
    "importlib.reload(sup_model)\n",
    "\n",
    "from ir_crosslingual.features import element_based\n",
    "importlib.reload(element_based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time(start, stop, message):\n",
    "    print('Computation time {}: {}'.format(message, stop-start))\n",
    "    print('Finished at: {}'.format(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model, mlp_prepared_features, mlp_features_dict = sup_model.SupModel.load_model(name='mlp_avg_best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_features = ['norm_diff_num_words', 'euclidean_distance', 'abs_diff_occ_exclamation_mark_0',\n",
    " 'abs_diff_occ_question_mark_2', 'abs_diff_occ_question_mark_0', 'cosine_similarity', 'norm_diff_translated_words',\n",
    " 'abs_diff_occ_exclamation_mark_1', 'abs_diff_occ_question_mark_1', 'abs_diff_num_words',\n",
    " 'abs_diff_occ_exclamation_mark_2', 'abs_diff_num_punctuation', 'src_embedding_pca_0', 'src_embedding_pca_1',\n",
    " 'src_embedding_pca_2', 'src_embedding_pca_3', 'src_embedding_pca_4', 'src_embedding_pca_5', 'src_embedding_pca_6',\n",
    " 'src_embedding_pca_7', 'src_embedding_pca_8', 'src_embedding_pca_9', 'trg_embedding_pca_0', 'trg_embedding_pca_1',\n",
    " 'trg_embedding_pca_2', 'trg_embedding_pca_3', 'trg_embedding_pca_4', 'trg_embedding_pca_5', 'trg_embedding_pca_6',\n",
    " 'trg_embedding_pca_7', 'trg_embedding_pca_8', 'trg_embedding_pca_9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define features\n",
    "model_features = ['src_sentence', 'trg_sentence', 'translation',\n",
    "                  'norm_diff_translated_words', 'abs_diff_num_words', 'abs_diff_num_punctuation',\n",
    "                  'abs_diff_occ_question_mark', 'abs_diff_occ_exclamation_mark',\n",
    "                  'rel_diff_num_words', 'rel_diff_num_punctuation', 'norm_diff_num_words',\n",
    "                  'norm_diff_num_punctuation', 'euclidean_distance', 'cosine_similarity'] \\\n",
    "                 + ['src_embedding_pca_{}'.format(i) for i in range(10)] \\\n",
    "                 + ['trg_embedding_pca_{}'.format(i) for i in range(10)]\n",
    "meta_features = ['src_sentence', 'trg_sentence']\n",
    "label = 'translation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = {}\n",
    "mean_scaler = {}\n",
    "scaler = joblib.load(open('../main/models/scaler/ct.pkl', 'rb'))\n",
    "for prefix in ['src', 'trg']:\n",
    "    mean_scaler['{}'.format(prefix)] = joblib.load(open('../main/models/mean_scaler/mean_scaler_{}.pkl'.format(prefix),\n",
    "                                                        'rb'))\n",
    "    pca['{}'.format(prefix)] = joblib.load(open('../main/models/pca/pca_{}.pkl'.format(prefix), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function that get feature names of transformed columns\n",
    "def get_transformer_feature_names(columnTransformer):\n",
    "\n",
    "    output_features = []\n",
    "\n",
    "    for name, pipe, features in columnTransformer.transformers_:\n",
    "        if name!='remainder':\n",
    "            for i in pipe:\n",
    "                trans_features = []\n",
    "                if hasattr(i,'categories_'):\n",
    "                    trans_features.extend(i.get_feature_names(features))\n",
    "                else:\n",
    "                    trans_features = features\n",
    "            output_features.extend(trans_features)\n",
    "\n",
    "    return output_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English/Finnish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load test collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- INFO: Learn projection matrix for en-fi\n",
      "---- INFO: Found 10141 valid translation pairs in expert dictionary.\n",
      "---- INFO: 1355 other pairs contained at least one unknown word (0 in source language, 1355 in target language).\n",
      "---- DONE: Seed dictionary extracted for the languages: en-fi\n",
      "---- INFO: Resulting subspace dimension: (10141, 300)\n",
      "---- INFO: Resulting subspace dimension: (10141, 300)\n",
      "---- DONE: Projection matrix learned from en to fi\n",
      "---- INFO: Learn projection matrix for fi-en\n",
      "---- INFO: Found 6946 valid translation pairs in expert dictionary.\n",
      "---- INFO: 185 other pairs contained at least one unknown word (0 in source language, 185 in target language).\n",
      "---- DONE: Seed dictionary extracted for the languages: fi-en\n",
      "---- INFO: Resulting subspace dimension: (6946, 300)\n",
      "---- INFO: Resulting subspace dimension: (6946, 300)\n",
      "---- DONE: Projection matrix learned from fi to en\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-bbf9c7e9c934>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menfi_sens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menfi_chunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_chunks_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fi'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/PycharmProjects/ir-crosslingual/ir_crosslingual/sentences/sentences.py\u001b[0m in \u001b[0;36mload_chunks_from_file\u001b[0;34m(cls, src_language, trg_language, vector_creation, n_chunks)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextracted_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'{src_language}-{trg_language}'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         sens.test_chunks = [pd.read_pickle(f'{path}test_collection_{idx:02d}_{vector_creation}.pkl')\n\u001b[0;32m--> 129\u001b[0;31m                             for idx in range(n_chunks)]\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'---- DONE: All chunks loaded'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/ir-crosslingual/ir_crosslingual/sentences/sentences.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextracted_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'{src_language}-{trg_language}'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         sens.test_chunks = [pd.read_pickle(f'{path}test_collection_{idx:02d}_{vector_creation}.pkl')\n\u001b[0;32m--> 129\u001b[0;31m                             for idx in range(n_chunks)]\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'---- DONE: All chunks loaded'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression)\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0;31m# We want to silence any warnings about, e.g. moved modules.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexcs_to_catch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;31m# e.g.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_new_Index\u001b[0;34m(cls, d)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_new_Index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m     \"\"\"\n\u001b[1;32m    159\u001b[0m     \u001b[0mThis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0mupon\u001b[0m \u001b[0munpickling\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrather\u001b[0m \u001b[0mthan\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mwhich\u001b[0m \u001b[0mdoesn\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "enfi_sens, enfi_chunks = sentences.Sentences.load_chunks_from_file('en', 'fi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enfi_sens.test_collection = pd.concat(enfi_chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enfi_sens = element_based.vec2features(enfi_sens, pca, mean_scaler, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply scaling of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide test collection into chunks which allows for faster operations on test collection\n",
    "n = 500000  #chunk row size\n",
    "chunks_test_collection = [enfi_sens.test_collection[i:i+n] for i in range(0, enfi_sens.test_collection.shape[0], n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale columns of test collection\n",
    "for i, chunk in enumerate(chunks_test_collection):\n",
    "    chunks_test_collection[i] = pd.DataFrame(scaler.transform(chunk[model_features]))\n",
    "    chunks_test_collection[i].columns = get_transformer_feature_names(scaler) + meta_features + [label]\n",
    "    chunks_test_collection[i] = chunks_test_collection[i].infer_objects()\n",
    "    print('Chunk {} scaled.'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update data attribute of Sentences object\n",
    "enfi_sens.test_collection = pd.concat([chunk for chunk in chunks_test_collection], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate test collection on best MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of base logistic regression\n",
    "# print('Evaluation on logistic regression model')\n",
    "start = datetime.datetime.now()\n",
    "sup = sup_model.SupModel()\n",
    "sup.evaluate_boolean(mlp_model, enfi_sens, mlp_features)\n",
    "print('Accuracy: {}'.format(sup.accuracy))\n",
    "print('Precision: {}'.format(sup.precision))\n",
    "print('Recall: {}'.format(sup.recall))\n",
    "print('F1: {}'.format(sup.f1))\n",
    "stop = datetime.datetime.now()\n",
    "time(start, stop, 'evaluating boolean')\n",
    "start = datetime.datetime.now()\n",
    "print('MAP: {}'.format(sup.compute_map(mlp_model, enfi_sens, mlp_features)))\n",
    "stop = datetime.datetime.now()\n",
    "time(start, stop, 'computing the MAP score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finnish/English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fien_sens, fien_chunks = sentences.Sentences.load_chunks_from_file('fi', 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fien_sens.test_collection = pd.concat(fien_chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fien_sens = element_based.vec2features(fien_sens, pca, mean_scaler, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide test collection into chunks which allows for faster operations on test collection\n",
    "n = 500000  #chunk row size\n",
    "chunks_test_collection = [fien_sens.test_collection[i:i+n] for i in range(0, fien_sens.test_collection.shape[0], n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale columns of test collection\n",
    "for i, chunk in enumerate(chunks_test_collection):\n",
    "    chunks_test_collection[i] = pd.DataFrame(scaler.transform(chunk[model_features]))\n",
    "    chunks_test_collection[i].columns = get_transformer_feature_names(scaler) + meta_features + [label]\n",
    "    chunks_test_collection[i] = chunks_test_collection[i].infer_objects()\n",
    "    print('Chunk {} scaled.'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update data attribute of Sentences object\n",
    "fien_sens.test_collection = pd.concat([chunk for chunk in chunks_test_collection], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of base logistic regression\n",
    "# print('Evaluation on logistic regression model')\n",
    "start = datetime.datetime.now()\n",
    "sup = sup_model.SupModel()\n",
    "sup.evaluate_boolean(mlp_model, fien_sens, mlp_features)\n",
    "print('Accuracy: {}'.format(sup.accuracy))\n",
    "print('Precision: {}'.format(sup.precision))\n",
    "print('Recall: {}'.format(sup.recall))\n",
    "print('F1: {}'.format(sup.f1))\n",
    "stop = datetime.datetime.now()\n",
    "time(start, stop, 'evaluating boolean')\n",
    "start = datetime.datetime.now()\n",
    "print('MAP: {}'.format(sup.compute_map(mlp_model, fien_sens, mlp_features)))\n",
    "stop = datetime.datetime.now()\n",
    "time(start, stop, 'computing the MAP score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English/French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enfr_sens, enfr_chunks = sentences.Sentences.load_chunks_from_file('en', 'fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enfr_sens.test_collection = pd.concat(enfr_chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enfr_sens = element_based.vec2features(enfr_sens, pca, mean_scaler, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide test collection into chunks which allows for faster operations on test collection\n",
    "n = 500000  #chunk row size\n",
    "chunks_test_collection = [enfr_sens.test_collection[i:i+n] for i in range(0, enfr_sens.test_collection.shape[0], n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale columns of test collection\n",
    "for i, chunk in enumerate(chunks_test_collection):\n",
    "    chunks_test_collection[i] = pd.DataFrame(scaler.transform(chunk[model_features]))\n",
    "    chunks_test_collection[i].columns = get_transformer_feature_names(scaler) + meta_features + [label]\n",
    "    chunks_test_collection[i] = chunks_test_collection[i].infer_objects()\n",
    "    print('Chunk {} scaled.'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update data attribute of Sentences object\n",
    "enfr_sens.test_collection = pd.concat([chunk for chunk in chunks_test_collection], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluation of base logistic regression\n",
    "print('Evaluation on logistic regression model')\n",
    "start = datetime.datetime.now()\n",
    "sup = sup_model.SupModel()\n",
    "sup.evaluate_boolean(mlp_model, enfr_sens, mlp_features)\n",
    "print('Accuracy: {}'.format(sup.accuracy))\n",
    "print('Precision: {}'.format(sup.precision))\n",
    "print('Recall: {}'.format(sup.recall))\n",
    "print('F1: {}'.format(sup.f1))\n",
    "stop = datetime.datetime.now()\n",
    "time(start, stop, 'evaluating boolean')\n",
    "start = datetime.datetime.now()\n",
    "print('MAP: {}'.format(sup.compute_map(mlp_model, enfr_sens, mlp_features)))\n",
    "stop = datetime.datetime.now()\n",
    "time(start, stop, 'computing the MAP score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## French/English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- INFO: Learn projection matrix for fr-en\n",
      "---- INFO: Found 7938 valid translation pairs in expert dictionary.\n",
      "---- INFO: 332 other pairs contained at least one unknown word (0 in source language, 332 in target language).\n",
      "---- DONE: Seed dictionary extracted for the languages: fr-en\n",
      "---- INFO: Resulting subspace dimension: (7938, 300)\n",
      "---- INFO: Resulting subspace dimension: (7938, 300)\n",
      "---- DONE: Projection matrix learned from fr to en\n",
      "---- INFO: Learn projection matrix for en-fr\n",
      "---- INFO: Found 10369 valid translation pairs in expert dictionary.\n",
      "---- INFO: 503 other pairs contained at least one unknown word (0 in source language, 503 in target language).\n",
      "---- DONE: Seed dictionary extracted for the languages: en-fr\n",
      "---- INFO: Resulting subspace dimension: (10369, 300)\n",
      "---- INFO: Resulting subspace dimension: (10369, 300)\n",
      "---- DONE: Projection matrix learned from en to fr\n",
      "---- DONE: All chunks loaded\n"
     ]
    }
   ],
   "source": [
    "fren_sens, fren_chunks = sentences.Sentences.load_chunks_from_file('fr', 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fren_sens.test_collection = pd.concat(fren_chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- INFO: Unique queries extracted\n",
      "---- INFO: Unique documents extracted\n",
      "---- INFO: src_embedding_pca elements extracted for unique queries.\n",
      "---- INFO: Unique queries merged to test collection\n",
      "---- INFO: trg_embedding_pca elements extracted for unique documents.\n",
      "---- INFO: Unique documents merged to test collection\n",
      "---- DONE: Extracted all vector elements and merged to test collection\n"
     ]
    }
   ],
   "source": [
    "fren_sens = element_based.vec2features(fren_sens, pca, mean_scaler, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide test collection into chunks which allows for faster operations on test collection\n",
    "n = 500000  #chunk row size\n",
    "chunks_test_collection = [fren_sens.test_collection[i:i+n] for i in range(0, fren_sens.test_collection.shape[0], n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 scaled.\n",
      "Chunk 1 scaled.\n",
      "Chunk 2 scaled.\n",
      "Chunk 3 scaled.\n",
      "Chunk 4 scaled.\n",
      "Chunk 5 scaled.\n",
      "Chunk 6 scaled.\n",
      "Chunk 7 scaled.\n",
      "Chunk 8 scaled.\n",
      "Chunk 9 scaled.\n",
      "Chunk 10 scaled.\n",
      "Chunk 11 scaled.\n",
      "Chunk 12 scaled.\n",
      "Chunk 13 scaled.\n",
      "Chunk 14 scaled.\n",
      "Chunk 15 scaled.\n",
      "Chunk 16 scaled.\n",
      "Chunk 17 scaled.\n",
      "Chunk 18 scaled.\n",
      "Chunk 19 scaled.\n"
     ]
    }
   ],
   "source": [
    "# scale columns of test collection\n",
    "for i, chunk in enumerate(chunks_test_collection):\n",
    "    chunks_test_collection[i] = pd.DataFrame(scaler.transform(chunk[model_features]))\n",
    "    chunks_test_collection[i].columns = get_transformer_feature_names(scaler) + meta_features + [label]\n",
    "    chunks_test_collection[i] = chunks_test_collection[i].infer_objects()\n",
    "    print('Chunk {} scaled.'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update data attribute of Sentences object\n",
    "fren_sens.test_collection = pd.concat([chunk for chunk in chunks_test_collection], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on logistic regression model\n",
      "Accuracy: 0.6560946\n",
      "Precision: 0.00029330909342490546\n",
      "Recall: 0.994088669950739\n",
      "F1: 0.0005864451543007527\n",
      "Computation time evaluating boolean: 0:00:41.533548\n",
      "Finished at: 2020-05-24 12:05:26.380415\n",
      "---- INFO: Start computing the MAP\n",
      "---- INFO: Probabilities predicted\n",
      "---- INFO: Dataframe with evaluation ranking created\n",
      "---- INFO: Probabilities sorted for each query\n",
      "---- INFO: Index of ranking of true translation retrieved\n",
      "MAP: 0.8590039263547499\n",
      "Computation time computing the MAP score: 0:07:57.005346\n",
      "Finished at: 2020-05-24 12:13:23.392081\n"
     ]
    }
   ],
   "source": [
    "# Evaluation of base logistic regression\n",
    "print('Evaluation on logistic regression model')\n",
    "start = datetime.datetime.now()\n",
    "sup = sup_model.SupModel()\n",
    "sup.evaluate_boolean(mlp_model, fren_sens, mlp_features)\n",
    "print('Accuracy: {}'.format(sup.accuracy))\n",
    "print('Precision: {}'.format(sup.precision))\n",
    "print('Recall: {}'.format(sup.recall))\n",
    "print('F1: {}'.format(sup.f1))\n",
    "stop = datetime.datetime.now()\n",
    "time(start, stop, 'evaluating boolean')\n",
    "start = datetime.datetime.now()\n",
    "print('MAP: {}'.format(sup.compute_map(mlp_model, fren_sens, mlp_features)))\n",
    "stop = datetime.datetime.now()\n",
    "time(start, stop, 'computing the MAP score')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
