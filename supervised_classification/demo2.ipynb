{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_vector_creation import load_sentences, preprocess_sentences, tf_idf, transform_into_sentence_vectors, AGGREGATION_METHODS, LANGUAGES\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "from numpy.linalg import svd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import os,sys,inspect\n",
    "current_dir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.insert(0, parent_dir) \n",
    "from induce_multilingual_embedding_space.mono_embedding_loading import load_monolingual_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised classifier for cross-lingual retrieval (L2R) - DEMO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Europarl sentences and transform to sentence vector embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_en, id2word_en, word2id_en = load_monolingual_embedding('../induce_multilingual_embedding_space/fastText_mon_emb/wiki.en.vec', 50000)\n",
    "emb_de, id2word_de, word2id_de = load_monolingual_embedding('../induce_multilingual_embedding_space/fastText_mon_emb/wiki.de.vec', 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_en = load_sentences('europarl_datasets/de-en/europarl-v7.de-en.en', 1)\n",
    "sen_de = load_sentences('europarl_datasets/de-en/europarl-v7.de-en.de', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 300)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen_emb_en, id2sentence_en = transform_into_sentence_vectors(sen_en, 'english', emb_en, word2id_en, agg_method='tf_idf')\n",
    "sen_emb_de, id2sentence_de = transform_into_sentence_vectors(sen_de, 'german', emb_de, word2id_de, agg_method='tf_idf')\n",
    "sen_emb_de.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute cosine similarity of two sentence translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_METHODS = {'procrustes'}  # todo: implement further methods\n",
    "\n",
    "\n",
    "def learn_projection_matrix(s_vecs, t_vecs, train_expert_dict, s_word2id: dict = None, t_word2id: dict = None,\n",
    "                            method: str = 'procrustes', n_max: int = 50000):\n",
    "    \"\"\"\n",
    "    Learns projection matrix W that maps source language monolingual embedding into multilingual word embedding space.\n",
    "    :param s_vecs: path of fastText source monolingual embedding text file or array of word embedding\n",
    "    :param t_vecs: path of fastText target monolingual embedding text file or array of word embedding\n",
    "                   (same type as s_vecs required)\n",
    "    :param train_expert_dict: path of external expert training translation dictionary\n",
    "    :param s_word2id: word/id dictionary, needed if only word vector embeddings are specified in s_vecs\n",
    "    :param t_word2id: word/id dictionary, needed if only word vector embeddings are specified in t_vecs\n",
    "    :param method: method to solve the learning problem\n",
    "    :param n_max: maximum number of most frequent words that are loaded in monolingual word embeddings\n",
    "    :return: projection matrix W\n",
    "    \"\"\"\n",
    "    if method not in LEARNING_METHODS:\n",
    "        raise ValueError(\"Method must be one of {}.\".format(LEARNING_METHODS))\n",
    "    if isinstance(s_vecs, np.ndarray) and s_word2id is None:\n",
    "        raise TypeError(\"word2id dictionaries have to be specified if embeddings are given as numpy arrays.\")\n",
    "\n",
    "    if isinstance(s_vecs, str) and os.path.isfile(s_vecs):\n",
    "        l1_emb, l1_id2word, l1_word2id = load_monolingual_embedding(s_vecs, n_max)\n",
    "        l2_emb, l2_id2word, l2_word2id = load_monolingual_embedding(t_vecs, n_max)\n",
    "        d_index, d_word = extract_seed_dictionary(train_expert_dict, l1_word2id, l2_word2id)\n",
    "        s_emb, t_emb = align_monolingual_subspaces(l1_emb, l2_emb, d_index)\n",
    "    else:\n",
    "        d_index, d_word = extract_seed_dictionary(train_expert_dict, s_word2id, t_word2id)\n",
    "        s_emb, t_emb = align_monolingual_subspaces(s_vecs, t_vecs, d_index)\n",
    "\n",
    "    if method == 'procrustes':\n",
    "        U, s, Vt = svd(np.transpose(s_emb) @ t_emb)\n",
    "        W = U @ Vt\n",
    "        return W\n",
    "\n",
    "def extract_seed_dictionary(expert_dict, s_word2id: dict, t_word2id: dict):\n",
    "    \"\"\"\n",
    "    Extract seed dictionary from external expert dictionary according to vocabularies included in monolingual embedding\n",
    "    spaces.\n",
    "    :param expert_dict: external expert dictionary (either text file or Python dictionary)\n",
    "    :param s_word2id: source dictionary of words and indices as returned from load_monolingual_embedding\n",
    "    :param t_word2id: target dictionary of indices and words as returned from load_monolingual_embedding\n",
    "    :return: index/word pairs of resulting seed dictionary\n",
    "    \"\"\"\n",
    "    index_pairs = []\n",
    "    word_pairs = []\n",
    "    misfit = 0\n",
    "    misfit_s = 0\n",
    "    misfit_t = 0\n",
    "\n",
    "    if isinstance(expert_dict, str) and os.path.isfile(expert_dict):\n",
    "        with io.open(expert_dict, 'r', encoding='utf-8') as file:\n",
    "            for index, word_pair in enumerate(file):\n",
    "                s_word, t_word = word_pair.rstrip().split()\n",
    "                if s_word in s_word2id and t_word in t_word2id:\n",
    "                    index_pairs.append((s_word2id[s_word], t_word2id[t_word]))\n",
    "                    word_pairs.append((s_word, t_word))\n",
    "                else:\n",
    "                    misfit += 1\n",
    "                    misfit_s += int(s_word not in s_word2id)\n",
    "                    misfit_t += int(t_word not in t_word2id)\n",
    "            print('Found {} valid translation pairs in expert dictionary.\\n'\n",
    "                  '{} other pairs contained at least one unknown word ({} in source language, {} in target language).'\n",
    "                  .format(len(word_pairs), misfit, misfit_s, misfit_t))\n",
    "            return index_pairs, word_pairs\n",
    "\n",
    "    elif isinstance(expert_dict, dict):\n",
    "        for s_word, t_word in expert_dict.items():\n",
    "            if s_word in s_word2id and t_word in t_word2id:\n",
    "                index_pairs.append((s_word2id[s_word], t_word2id[t_word]))\n",
    "                word_pairs.append((s_word, t_word))\n",
    "            else:\n",
    "                misfit += 1\n",
    "                misfit_s += int(s_word not in s_word2id)\n",
    "                misfit_t += int(t_word not in t_word2id)\n",
    "        print('Found {} valid translation pairs.\\n'\n",
    "              '{} other pairs contained at least one unknown word ({} in source language, {} in target language).'\n",
    "              .format(len(word_pairs), misfit, misfit_s, misfit_t))\n",
    "        return index_pairs, word_pairs\n",
    "\n",
    "    else:\n",
    "        print('Invalid translation dictionary type. Text file or Python dictionary is required.')\n",
    "        return False\n",
    "\n",
    "\n",
    "def align_monolingual_subspaces(s_emb: np.ndarray, t_emb: np.ndarray, seed_dictionary: list):\n",
    "    \"\"\"\n",
    "    Create aligned monolingual subspaces from seed dictionary.\n",
    "    :param s_emb: monolingual source embedding as returned from load_monolingual_embedding\n",
    "    :param t_emb: monolingual target embedding as returned from load_monolingual_embedding\n",
    "    :param seed_dictionary: index pairs of seed dictionary as returned from extract_seed_dictionary\n",
    "    :return: aligned source and target subspaces\n",
    "    \"\"\"\n",
    "    s_subspace = s_emb[[tuples[0] for tuples in seed_dictionary]]\n",
    "    t_subspace = t_emb[[tuples[1] for tuples in seed_dictionary]]\n",
    "    print(\"Resulting subspace dimension: {}\".format(s_subspace.shape))\n",
    "    return s_subspace, t_subspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13700 valid translation pairs in expert dictionary.\n",
      "977 other pairs contained at least one unknown word (0 in source language, 977 in target language).\n",
      "Resulting subspace dimension: (13700, 300)\n"
     ]
    }
   ],
   "source": [
    "W = learn_projection_matrix(emb_en, emb_de, '../induce_multilingual_embedding_space/expert_dictionaries/MUSE_en-de.0-5000.txt', s_word2id=word2id_en, t_word2id=word2id_de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.69952961])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity((sen_emb_en[0] @ W).reshape(1, -1), sen_emb_de[0].reshape(1,-1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
